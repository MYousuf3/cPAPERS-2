import os
import json
import re
import time
from tqdm import tqdm
from openai import OpenAI
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

def openai_inference(model: str, system_prompt: str, prompt: str):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        response_format={ "type": "json_object" }
    )
    return response.choices[0].message.content

# source of the example https://openreview.net/forum?id=sP1fo2K9DFG&noteId=7nn5Z7rH2WQ
def get_qapairs_llama_figure(review: str, comment: str):
    prompt = 'Given this example Content, Question and Answer, look through the Summary and Comments that are dedicated to the paper and extract Question and Answer pairs that specifically target a particualr Figure in the content. Questions may be under sections labelled similarly to “Weaknesses” or “Questions” or "Review"  or "Clarity, Quality, Novelty And Reproducibility" in the comments, and Answers may be under sections labelled “Response to Reviewer” or “Rebuttal" or "Comments".'
    example_content = """
    Summary Of The Paper:
    ...
    ...
    Strength And Weaknesses:
    ...
    - What is the performance metric used in Figure 4? 
    ...
    Clarity, Quality, Novelty And Reproducibility:
    ...
    ...
    Summary Of The Review:
    ...
    Comments:
    ...
    In Figure 4, we compare the performance of Decision Diffuser (DD) to Conservative Q-learning (CQL) and Behavior Cloning (BC) on three benchmarks: D4RL Locomotion, D4RL Kitchen, and Kuka Block Stacking. Each plot demonstrates the performance of these methods on one of the three benchmarks. The performance metric shown on the y-axis depends on the plot. The plot comparing these methods on the D4RL Locomotion and Kitchen benchmark uses normalized average returns (taken from [1]) as the performance metric. In order to measure performance on the Kuka Block Stacking benchmark, we use the success rate of how often trajectories generated by one of the methods (i.e. DD, CQL, BC) satisfy the block stacking constraints of the given task.
    ...
    """

    example_question = "What is the performance metric used in Figure 4? "
    example_answer = "In Figure 4, we compare the performance of Decision Diffuser (DD) to Conservative Q-learning (CQL) and Behavior Cloning (BC) on three benchmarks: D4RL Locomotion, D4RL Kitchen, and Kuka Block Stacking. Each plot demonstrates the performance of these methods on one of the three benchmarks. The performance metric shown on the y-axis depends on the plot. The plot comparing these methods on the D4RL Locomotion and Kitchen benchmark uses normalized average returns (taken from [1]) as the performance metric. In order to measure performance on the Kuka Block Stacking benchmark, we use the success rate of how often trajectories generated by one of the methods (i.e. DD, CQL, BC) satisfy the block stacking constraints of the given task."
    content = (
        f'[Context]\n{prompt}\n\n'
        f'[Content]\n{example_content}\n\n'
        f'[Question]\n{example_question}\n\n'
        f'[Answer]\n{example_answer}\n\n'
        f'[Content]\n{review + comment}\n')
    
    system_prompt = "You are a helpful question answer extracting assistant. Our goal is to extract Question and Answer pairs pertaining to a specific a Figure in the content. Your response should be in the format of [Question] <question> [Answer] <answer> Do not add any other unnecessary content in your response."

    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
    formatted_input = f"{B_SYS}{system_prompt}{E_SYS}{B_INST}{content}{E_INST}"
    return formatted_input

# https://openreview.net/forum?id=ueYYgo2pSSU
def get_qapairs_llama_equation(review: str, comment: str):
    prompt = 'Given this example Content, Question and Answer, look through the Summary and Comments that are dedicated to the paper and extract Question and Answer pairs that specifically target a particualr Equation in the content. Questions may be under sections labelled similarly to “Weaknesses” or “Questions” or "Review" or "Clarity, Quality, Novelty And Reproducibility" in the comments, and Answers may be under sections labelled “Response to Reviewer” or “Rebuttal" or "Comments".'
    example_content = """
    Summary Of The Paper:
    ...
    ...
    Strength And Weaknesses:
    ...
    ...
    Clarity, Quality, Novelty And Reproducibility:
    ...
    ...
    Summary Of The Review:
    ...
    Can you clarify how Jensen's inequality is applied in Equation 2?
    ...
    Comments:
    ...
    The regularization term is equivalent to \mathbb{E}\pi \left[f \left(\frac{\pi}{\mu} \right) \right] = \mathbb{E}\mu \left[\frac{\pi}{\mu}f \left(\frac{\pi}{\mu} \right) \right] because is strictly convex, we can apply Jensen's inequality by moving the expectation inside, which has \mathbb{E}\mu \left[\frac{\pi}{\mu}f \left(\frac{\pi}{\mu} \right) \right] = \mathbb{E}\mu \left[h_f \left(\frac{\pi}{\mu} \right) \right] \geq h_f \left(\mathbb{E}_\mu \left[\frac{\pi}{\mu} \right] \right) = h_f(1) = 1 f(1) = 0
    ...
    """

    example_question = "Can you clarify how Jensen's inequality is applied in Equation 2?"
    example_answer = "The regularization term is equivalent to \mathbb{E}\pi \left[f \left(\frac{\pi}{\mu} \right) \right] = \mathbb{E}\mu \left[\frac{\pi}{\mu}f \left(\frac{\pi}{\mu} \right) \right] because  is strictly convex, we can apply Jensen's inequality by moving the expectation inside, which has \mathbb{E}\mu \left[\frac{\pi}{\mu}f \left(\frac{\pi}{\mu} \right) \right] = \mathbb{E}\mu \left[h_f \left(\frac{\pi}{\mu} \right) \right] \geq h_f \left(\mathbb{E}_\mu \left[\frac{\pi}{\mu} \right] \right) = h_f(1) = 1 f(1) = 0 "
    content = (
        f'[Context]\n{prompt}\n\n'
        f'[Content]\n{example_content}\n\n'
        f'[Question]\n{example_question}\n\n'
        f'[Answer]\n{example_answer}\n\n'
        f'[Content]\n{review + comment}\n')
    
    system_prompt = "You are a helpful question answer extracting assistant. Our goal is to extract Question and Answer pairs pertaining to a specific a Equation in the content. Your response should be in the format of [Question] <question> [Answer] <answer> Do not add any other unnecessary content in your response."

    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
    formatted_input = f"{B_SYS}{system_prompt}{E_SYS}{B_INST}{content}{E_INST}"
    return formatted_input

# source of the example: https://openreview.net/forum?id=sWOsRj4nT1n
def get_qapairs_llama_table(review: str, comment: str):
    prompt = 'Given this example Content, Question and Answer, look through the Summary and Comments that are dedicated to the paper and extract Question and Answer pairs that specifically target a particualr Table in the content. Questions may be under sections labelled similarly to “Weaknesses” or “Questions” or "Review" or "Clarity, Quality, Novelty And Reproducibility" in the comments, and Answers may be under sections labelled “Response to Reviewer” or “Rebuttal" or "Comments".'
    example_content = """
    Summary Of The Paper:
    ...
    ...
    Strength And Weaknesses:
    ...
    ...
    Clarity, Quality, Novelty And Reproducibility:
    ...
    - Table 2 line one does not correspond to the performance of IncFinetune but offline (so I think there is a typo somewhere)
    ...
    Summary Of The Review:
    ...
    Comments:
    ...
    A3: This is a typo. The first row of Table 2 corresponds to the offline method. Thank you for pointing it out and we have revised this in our re-uploaded version in Table 2.
    ...
    """

    example_question = "Table 2 line one does not correspond to the performance of IncFinetune but offline (so I think there is a typo somewhere)"
    example_answer = "This is a typo. The first row of Table 2 corresponds to the offline method. Thank you for pointing it out and we have revised this in our re-uploaded version in Table 2."
    content = (
        f'[Context]\n{prompt}\n\n'
        f'[Content]\n{example_content}\n\n'
        f'[Question]\n{example_question}\n\n'
        f'[Answer]\n{example_answer}\n\n'
        f'[Content]\n{review + comment}\n')
    
    system_prompt = "You are a helpful question answer extracting assistant. Our goal is to extract Question and Answer pairs pertaining to a specific a Table in the content. Your response should be in the format of [Question] <question> [Answer] <answer> Do not add any other unnecessary content in your response."

    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
    formatted_input = f"{B_SYS}{system_prompt}{E_SYS}{B_INST}{content}{E_INST}"
    return formatted_input


def save_json(data, filename):
    with open(filename, "w") as f:
        json.dump(data, f, indent=4)

def open_json(filename):
    with open(filename, "r") as f:
        return json.load(f)

def check_for_qa_and_reference(base_path):
    or_data = open_json(base_path + "raw_qas.json")

    pattern_figure = re.compile(r'\b(?:Figure|fig)\.?\s*\d+\b', re.IGNORECASE)
    pattern_equation = re.compile(r'\b(?:Equation|Eq\.?)\s*\(?\s*\d+\)?\b', re.IGNORECASE)
    pattern_table = re.compile(r'\bTable\s*\d+|\btables\b', re.IGNORECASE)

    review_comment_figure = []
    review_comment_equation = []
    review_comment_table = []

    count = 0
    for i, item in tqdm(enumerate(or_data)):
        # Debug: print the full path and repr of paper_id for the first 10 items
        if not os.path.exists(base_path + f"tar_files/{item['paper_id']}.tar.gz"):
            print(f"Tar file tar_files/{item['paper_id']}.tar.gz does not exist")
            count += 1
            continue
        
        if "review" in item and "comment" in item:


            has_figure = re.search(pattern_figure, item["review"] + item["comment"])
            has_equation = re.search(pattern_equation, item["review"] + item["comment"])
            has_table = re.search(pattern_table, item["review"] + item["comment"])
                
            if has_figure:
                review_comment_figure.append(item)
            if has_equation:
                review_comment_equation.append(item)
            if has_table:
                review_comment_table.append(item)

            # if review or comment contains Figure or Table or Equation 
            # pattern = re.compile(r'(Table|Figure|Fig\.|Eq\.|Equation)\s+\d+', re.IGNORECASE)

            # match = re.search(pattern, item["review"] + item["comment"])
            # if match:
            #     matched_qas.append(item)
            # else:
            #     continue
    
    save_json(review_comment_figure, base_path + "review_comment_figure.json")
    save_json(review_comment_equation, base_path + "review_comment_equation.json")
    save_json(review_comment_table, base_path + "review_comment_table.json")
    print(f"Missing {count} papers !!!")
    return review_comment_figure, review_comment_equation,review_comment_table

def extract_qa_pairs(text):
    cleaned_qa = []
    try:
        for pair in text["question_answers"]:
            pattern_figure = re.compile(r'\b(?:Figure|fig)\.?\s*\d+\b', re.IGNORECASE)
            pattern_equation = re.compile(r'\b(?:Equation|Eq\.?)\s*\(?\s*\d+\)?\b', re.IGNORECASE)
            pattern_table = re.compile(r'\bTable\s*\d+|\btables\b', re.IGNORECASE)

            has_figure = re.search(pattern_figure, pair["question"] + pair["answer"])
            has_equation = re.search(pattern_equation, pair["question"] + pair["answer"])
            has_table = re.search(pattern_table, pair["question"] + pair["answer"])
            
            if has_figure:
                pair["Figure"] = True
                pair["Figure_with_num"] = has_figure.group()

            if has_equation:
                pair["Equation"] = True
                pair["Equation_with_num"] = has_equation.group()
            if has_table:
                pair["Table"] = True
                pair["Table_with_num"] = has_table.group()

            if has_figure or has_equation or has_table:
                cleaned_qa.append(pair)

        return cleaned_qa
    except Exception as e:
        return None

def extract_qas(or_data, type, path, tokenizer, llm):
    
        
    sampling_params = SamplingParams(
            top_k=10,
            n=1,
            max_tokens=2000,
            stop_token_ids=[tokenizer.eos_token_id])

    # or_data = open_json("full_matched_qas.json")
    # print(len(or_data))
    if type == 'figure':
        prompt_batch = [get_qapairs_llama_figure(item["review"], item["comment"]) for item in or_data]
    if type == 'equation':
        prompt_batch = [get_qapairs_llama_equation(item["review"], item["comment"]) for item in or_data]
    if type == 'table':
        prompt_batch = [get_qapairs_llama_table(item["review"], item["comment"]) for item in or_data]

    try:
        outputs = llm.generate(prompt_batch, sampling_params=sampling_params)
    except Exception as e:
        print(f"Error during llm.generate: {e}")
        outputs = []
    for idx, output in enumerate(outputs):
        # print(f"Output {idx}: {getattr(output, 'outputs', None)}")
        if hasattr(output, 'outputs') and output.outputs:
            text = output.outputs[0].text
            # print(f"Text: {text}")
            or_data[idx]["extracted_qa"] = text
        else:
            print("No outputs for this prompt")
            or_data[idx]["extracted_qa"] = None

    save_json(or_data, f"{path}/qa_extraction_{type}.json")
    #import ray
    #ray.shutdown()
    return or_data

def format_gpt(data, path):
    for idx, item in enumerate(tqdm(data)):
        prompt = item["extracted_qa"]
        try:
            system_prompt = """You are a helpful assistant. Please find the Question Answer pairs, and format it as a json ({"question_answers": [{ "question": "", "answer": "" },...]})"""
            response = openai_inference("gpt-3.5-turbo", system_prompt, prompt)
            data[idx]["extracted_qa_pairs"] = response
        except Exception as e:
            print(e)
            data[idx]["extracted_qa_pairs"] = None

        save_json(data, f"{path}/formatted_qa_extraction.json")
        time.sleep(1)

def main():
    # filter view and coments pairs 
    base_path = './neurips/2021/'
    """
    review_figure, review_equation, review_table = check_for_qa_and_reference(base_path)
    """
    
    """
    model = "meta-llama/Llama-3.1-8B-Instruct"
    gpus = 4
    tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)
    llm = LLM(
            model, 
            tokenizer_mode="auto",
            tensor_parallel_size=gpus, 
            enforce_eager=True)
    
    
    review_figure = open_json(base_path + "/review_comment_figure.json")
    review_equation = open_json(base_path + "/review_comment_equation.json")
    review_table = open_json(base_path + "/review_comment_table.json")

    
    qas_figure = extract_qas(review_figure, "figure", base_path, tokenizer, llm)
    qas_equation = extract_qas(review_equation, "equation", base_path, tokenizer, llm)
    qas_table = extract_qas(review_table, "table", base_path, tokenizer, llm)
    
    print("Done!")
    # Ensure Ray cleans up after execution
    """
    qas_figure = open_json(base_path+'/qa_extraction_figure.json')
    qas_equation = open_json(base_path+'/qa_extraction_equation.json')
    qas_table = open_json(base_path+'/qa_extraction_table.json')

    # sampling only 1/3
    qas_figure = qas_figure[:333]
    qas_equation = qas_equation[:333]
    qas_table = qas_table[:333]


    combined_qas = qas_figure + qas_equation + qas_table
    print(len(combined_qas))

    
    #format_gpt(combined_qas, base_path)
    print("check cleaned_qa_pair.")
    # ## Next cleaning step
    data = open_json(base_path + "/formatted_qa_extraction.json")
    print(len(data))
    count_cleaned = 0
    for idx, item in enumerate(tqdm(data)):
        if item["extracted_qa_pairs"]:
            try:
                text = json.loads(item["extracted_qa_pairs"])
            except Exception as e:
                data[idx]["cleaned_qa"] = None
                continue

            cleaned_qa = extract_qa_pairs(text)
            if cleaned_qa:
                data[idx]["cleaned_qa"] = cleaned_qa
                count_cleaned += 1
            else:
                data[idx]["cleaned_qa"] = None
    print(len(data))
    print("Count of cleaned items:", count_cleaned)

    save_json(data, base_path+"/cleaned_qa_pair.json")

if __name__ == "__main__":
    main()