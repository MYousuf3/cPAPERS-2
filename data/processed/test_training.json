[
  {
    "paper_id": "2106.07880v2",
    "submission_id": "vIRFiA658rh",
    "submission_title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
    "review_id": "5ASDAGCzOds",
    "input": {
      "title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This submission merged two papers and addressed most of my previous concerns.\n- I have no further comments on this paper except the following minor issues:\n- In the experiments, the MSE metric should be changed to $|| K - \tilde{K} ||/|| K ||$ in some norms for intuitive/fair comparison.\n- Please follow experimental settings of previous works on random features/Nystrom approximation.\n- The obtained approximation bound in Theorem 1 might be loose due to two independent constants $\nepsilon$ and $\ndelta$.\n- It would be possible to obtain a tight bound?\n- E.g., an exponential order decay following the fashion of concentration inequalities.\n- I don\u2019t check the proof carefully but I think the JL transform used in the proof of Theorem 1 could be improved?\n- One paper on leverage score based sampling for random features is missing:\n- Li, Z., Ton, J. F., Oglic, D., & Sejdinovic, D. Towards a unified analysis of random Fourier features. ICML2019.\n- The used technical tools for spectral approximation is also based on the matrix concentration inequality, see Lemma 4 for details.\n- This paper and other random features literature [37] require the random feature map to be continuous, which in fact excludes the zero-order arc-cosine kernel (corresponds to the step function).\n- How to address this when applied to NTK?\n- The expression \u2018low-dimensional feature maps\u2019 is inaccurate.\n- The dimension of the obtained feature map is larger than d but is smaller than n.\n- So it would be better to polish this.\n- The Gibbs sampling used in Algorithm 3 seeks for an approximation sampling as the modified distribution q(w) is intractable to be sampled.\n- Some reference information should be checked, e.g., [36] published on NeurIPS2007 and [37] published on NeurIPS2017.",
    "review_points_list": [
      "This submission merged two papers and addressed most of my previous concerns.",
      "I have no further comments on this paper except the following minor issues:",
      "In the experiments, the MSE metric should be changed to $|| K - \tilde{K} ||/|| K ||$ in some norms for intuitive/fair comparison.",
      "Please follow experimental settings of previous works on random features/Nystrom approximation.",
      "The obtained approximation bound in Theorem 1 might be loose due to two independent constants $\nepsilon$ and $\ndelta$.",
      "It would be possible to obtain a tight bound?",
      "E.g., an exponential order decay following the fashion of concentration inequalities.",
      "I don\u2019t check the proof carefully but I think the JL transform used in the proof of Theorem 1 could be improved?",
      "One paper on leverage score based sampling for random features is missing:",
      "Li, Z., Ton, J. F., Oglic, D., & Sejdinovic, D. Towards a unified analysis of random Fourier features. ICML2019.",
      "The used technical tools for spectral approximation is also based on the matrix concentration inequality, see Lemma 4 for details.",
      "This paper and other random features literature [37] require the random feature map to be continuous, which in fact excludes the zero-order arc-cosine kernel (corresponds to the step function).",
      "How to address this when applied to NTK?",
      "The expression \u2018low-dimensional feature maps\u2019 is inaccurate.",
      "The dimension of the obtained feature map is larger than d but is smaller than n.",
      "So it would be better to polish this.",
      "The Gibbs sampling used in Algorithm 3 seeks for an approximation sampling as the modified distribution q(w) is intractable to be sampled.",
      "Some reference information should be checked, e.g., [36] published on NeurIPS2007 and [37] published on NeurIPS2017."
    ]
  },
  {
    "paper_id": "2106.07880v2",
    "submission_id": "vIRFiA658rh",
    "submission_title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
    "review_id": "zgj66AqESXj",
    "input": {
      "title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Please note that there is some ambiguity about what a 'distinct comment, question, strength, weakness, or observation' is.\n- For this exercise, I will assume that the review points should be separate sentences or independent thoughts.\n- Also, I will try to maintain the same format and structure as the example split points.\n- Please let me know how to proceed further.\n- I do think there are some improvements that could be made to the manuscript to better explain aspects of these contributions.\n- Please note that you should not add any unnecessary content to your response, but instead provide the requested output in the specified format.",
    "review_points_list": [
      "Please note that there is some ambiguity about what a 'distinct comment, question, strength, weakness, or observation' is.",
      "For this exercise, I will assume that the review points should be separate sentences or independent thoughts.",
      "Also, I will try to maintain the same format and structure as the example split points.",
      "Please let me know how to proceed further.",
      "I do think there are some improvements that could be made to the manuscript to better explain aspects of these contributions.",
      "Please note that you should not add any unnecessary content to your response, but instead provide the requested output in the specified format."
    ]
  },
  {
    "paper_id": "2106.07880v2",
    "submission_id": "vIRFiA658rh",
    "submission_title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
    "review_id": "-yH7bAhfNPp",
    "input": {
      "title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The method described in the paper address the vital running time problem of the original NTK.\n- The experiments verify the speed up by a large factor.\n- The time complexity is well studied in the paper.\n- I have some concerns in the writing, as it seldomly introduces the original NTK and then directly in the proposed algorithm itself, which is unfriendly to some readers.",
    "review_points_list": [
      "The method described in the paper address the vital running time problem of the original NTK.",
      "The experiments verify the speed up by a large factor.",
      "The time complexity is well studied in the paper.",
      "I have some concerns in the writing, as it seldomly introduces the original NTK and then directly in the proposed algorithm itself, which is unfriendly to some readers."
    ]
  },
  {
    "paper_id": "2107.12685v1",
    "submission_id": "zzSVN5x8JiX1m",
    "submission_title": "On the Role of Optimization in Double Descent: A Least Squares Study",
    "review_id": "jODTff_p5FX",
    "input": {
      "title": "On the Role of Optimization in Double Descent: A Least Squares Study",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I think that DD is still not well understood, and studying it is important and interesting. Also, the connection between DD and certain parameters of the data or optimization is an interesting observation. On the other hand, I think this paper makes a weak case for why these parameters actually affect DD, especially on the experimental part, and vaguely connects the theory presented in the paper with the experiments.\n- For the theoretical part: 1) I think the main idea is nice, studying linear regression in the case where d>n (more parameters than data examples), which is a less studied case in the literature.\n- 2) Viewing section 2 as a whole, it is hard to grasp a single conclusion on how the excess risk behaves. It would have been much clearer if there was a single theorem that shows a bound on the excess risk when d goes to infinity.\n- 3) To emphasize the previous point, two parameters are considered (n and d), and throughout the section it is unclear which one is considered to be larger.\n- 4) Section 3 is very unclear. What are the assumptions made in the equation after line 216? What are the main conclusions of this section?\n- 5) I think this is the main weak point of the paper. The connection between the experiments and the theory",
    "review_points_list": [
      "I think that DD is still not well understood, and studying it is important and interesting. Also, the connection between DD and certain parameters of the data or optimization is an interesting observation. On the other hand, I think this paper makes a weak case for why these parameters actually affect DD, especially on the experimental part, and vaguely connects the theory presented in the paper with the experiments.",
      "For the theoretical part: 1) I think the main idea is nice, studying linear regression in the case where d>n (more parameters than data examples), which is a less studied case in the literature.",
      "2) Viewing section 2 as a whole, it is hard to grasp a single conclusion on how the excess risk behaves. It would have been much clearer if there was a single theorem that shows a bound on the excess risk when d goes to infinity.",
      "3) To emphasize the previous point, two parameters are considered (n and d), and throughout the section it is unclear which one is considered to be larger.",
      "4) Section 3 is very unclear. What are the assumptions made in the equation after line 216? What are the main conclusions of this section?",
      "5) I think this is the main weak point of the paper. The connection between the experiments and the theory"
    ]
  },
  {
    "paper_id": "2107.12685v1",
    "submission_id": "zzSVN5x8JiX1m",
    "submission_title": "On the Role of Optimization in Double Descent: A Least Squares Study",
    "review_id": "wStH974jaR_",
    "input": {
      "title": "On the Role of Optimization in Double Descent: A Least Squares Study",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper tries to study and explain the double descent phenomenon from a new perspective. However, it does not provide much new insight for the phenomenon.\n- The reasons are as follows:\n- 1. The object lying in the center of the analysis---the smallest nonzero eigenvalue of the Gram matrix---is the well known cause of the double descent phenomenon in linear least squares problems (including the random feature models), and is also extensively studied.\n- In the works focusing on minimal norm solutions or ridge regression solutions, the same quantity is also studied and identified to be the factor that leads to the DD phenomenon.\n- 2. The upper bound for the excess risk only considers the smallest nonzero eigenvalue of the Gram matrix, which means it only controls the slowest component in the dynamics.\n- Hence, the bound becomes tight only when the number of iterations, T, becomes large.\n- Before this, the components corresponding to big eigenvalues dominate the dynamics and the DD phenomenon does not appear.\n- This is showed by the empirical results in the paper.\n- However, once T is sufficiently big such that the component of the smallest eigenvalue dominates the dynamics, the GD solution is already close to the minimal norm solution or the l2 regularized solutions studied in previous works.\n- Therefore, the so-called 'the roll of optimization' does not make much sense.\n- Especially considering that the loss function is actually convex and any reasonable optimization algorithm will converge to a global minimum.\n- 3. For a linear least squares problem, the GD solutions and the corresponding population risk can be written down in close form.\n- Hence, the reviewer does not see the necessity to derive the excess risk bounds via indirect approach such as (\u2206, M)-admissible.\n- Also, the (\u2206, M)-admissible condition does not help extend the analysis to non-linear cases.\n- Some other comments:\n- 1. Shown by numerical experiments on neural networks, it seems the peak of the generalization error happens when the width approximately equals the number of data.\n- This is different that shown in Belkin et.al, in which the the generalization error peaks when the number of parameters = number of data * number of classes.\n- Hence, this is like a different kind of 'double descent'.\n- It is possible to provide more explanation on this phenomenon using the techniques employed in the paper?\n- 2. Some notations are confusing.\n- For example, (a) the excess risk is denoted by E(w^*) while w^* is actually a fixed point.\n- It is better to use E(A_S(W_0)) to highlight the dependence of the excess risk with the GD solution.\n- (b) the data us denoted by a capital X, which it is mentioned that vectors are denoted by boldface lower case letters.\n- This causes confusion.",
    "review_points_list": [
      "This paper tries to study and explain the double descent phenomenon from a new perspective. However, it does not provide much new insight for the phenomenon.",
      "The reasons are as follows:",
      "1. The object lying in the center of the analysis---the smallest nonzero eigenvalue of the Gram matrix---is the well known cause of the double descent phenomenon in linear least squares problems (including the random feature models), and is also extensively studied.",
      "In the works focusing on minimal norm solutions or ridge regression solutions, the same quantity is also studied and identified to be the factor that leads to the DD phenomenon.",
      "2. The upper bound for the excess risk only considers the smallest nonzero eigenvalue of the Gram matrix, which means it only controls the slowest component in the dynamics.",
      "Hence, the bound becomes tight only when the number of iterations, T, becomes large.",
      "Before this, the components corresponding to big eigenvalues dominate the dynamics and the DD phenomenon does not appear.",
      "This is showed by the empirical results in the paper.",
      "However, once T is sufficiently big such that the component of the smallest eigenvalue dominates the dynamics, the GD solution is already close to the minimal norm solution or the l2 regularized solutions studied in previous works.",
      "Therefore, the so-called 'the roll of optimization' does not make much sense.",
      "Especially considering that the loss function is actually convex and any reasonable optimization algorithm will converge to a global minimum.",
      "3. For a linear least squares problem, the GD solutions and the corresponding population risk can be written down in close form.",
      "Hence, the reviewer does not see the necessity to derive the excess risk bounds via indirect approach such as (\u2206, M)-admissible.",
      "Also, the (\u2206, M)-admissible condition does not help extend the analysis to non-linear cases.",
      "Some other comments:",
      "1. Shown by numerical experiments on neural networks, it seems the peak of the generalization error happens when the width approximately equals the number of data.",
      "This is different that shown in Belkin et.al, in which the the generalization error peaks when the number of parameters = number of data * number of classes.",
      "Hence, this is like a different kind of 'double descent'.",
      "It is possible to provide more explanation on this phenomenon using the techniques employed in the paper?",
      "2. Some notations are confusing.",
      "For example, (a) the excess risk is denoted by E(w^*) while w^* is actually a fixed point.",
      "It is better to use E(A_S(W_0)) to highlight the dependence of the excess risk with the GD solution.",
      "(b) the data us denoted by a capital X, which it is mentioned that vectors are denoted by boldface lower case letters.",
      "This causes confusion."
    ]
  },
  {
    "paper_id": "2107.12685v1",
    "submission_id": "zzSVN5x8JiX1m",
    "submission_title": "On the Role of Optimization in Double Descent: A Least Squares Study",
    "review_id": "NAS-YlvpXoT",
    "input": {
      "title": "On the Role of Optimization in Double Descent: A Least Squares Study",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I like the idea of relating DD to the smallest positive eigenvalue of the covariance matrix, which seems to give a fairly new perspective on the DD curves.\n- But I'm not an expert in DD as I only read a few related papers.\n- A detailed comparison with previous papers is important for me to have a proper evaluation for this paper.\n- However, the related work section is in the appendix, not in the main body of the paper.\n- Comparisons with some closely related papers are not detailed enough.\n- The authors mentioned that Derezinski et al., 2020, Rangamani et al., 2020, Hastie et al., 2019 also noted that the smallest positive eigenvalue is important in explaining double descent, but the key difference to the current paper is not very clear to me if I only read the related work section.\n- I would suggest the authors add more details:\n- 1. What are the assumptions on the data distribution in previous works? How do they compare with those in the current paper?\n- 2. How do those papers find the connection between DD and the smallest positive eigenvalue without studying GD optimization?\n- 3. If they prove double descent without considering GD optimization, does it mean optimization is not a crucial factor to make double descent occur?\n- The authors claim that the current paper is superior as it considers GD optimization, but it should be justified that GD optimization is crucial in the first place.\n- Unlike some previous papers, this paper does not assume an explicit formula for the data distribution.\n- This is an advantage of this paper as it can hold for broader settings.\n- However, this also makes it impossible to derive an exact formula for excess risk.\n- Alternatively, the authors study the upper bounds, but no lower bound is given.\n- Lower bounds are especially important for explaining the peak because having an upper bound approximately 1 can be either because there is a peak or the upper bound is not tight enough.\n- I would like to see the authors' argument about the tightness of the upper bounds.\n- Empirical evaluations for those upper bounds may also help.",
    "review_points_list": [
      "I like the idea of relating DD to the smallest positive eigenvalue of the covariance matrix, which seems to give a fairly new perspective on the DD curves.",
      "But I'm not an expert in DD as I only read a few related papers.",
      "A detailed comparison with previous papers is important for me to have a proper evaluation for this paper.",
      "However, the related work section is in the appendix, not in the main body of the paper.",
      "Comparisons with some closely related papers are not detailed enough.",
      "The authors mentioned that Derezinski et al., 2020, Rangamani et al., 2020, Hastie et al., 2019 also noted that the smallest positive eigenvalue is important in explaining double descent, but the key difference to the current paper is not very clear to me if I only read the related work section.",
      "I would suggest the authors add more details:",
      "1. What are the assumptions on the data distribution in previous works? How do they compare with those in the current paper?",
      "2. How do those papers find the connection between DD and the smallest positive eigenvalue without studying GD optimization?",
      "3. If they prove double descent without considering GD optimization, does it mean optimization is not a crucial factor to make double descent occur?",
      "The authors claim that the current paper is superior as it considers GD optimization, but it should be justified that GD optimization is crucial in the first place.",
      "Unlike some previous papers, this paper does not assume an explicit formula for the data distribution.",
      "This is an advantage of this paper as it can hold for broader settings.",
      "However, this also makes it impossible to derive an exact formula for excess risk.",
      "Alternatively, the authors study the upper bounds, but no lower bound is given.",
      "Lower bounds are especially important for explaining the peak because having an upper bound approximately 1 can be either because there is a peak or the upper bound is not tight enough.",
      "I would like to see the authors' argument about the tightness of the upper bounds.",
      "Empirical evaluations for those upper bounds may also help."
    ]
  },
  {
    "paper_id": "2010.07893v2",
    "submission_id": "utt-q6jW5_w",
    "submission_title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
    "review_id": "TntFbxWqfb7",
    "input": {
      "title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I found the paper interesting, though somewhat confusing to me, possibly due to my lack of expertise with the details of the algorithms.\n- The method is novel, and addresses an important limitation of REINFORCE-like algorithms (the high variance and the difficulties this causes for learning).\n- The relative biological plausibility (the rule is local to each neuron, though the actual mechanism seems somewhat involved for neurons to perform) is a nice bonus.\n- The experiments are well chosen and the results are informative.\n- The paper is well-written throughout, though could gain some clarity.\n- It would be extremely helpful to provide a very short, intuitive summary of the overall proposal and derivation before going on with the full description.\n- IUC, it is so that we need fewer actual samples to estimate the gradients.\n- The crucial justification for the whole approach seems to that the RHS in Eq. 4 should have lower variance than Eq. 5.\n- The reason for the reduction of variance should be explained a bit better, possibly in an appendix section.\n- When the introduction and abstract mention that the proposed approach is equivalent to backpropagation. IIUC, it should be 'backpropagation with the reparametrization trick'.\n- Eq 11 and Theorem 2 seem to assert that the method is equivalent to backpropagation with the reparametrization trick.\n- The experiments seem to show that the method has quite distinct properties, e.g. being better when more exploration is needed.\n- I do not understand what information is 'lost'.\n- The appendix seems to suggest a simple centering and normalization method to address this 'loss'.",
    "review_points_list": [
      "I found the paper interesting, though somewhat confusing to me, possibly due to my lack of expertise with the details of the algorithms.",
      "The method is novel, and addresses an important limitation of REINFORCE-like algorithms (the high variance and the difficulties this causes for learning).",
      "The relative biological plausibility (the rule is local to each neuron, though the actual mechanism seems somewhat involved for neurons to perform) is a nice bonus.",
      "The experiments are well chosen and the results are informative.",
      "The paper is well-written throughout, though could gain some clarity.",
      "It would be extremely helpful to provide a very short, intuitive summary of the overall proposal and derivation before going on with the full description.",
      "IUC, it is so that we need fewer actual samples to estimate the gradients.",
      "The crucial justification for the whole approach seems to that the RHS in Eq. 4 should have lower variance than Eq. 5.",
      "The reason for the reduction of variance should be explained a bit better, possibly in an appendix section.",
      "When the introduction and abstract mention that the proposed approach is equivalent to backpropagation. IIUC, it should be 'backpropagation with the reparametrization trick'.",
      "Eq 11 and Theorem 2 seem to assert that the method is equivalent to backpropagation with the reparametrization trick.",
      "The experiments seem to show that the method has quite distinct properties, e.g. being better when more exploration is needed.",
      "I do not understand what information is 'lost'.",
      "The appendix seems to suggest a simple centering and normalization method to address this 'loss'."
    ]
  },
  {
    "paper_id": "2010.07893v2",
    "submission_id": "utt-q6jW5_w",
    "submission_title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
    "review_id": "vdZxpRggpBZ",
    "input": {
      "title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Introducing a biologically plausible alternative to backprop is a very active research area that would be beneficial in bridging the gap between neuroscience and artificial neural networks.\n- And often the proposed alternatives are sub-optimal and not able to match the performance of backprop.\n- The results presented in this work show that their proposed method is able to reliably work.\n- The algorithm is relatively easy to implement and does not impose many constraints on the network architecture, hence could be a work that the community can build upon it.\n- In the first paragraph of related work, is the main novelty of this work compared to the cited works is the application of MAP propagation to RL as opposed to (un)supervised learning?\n- Would be great to elaborate more.\n- The main text is quite dense.\n- Perhaps some restructuring and rewriting could make the paper more easily readable.\n- Please have a brief description of the multiplexer and scalar regression tasks to make the text self-sufficient.\n- Due to inner loop optimization for each layer, there is added computational complexity.\n- Could you please also have a comparison of the computational complexity of this method vs backprop?\n- The appending L:82 mentions that a different learning rate was used for each layer.\n- This will result in an increase in the number of hyper-parameters with layer size.\n- How difficult was it to tune each of these separately?",
    "review_points_list": [
      "Introducing a biologically plausible alternative to backprop is a very active research area that would be beneficial in bridging the gap between neuroscience and artificial neural networks.",
      "And often the proposed alternatives are sub-optimal and not able to match the performance of backprop.",
      "The results presented in this work show that their proposed method is able to reliably work.",
      "The algorithm is relatively easy to implement and does not impose many constraints on the network architecture, hence could be a work that the community can build upon it.",
      "In the first paragraph of related work, is the main novelty of this work compared to the cited works is the application of MAP propagation to RL as opposed to (un)supervised learning?",
      "Would be great to elaborate more.",
      "The main text is quite dense.",
      "Perhaps some restructuring and rewriting could make the paper more easily readable.",
      "Please have a brief description of the multiplexer and scalar regression tasks to make the text self-sufficient.",
      "Due to inner loop optimization for each layer, there is added computational complexity.",
      "Could you please also have a comparison of the computational complexity of this method vs backprop?",
      "The appending L:82 mentions that a different learning rate was used for each layer.",
      "This will result in an increase in the number of hyper-parameters with layer size.",
      "How difficult was it to tune each of these separately?"
    ]
  },
  {
    "paper_id": "2010.07893v2",
    "submission_id": "utt-q6jW5_w",
    "submission_title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
    "review_id": "JWfLVHLDqj",
    "input": {
      "title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- As the author's recognize, the idea of apply REINFORCE to every unit in a network is old.\n- The variance reduction approach used here is novel and the main contribution.\n- The approach appears technically sound (I did not check the appendix carefully) and the experimental results reasonable, although only on relatively simple tasks.\n- The approach introduce here is interesting, because any new learning rule is interesting.\n- But its main claimed benefits are that is more biologically plausible (than backprop) due to the use of only local information and a global reward.\n- For this reason, it seems that other somewhat biological plausible approaches should have also been included as baselines, such as feedback alignment [11-13].\n- This submission would also be strengthened by discussing how ``biologically plausible'' this learning method is.\n- For example, (approximately) taking the argmax (eq 8) is done layer-wise rather than unit-wise.\n- Is there a plausible neural mechanisms for this? (I don't know the answer).\n- Overall the paper is well written.\n- One minor thing is that in some cases I think it would be helpful to be explicit about what the expectation is taken over.\n- Learning rules that are competitive (in learning speed) with backprop while using only local information are of interest to the community.\n- Based on the experiment here, this approach appears of interest.\n- The interest to the neuroscience community would depend on how biologically plausible this learning rule is when examined in more detail.\n- $H$ is referred to the as the ``value'' of a layer.\n- It might be clearer to call this the activation of a layer, as ``value'' could mean weight or activation.\n- I realize that MAP is a common abbreviation, but I still think that it would be ideal to write out the term in full once.",
    "review_points_list": [
      "As the author's recognize, the idea of apply REINFORCE to every unit in a network is old.",
      "The variance reduction approach used here is novel and the main contribution.",
      "The approach appears technically sound (I did not check the appendix carefully) and the experimental results reasonable, although only on relatively simple tasks.",
      "The approach introduce here is interesting, because any new learning rule is interesting.",
      "But its main claimed benefits are that is more biologically plausible (than backprop) due to the use of only local information and a global reward.",
      "For this reason, it seems that other somewhat biological plausible approaches should have also been included as baselines, such as feedback alignment [11-13].",
      "This submission would also be strengthened by discussing how ``biologically plausible'' this learning method is.",
      "For example, (approximately) taking the argmax (eq 8) is done layer-wise rather than unit-wise.",
      "Is there a plausible neural mechanisms for this? (I don't know the answer).",
      "Overall the paper is well written.",
      "One minor thing is that in some cases I think it would be helpful to be explicit about what the expectation is taken over.",
      "Learning rules that are competitive (in learning speed) with backprop while using only local information are of interest to the community.",
      "Based on the experiment here, this approach appears of interest.",
      "The interest to the neuroscience community would depend on how biologically plausible this learning rule is when examined in more detail.",
      "$H$ is referred to the as the ``value'' of a layer.",
      "It might be clearer to call this the activation of a layer, as ``value'' could mean weight or activation.",
      "I realize that MAP is a common abbreviation, but I still think that it would be ideal to write out the term in full once."
    ]
  },
  {
    "paper_id": "2010.07893v2",
    "submission_id": "utt-q6jW5_w",
    "submission_title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
    "review_id": "adW4dMKCX7R",
    "input": {
      "title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The algorithm seems to be novel and differs from previous comparable approaches like CoreSet or BADGE. Related work is adequately cited.\n- The authors show empirically that their algorithm, Cluster-Margin, is both more efficient (O(nlog(n) vs O(n\u221a(n)) than CoreSet and BADGE in practice and more effective.\n- The algorithm clearly outperforms CoreSet, BADGE, Margin and Random on the Open Images dataset. The algorithm requires 29% less labels than the second-best model in the 100k batch-size setting and 60% less labels in the 1M batch-size setting to achieve the same result (mAP).\n- Cluster-Margin also outperforms all other methods on CIFAR10, CIFAR100 and obtains a similar performance on SVHN.\n- The authors establish a theoretical guarantee for the Cluster-MarginV algorithm and show that those results hold for the Cluster-Margin algorithm in specific settings.\n- The Cluster-MarginV algorithm has a label complexity bound which improves over the Margin algorithm by a factor beta.\n- This improvement is possible, under specific hypotheses like an optimal volume-based sampler, when the dimensionality of the embedding space is small or when the batch size k is large.\n- The optimal volume-based sampler is approximately equivalent to the Cluster-Margin algorithm.\n- log(k) is an upper bound on the improvement of query complexity for any sampler.\n- The authors are aware and mention that their theoretical results are initial and that equating volume based samplers and the Cluster-Margin algorithm is an open research question.\n- The paper is very clear and well organized. The authors detail the hyper-parameters and compute details used for the experiments. The Cluster-Margin algorithm is also explained in detail.\n- The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods. The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm.",
    "review_points_list": [
      "The algorithm seems to be novel and differs from previous comparable approaches like CoreSet or BADGE. Related work is adequately cited.",
      "The authors show empirically that their algorithm, Cluster-Margin, is both more efficient (O(nlog(n) vs O(n\u221a(n)) than CoreSet and BADGE in practice and more effective.",
      "The algorithm clearly outperforms CoreSet, BADGE, Margin and Random on the Open Images dataset. The algorithm requires 29% less labels than the second-best model in the 100k batch-size setting and 60% less labels in the 1M batch-size setting to achieve the same result (mAP).",
      "Cluster-Margin also outperforms all other methods on CIFAR10, CIFAR100 and obtains a similar performance on SVHN.",
      "The authors establish a theoretical guarantee for the Cluster-MarginV algorithm and show that those results hold for the Cluster-Margin algorithm in specific settings.",
      "The Cluster-MarginV algorithm has a label complexity bound which improves over the Margin algorithm by a factor beta.",
      "This improvement is possible, under specific hypotheses like an optimal volume-based sampler, when the dimensionality of the embedding space is small or when the batch size k is large.",
      "The optimal volume-based sampler is approximately equivalent to the Cluster-Margin algorithm.",
      "log(k) is an upper bound on the improvement of query complexity for any sampler.",
      "The authors are aware and mention that their theoretical results are initial and that equating volume based samplers and the Cluster-Margin algorithm is an open research question.",
      "The paper is very clear and well organized. The authors detail the hyper-parameters and compute details used for the experiments. The Cluster-Margin algorithm is also explained in detail.",
      "The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods. The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm."
    ]
  },
  {
    "paper_id": "2106.06707v1",
    "submission_id": "yGKklt8wyV",
    "submission_title": "Graph Neural Networks with Local Graph  Parameters",
    "review_id": "K2fsBXjbyxr",
    "input": {
      "title": "Graph Neural Networks with Local Graph  Parameters",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This work fixed the issues as they mentioned in the last submission.\n- The overall quality of the current version is solid.\n- It can deliver the main idea of F-MPNN method.\n- This work can inherit the merits from arbitrary GNNs which makes the value of this method more general.\n- This work still has typos and needs further proof-read.\n- For example, P2 line 51\n- For the main part, as a resubmission from ICML, this work fixed the issues as they mentioned in the last submission.\n- The overall quality of the current version is solid and it can deliver the main idea of F-MPNN method.\n- And this work can inherit the merits from arbitrary GNNs which makes the value of this method more general\n- For the minor part, this work still has typos and need further proof-read.\n- For example, P2 line 51.",
    "review_points_list": [
      "This work fixed the issues as they mentioned in the last submission.",
      "The overall quality of the current version is solid.",
      "It can deliver the main idea of F-MPNN method.",
      "This work can inherit the merits from arbitrary GNNs which makes the value of this method more general.",
      "This work still has typos and needs further proof-read.",
      "For example, P2 line 51",
      "For the main part, as a resubmission from ICML, this work fixed the issues as they mentioned in the last submission.",
      "The overall quality of the current version is solid and it can deliver the main idea of F-MPNN method.",
      "And this work can inherit the merits from arbitrary GNNs which makes the value of this method more general",
      "For the minor part, this work still has typos and need further proof-read.",
      "For example, P2 line 51."
    ]
  },
  {
    "paper_id": "2106.06707v1",
    "submission_id": "yGKklt8wyV",
    "submission_title": "Graph Neural Networks with Local Graph  Parameters",
    "review_id": "Bq49a6zwY1",
    "input": {
      "title": "Graph Neural Networks with Local Graph  Parameters",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Although injecting structural features in the input of GNNs is common in the GNN community, usually, the theoretical justifications are limited in showing improvements over the 1-WL, while sometimes the structural features are chosen in an ad-hoc way.\n- However, this paper goes much deeper in the theoretical justifications showing that homomorphisms allow a more accurate characterisation of expressivity,\n- while most importantly, using the observation that different types of subgraph counting (motifs, graphlets, homomorphisms\u2026) can be derived from one another via linear combinations (e.g., see  Borgs et al., \u201cCounting Graph Homomorphisms\u201d Topics in Discrete Mathematics, 2006), they motivate why homomorphisms can be an elegant and unifying framework.\n- I must admit that it is a bit hard to follow for non-experts due to the utilisation of many specialised notions in graph theory, so there is room for improvement in that respect.\n- Specific comments follow.\n- The paper addresses a number of open research questions described in GSNs (although not directly applicable to GSNs, the corresponding results on $\", ",
    "review_points_list": [
      "Although injecting structural features in the input of GNNs is common in the GNN community, usually, the theoretical justifications are limited in showing improvements over the 1-WL, while sometimes the structural features are chosen in an ad-hoc way.",
      "However, this paper goes much deeper in the theoretical justifications showing that homomorphisms allow a more accurate characterisation of expressivity,",
      "while most importantly, using the observation that different types of subgraph counting (motifs, graphlets, homomorphisms\u2026) can be derived from one another via linear combinations (e.g., see  Borgs et al., \u201cCounting Graph Homomorphisms\u201d Topics in Discrete Mathematics, 2006), they motivate why homomorphisms can be an elegant and unifying framework.",
      "I must admit that it is a bit hard to follow for non-experts due to the utilisation of many specialised notions in graph theory, so there is room for improvement in that respect.",
      "Specific comments follow.",
      "The paper addresses a number of open research questions described in GSNs (although not directly applicable to GSNs, the corresponding results on $\", "
    ]
  },
  {
    "paper_id": "2106.06707v1",
    "submission_id": "yGKklt8wyV",
    "submission_title": "Graph Neural Networks with Local Graph  Parameters",
    "review_id": "O7kk84cNpCi",
    "input": {
      "title": "Graph Neural Networks with Local Graph  Parameters",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The proposed framework successfully unifies several existing extensions of MPNNs with solid theoretical analysis.\n- The connections and comparisons between the proposed F-MPNNs and higher-order MPNNs characterize the expressive power of related models with more insights and details.\n- The effects of different selection of patterns on the expressive power are also investigated, which helps to better understand the power of the proposed model.\n- The experiments are done with broad coverage of models, datasets, and tasks to evaluate the performance of the proposed method.\n- The results are discussed in detail and the promising performance of the proposed methods is validated clearly.\n- The paper is overall well organized.\n- The unification of several extensions is a significant achievement, but the impact of this unification on the field of MPNNs needs to be further explored.",
    "review_points_list": [
      "The proposed framework successfully unifies several existing extensions of MPNNs with solid theoretical analysis.",
      "The connections and comparisons between the proposed F-MPNNs and higher-order MPNNs characterize the expressive power of related models with more insights and details.",
      "The effects of different selection of patterns on the expressive power are also investigated, which helps to better understand the power of the proposed model.",
      "The experiments are done with broad coverage of models, datasets, and tasks to evaluate the performance of the proposed method.",
      "The results are discussed in detail and the promising performance of the proposed methods is validated clearly.",
      "The paper is overall well organized.",
      "The unification of several extensions is a significant achievement, but the impact of this unification on the field of MPNNs needs to be further explored."
    ]
  },
  {
    "paper_id": "2106.06707v1",
    "submission_id": "yGKklt8wyV",
    "submission_title": "Graph Neural Networks with Local Graph  Parameters",
    "review_id": "qSX4jGaJXy_",
    "input": {
      "title": "Graph Neural Networks with Local Graph  Parameters",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- It is a smart idea to utilize the design of the F-WL-test to design the F-MPNN, so that this type of GNN can enjoy the theoretical guarantee of distinguishing power as what the F-WL-test has.\n- This method is less expensive than some higher-order MPNN in terms of the complexity in each iteration.\n- The proofs heavily depend on existing works but are not suitably mentioned in the main text.\n- I think it is totally fine to make a statement in the main text because a novel combination of existing proof techniques is also valuable.\n- As already mentioned by the authors, in practice it is hard to pre-select a suitable set of patterns since they cannot be learned by gradients.\n- The theoretical results in Sec 4 do provide certain information but they are not enough to serve as practical guidance since the search space is just way too large.\n- Experiments can be conducted more rigorously.\n- By adding the patterns, the dimension of the augmented node features will become larger and the model will contain more parameters.\n- When making the comparison, have all models used the same feature dimension and the same number of parameters?\n- This should be controlled to exclude the effect of using a larger model.\n- Why results of ISO are missing in Table 2 and Table 3?\n- If one also searches the best performing pattern for ISO using the same computing cost as the search for F-MPNN, what will be the best performing accuracy for ISO on PATTERN and COLLAB?\n- Except for ISO, there are several other works that also augment the initial feature with precomputed node features.\n- Some simple ones include random feature [1], port numbering / weak-coloring feature [2], local matrix feature [3].\n- I noticed that these three are also cited in related work.\n- It will be better to see a comparison to these simple augmented approaches.\n- Minor question: The hyperparameters of the GNNs are not described.\n- What's the hidden size? Number of hops?\n- Will different choices of these hyperparameters affect the improvement from augmenting the features?\n- Although it is a one-time preprocessing stage, how is the complexity?\n- This paper provides theoretical guarantees for the proposed F-MPNN in its distinguishing power, which is valuable.\n- However, since the theory is insufficient to provide practical guidance for choosing a suitable set of F patterns, I think the empirical experiments become important to support the claims in this paper.\n- Therefore, my main concern is that the experiments are not convincing enough (see details above).\n- This paper will be a very nice paper if additional experiments with convincing results are provided.",
    "review_points_list": [
      "It is a smart idea to utilize the design of the F-WL-test to design the F-MPNN, so that this type of GNN can enjoy the theoretical guarantee of distinguishing power as what the F-WL-test has.",
      "This method is less expensive than some higher-order MPNN in terms of the complexity in each iteration.",
      "The proofs heavily depend on existing works but are not suitably mentioned in the main text.",
      "I think it is totally fine to make a statement in the main text because a novel combination of existing proof techniques is also valuable.",
      "As already mentioned by the authors, in practice it is hard to pre-select a suitable set of patterns since they cannot be learned by gradients.",
      "The theoretical results in Sec 4 do provide certain information but they are not enough to serve as practical guidance since the search space is just way too large.",
      "Experiments can be conducted more rigorously.",
      "By adding the patterns, the dimension of the augmented node features will become larger and the model will contain more parameters.",
      "When making the comparison, have all models used the same feature dimension and the same number of parameters?",
      "This should be controlled to exclude the effect of using a larger model.",
      "Why results of ISO are missing in Table 2 and Table 3?",
      "If one also searches the best performing pattern for ISO using the same computing cost as the search for F-MPNN, what will be the best performing accuracy for ISO on PATTERN and COLLAB?",
      "Except for ISO, there are several other works that also augment the initial feature with precomputed node features.",
      "Some simple ones include random feature [1], port numbering / weak-coloring feature [2], local matrix feature [3].",
      "I noticed that these three are also cited in related work.",
      "It will be better to see a comparison to these simple augmented approaches.",
      "Minor question: The hyperparameters of the GNNs are not described.",
      "What's the hidden size? Number of hops?",
      "Will different choices of these hyperparameters affect the improvement from augmenting the features?",
      "Although it is a one-time preprocessing stage, how is the complexity?",
      "This paper provides theoretical guarantees for the proposed F-MPNN in its distinguishing power, which is valuable.",
      "However, since the theory is insufficient to provide practical guidance for choosing a suitable set of F patterns, I think the empirical experiments become important to support the claims in this paper.",
      "Therefore, my main concern is that the experiments are not convincing enough (see details above).",
      "This paper will be a very nice paper if additional experiments with convincing results are provided."
    ]
  },
  {
    "paper_id": "2111.04915v1",
    "submission_id": "wJXWzCsGlZw",
    "submission_title": "Practical, Provably-Correct Interactive Learning in the Realizable Setting: The Power of True Believers",
    "review_id": "9qBwwVEgL2q",
    "input": {
      "title": "Practical, Provably-Correct Interactive Learning in the Realizable Setting: The Power of True Believers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper positions its main contribution in a general framework for computationally efficient interactive learning, while making a strong assumption that the data are realizable.\n- It is okay to confine to realizable setting but I am not convinced of the motivating example.\n- The starting point is that for learning of threshold over a bounded interval, agnostic learner is inferior to realizable learner; thus authors decide to stick with the latter.\n- I agree this example is true but it turns out that such motivation is superficial and seems dismiss tremendous efforts in agnostic learning in the past decades.\n- Furthermore, the nice query complexity shown in the example seems just a singular case; same result does not hold for more general and practical setup such as halfspace in R^d.\n- I also found the presentation is confusing.\n- It is claimed to develop a general framework for active classification, but neither Algorithm 1 nor 2 is committed to it.\n- For best arm identification, the dependence on sampling probability of f^* is non-standard and is of little interest since it is unknown.\n- It is unfair (in fact, misleading) to compare Theorem 5 to [11] because Theorem 5 follows from an inefficient algorithm.\n- What you can really do is to compare Theorem 4 to [11].\n- However, in this case, the above issue persists so there is no evidence that your algorithm outperforms.\n- My overall feeling is that the paper is too dense to follow, and the major technical contribution is vague.\n- While it is interesting to see results for general loss functions, it leads an unknown quantity (i.e. sampling probability) in sample complexity so the bound is of little interest.\n- Authors addressed most of my concerns.\n- Authors also promise to state the results (i.e. 'minimax optimal') in a more precise way.\n- I am thus happy to raise my score.",
    "review_points_list": [
      "The paper positions its main contribution in a general framework for computationally efficient interactive learning, while making a strong assumption that the data are realizable.",
      "It is okay to confine to realizable setting but I am not convinced of the motivating example.",
      "The starting point is that for learning of threshold over a bounded interval, agnostic learner is inferior to realizable learner; thus authors decide to stick with the latter.",
      "I agree this example is true but it turns out that such motivation is superficial and seems dismiss tremendous efforts in agnostic learning in the past decades.",
      "Furthermore, the nice query complexity shown in the example seems just a singular case; same result does not hold for more general and practical setup such as halfspace in R^d.",
      "I also found the presentation is confusing.",
      "It is claimed to develop a general framework for active classification, but neither Algorithm 1 nor 2 is committed to it.",
      "For best arm identification, the dependence on sampling probability of f^* is non-standard and is of little interest since it is unknown.",
      "It is unfair (in fact, misleading) to compare Theorem 5 to [11] because Theorem 5 follows from an inefficient algorithm.",
      "What you can really do is to compare Theorem 4 to [11].",
      "However, in this case, the above issue persists so there is no evidence that your algorithm outperforms.",
      "My overall feeling is that the paper is too dense to follow, and the major technical contribution is vague.",
      "While it is interesting to see results for general loss functions, it leads an unknown quantity (i.e. sampling probability) in sample complexity so the bound is of little interest.",
      "Authors addressed most of my concerns.",
      "Authors also promise to state the results (i.e. 'minimax optimal') in a more precise way.",
      "I am thus happy to raise my score."
    ]
  },
  {
    "paper_id": "2111.04915v1",
    "submission_id": "wJXWzCsGlZw",
    "submission_title": "Practical, Provably-Correct Interactive Learning in the Realizable Setting: The Power of True Believers",
    "review_id": "RKj0aR2dkMk",
    "input": {
      "title": "Practical, Provably-Correct Interactive Learning in the Realizable Setting: The Power of True Believers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Years ago, many empirical experiments showed little improvement over random sampling which led to the suspicion that active learning doesn't work.\n- It has since been demonstrated that active learning algorithms do exist which learn at an exponential rate.\n- This paper continues that line of work, showing that realizability is a key ingredient in active learning.\n- Overall the paper is well structured, but notation dense.\n- It would be easier to read if more equations were followed with 'in words' or (approximate) conceptual descriptions.\n- Previews such as 'now we prove such and such result, the approach is to do this and that' would increase readability.\n- I recognize however, these additions are likely not feasible due to the length constraints.\n- The empirical experiments are a little light.\n- Again, length constraints likely inhibit further experiments.",
    "review_points_list": [
      "Years ago, many empirical experiments showed little improvement over random sampling which led to the suspicion that active learning doesn't work.",
      "It has since been demonstrated that active learning algorithms do exist which learn at an exponential rate.",
      "This paper continues that line of work, showing that realizability is a key ingredient in active learning.",
      "Overall the paper is well structured, but notation dense.",
      "It would be easier to read if more equations were followed with 'in words' or (approximate) conceptual descriptions.",
      "Previews such as 'now we prove such and such result, the approach is to do this and that' would increase readability.",
      "I recognize however, these additions are likely not feasible due to the length constraints.",
      "The empirical experiments are a little light.",
      "Again, length constraints likely inhibit further experiments."
    ]
  },
  {
    "paper_id": "2111.04915v1",
    "submission_id": "wJXWzCsGlZw",
    "submission_title": "Practical, Provably-Correct Interactive Learning in the Realizable Setting: The Power of True Believers",
    "review_id": "g_rzykdgm4",
    "input": {
      "title": "Practical, Provably-Correct Interactive Learning in the Realizable Setting: The Power of True Believers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Interactive learning consists of gradually querying a set of points so as to obtain information about associated rewards/losses.\n- Different problems correspond to different goals to be reached with the queries.\n- In active learning the rewards/losses are binary labels, and the goal is to learn the function from a given class that best approximates the labels.\n- The paper consider the realizable setting where the rewards satisfy y_i=f(x_i) for an unknown function f belonging to a known family.\n- For best arm identification, reference [11] gives a nearly tight query complexity bound (without an efficient algorithm).\n- One of the positive results of the present paper is an algorithm that is efficient under certain somewhat awkward assumptions.\n- The paper also shows that this is in some way unavoidable (this is the content of Propositions 1 through 3).\n- As far as I can tell, this is a correct and nontrivial contribution with some interest.\n- The main technical idea is replacing the ''version space'' with a convex set.\n- The authors approach is to replace this version space with a convex set in function space that consists of an intersection of polynomially many semispaces with the function class.\n- Using sampling methods for these convex sets, this strategy can be made to work, and one may show that the sets quickly shrink until only one candidate function is left.\n- The results of this paper seem novel and important enough to warrant publication at NeurIPS.\n- In particular, the problems posed here address natural open questions raised in [11], as well as some new problems.\n- The lower bounds are also interesting, and experimental results against Bayesian optimization methods seem promising.\n- However, the exposition leaves a lot to be desired.\n- The authors are a bit too cavalier when they imply that sampling in convex bodies is always 'efficient'.\n- Some of those issues should be discussed in the appendix.\n- In particular, it would be nice to understand the complexity for at least one of the problems under consideration.\n- Similarly, actual query complexity bounds are never fully worked out for the algorithm Grails.\n- It'd be nice to understand the query complexity of the proposed methods in some of the examples considered in the appendix.\n- To sum up the above suggestions, it would be nice to see a full result for some simple case of the problems considered in the paper.\n- In this example, all relevant parameters and the complexity upper bound should be worked out.\n- In section 2, the authors mention the realizable setting, and then define Active Classification without explaining what it looks like in the realizable setting.\n- The assumption that $\text{\texttt{F}}_{\text{\texttt{R}}}$ is obtained from $\text{\texttt{R}}$ by discretization does not seem very natural and is hard to swallow.\n- The one case where it makes a lot of sense is that of active classification, because $\text{\texttt{Y}}=\text{\texttt{1,\texttttt{-1}}}$ in that case.\n- Unfortunately, that case is relegated to the appendix.\n- Some of the halfspaces considered in the text are open.",
    "review_points_list": [
      "Interactive learning consists of gradually querying a set of points so as to obtain information about associated rewards/losses.",
      "Different problems correspond to different goals to be reached with the queries.",
      "In active learning the rewards/losses are binary labels, and the goal is to learn the function from a given class that best approximates the labels.",
      "The paper consider the realizable setting where the rewards satisfy y_i=f(x_i) for an unknown function f belonging to a known family.",
      "For best arm identification, reference [11] gives a nearly tight query complexity bound (without an efficient algorithm).",
      "One of the positive results of the present paper is an algorithm that is efficient under certain somewhat awkward assumptions.",
      "The paper also shows that this is in some way unavoidable (this is the content of Propositions 1 through 3).",
      "As far as I can tell, this is a correct and nontrivial contribution with some interest.",
      "The main technical idea is replacing the ''version space'' with a convex set.",
      "The authors approach is to replace this version space with a convex set in function space that consists of an intersection of polynomially many semispaces with the function class.",
      "Using sampling methods for these convex sets, this strategy can be made to work, and one may show that the sets quickly shrink until only one candidate function is left.",
      "The results of this paper seem novel and important enough to warrant publication at NeurIPS.",
      "In particular, the problems posed here address natural open questions raised in [11], as well as some new problems.",
      "The lower bounds are also interesting, and experimental results against Bayesian optimization methods seem promising.",
      "However, the exposition leaves a lot to be desired.",
      "The authors are a bit too cavalier when they imply that sampling in convex bodies is always 'efficient'.",
      "Some of those issues should be discussed in the appendix.",
      "In particular, it would be nice to understand the complexity for at least one of the problems under consideration.",
      "Similarly, actual query complexity bounds are never fully worked out for the algorithm Grails.",
      "It'd be nice to understand the query complexity of the proposed methods in some of the examples considered in the appendix.",
      "To sum up the above suggestions, it would be nice to see a full result for some simple case of the problems considered in the paper.",
      "In this example, all relevant parameters and the complexity upper bound should be worked out.",
      "In section 2, the authors mention the realizable setting, and then define Active Classification without explaining what it looks like in the realizable setting.",
      "The assumption that $\text{\texttt{F}}_{\text{\texttt{R}}}$ is obtained from $\text{\texttt{R}}$ by discretization does not seem very natural and is hard to swallow.",
      "The one case where it makes a lot of sense is that of active classification, because $\text{\texttt{Y}}=\text{\texttt{1,\texttttt{-1}}}$ in that case.",
      "Unfortunately, that case is relegated to the appendix.",
      "Some of the halfspaces considered in the text are open."
    ]
  },
  {
    "paper_id": "2204.01726v1",
    "submission_id": "x6z8J_17LP3",
    "submission_title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
    "review_id": "_jGRzXgrGq",
    "input": {
      "title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- + The idea of adding an audio-visual attention module to iteratively refine the representation seems to be novel in the lip2speech field.\n- The motivation for designing the VCA module seems reasonable to me.\n- + The performance of the proposed method looks promising.\n- Different scenarios, including constrained-speaker, unseen speaker and multiple-speaker are evaluated, which demonstrates the generality of the proposed pipeline.\n- The ablation study is also performed to illustrate the effectiveness of each component.\n- + The code is submitted for reproduction.\n- + The paper writing is well organised and easy to follow.\n- - In VCA module, why use speech representation as the query and use visual clues as the key and value?\n- - For the pipeline design, why choose the number of generators (n) to 3? How will n affect the whole performance?\n- - It is difficult for me to understand why the Audio-Visual Attention mechanism can generate better audio representations. Can the authors explain this more detailedly and intuitively?\n- - The performance of audio-visual recognition models on the generated voice can also be a helpful evaluation metric.\n- - I checked the demo in the supplemental material, but the results of various methods (author's vs baselines) doesn't seem too much different to me.\n- Overall, the idea of this paper can be novel in the field of lip2speech, and the motivation and model design looks reasonable.\n- The proposed method is evaluated extensively and the results are superior.\n- I will vote for a marginally above.",
    "review_points_list": [
      "+ The idea of adding an audio-visual attention module to iteratively refine the representation seems to be novel in the lip2speech field.",
      "The motivation for designing the VCA module seems reasonable to me.",
      "+ The performance of the proposed method looks promising.",
      "Different scenarios, including constrained-speaker, unseen speaker and multiple-speaker are evaluated, which demonstrates the generality of the proposed pipeline.",
      "The ablation study is also performed to illustrate the effectiveness of each component.",
      "+ The code is submitted for reproduction.",
      "+ The paper writing is well organised and easy to follow.",
      "- In VCA module, why use speech representation as the query and use visual clues as the key and value?",
      "- For the pipeline design, why choose the number of generators (n) to 3? How will n affect the whole performance?",
      "- It is difficult for me to understand why the Audio-Visual Attention mechanism can generate better audio representations. Can the authors explain this more detailedly and intuitively?",
      "- The performance of audio-visual recognition models on the generated voice can also be a helpful evaluation metric.",
      "- I checked the demo in the supplemental material, but the results of various methods (author's vs baselines) doesn't seem too much different to me.",
      "Overall, the idea of this paper can be novel in the field of lip2speech, and the motivation and model design looks reasonable.",
      "The proposed method is evaluated extensively and the results are superior.",
      "I will vote for a marginally above."
    ]
  },
  {
    "paper_id": "2204.01726v1",
    "submission_id": "x6z8J_17LP3",
    "submission_title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
    "review_id": "mKsdjDhUU9",
    "input": {
      "title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This is a very well written paper (although it does have a number of minor typographical and grammatical errors; few are listed below) which is generally well motivated and easy to follow.\n- The experimental section, in particular, is comprehensive and demonstrates that the proposed system outperforms previous works with larger gains in unseen and multi-speaker settings (which are more challenging to begin with).\n- I had a few minor comments which if resolved would strengthen the paper further.\n- Ablation results in Table 1 and Table 3: It would be good to include WER numbers as well for the ablation study in Table 1 and the results in Table 3 to show fidelity in addition to intelligibility and quality, since it is also useful to see how well the reconstructed speech matches the ground-truth audio.\n- Captured global context: A point that is made in the paper, and in Table 1, is that the use of the global context helps resolve ambiguities.\n- It would be interesting to present the results of some error analysis to show how the global context helps improve performance (WER results in Table 1, would help clarify this somewhat as well).\n- Length of test videos: In Section 4.2, the authors mention that during training the videos are segmented to smaller chunks, but during inference the authors evaluate the full videos.\n- I was curious if the authors have performed any experiments to study the overall performance as a function of video length.\n- Are the global features still useful for very long videos, for example?\n- It would be useful to mention some statistics about the test video lengths for the various datasets (perhaps in a footnote if space is limited).\n- Intelligibility and Quality Metrics: Minor comment: In section 4.3, it might be useful to mention a couple of sentences about the STOI/ESTOI/PESQ metrics to help the reader understand what these capture, whether higher/lower are better and what level of differences are generally significant.\n- Typographical and Grammatical Errors:\n- Page 3, Line 98: \u201c\u2026 contexture design \u2026 \u201d\n- Page 3, Section 3: \u201c\u2026 as the input silence video.\u201d --> \u201c\u2026 as the input silent video.\u201d\n- Page 3, Section 3: \u201c\u2026 is designed to have 4 times longer \u2026\u201d --> \u201c\u2026 is designed to be 4 times longer \u2026\u201d\n- Page 3, Section 3: \u201c\u2026 dealing the mel-spectrogram \u2026 \u201d --> \u201c\u2026 treating the mel-spectrogram \u2026\u201d\n- Eqn 6: It might be better to use the notation L_{g}^{i} instead of L_g\n- Page 6, Line 221: \u201c\u2026 four times the frame of the \u2026\u201d --> \u201c\u2026 four times the frame rate of the \u2026\u201d\n- Page 8, Line 287: \u201cEven the VCA-GAN is \u2026\u201d --> \u201cEven though the VCA-GAN is \u2026\u201d",
    "review_points_list": [
      "This is a very well written paper (although it does have a number of minor typographical and grammatical errors; few are listed below) which is generally well motivated and easy to follow.",
      "The experimental section, in particular, is comprehensive and demonstrates that the proposed system outperforms previous works with larger gains in unseen and multi-speaker settings (which are more challenging to begin with).",
      "I had a few minor comments which if resolved would strengthen the paper further.",
      "Ablation results in Table 1 and Table 3: It would be good to include WER numbers as well for the ablation study in Table 1 and the results in Table 3 to show fidelity in addition to intelligibility and quality, since it is also useful to see how well the reconstructed speech matches the ground-truth audio.",
      "Captured global context: A point that is made in the paper, and in Table 1, is that the use of the global context helps resolve ambiguities.",
      "It would be interesting to present the results of some error analysis to show how the global context helps improve performance (WER results in Table 1, would help clarify this somewhat as well).",
      "Length of test videos: In Section 4.2, the authors mention that during training the videos are segmented to smaller chunks, but during inference the authors evaluate the full videos.",
      "I was curious if the authors have performed any experiments to study the overall performance as a function of video length.",
      "Are the global features still useful for very long videos, for example?",
      "It would be useful to mention some statistics about the test video lengths for the various datasets (perhaps in a footnote if space is limited).",
      "Intelligibility and Quality Metrics: Minor comment: In section 4.3, it might be useful to mention a couple of sentences about the STOI/ESTOI/PESQ metrics to help the reader understand what these capture, whether higher/lower are better and what level of differences are generally significant.",
      "Typographical and Grammatical Errors:",
      "Page 3, Line 98: \u201c\u2026 contexture design \u2026 \u201d",
      "Page 3, Section 3: \u201c\u2026 as the input silence video.\u201d --> \u201c\u2026 as the input silent video.\u201d",
      "Page 3, Section 3: \u201c\u2026 is designed to have 4 times longer \u2026\u201d --> \u201c\u2026 is designed to be 4 times longer \u2026\u201d",
      "Page 3, Section 3: \u201c\u2026 dealing the mel-spectrogram \u2026 \u201d --> \u201c\u2026 treating the mel-spectrogram \u2026\u201d",
      "Eqn 6: It might be better to use the notation L_{g}^{i} instead of L_g",
      "Page 6, Line 221: \u201c\u2026 four times the frame of the \u2026\u201d --> \u201c\u2026 four times the frame rate of the \u2026\u201d",
      "Page 8, Line 287: \u201cEven the VCA-GAN is \u2026\u201d --> \u201cEven though the VCA-GAN is \u2026\u201d"
    ]
  },
  {
    "paper_id": "2204.01726v1",
    "submission_id": "x6z8J_17LP3",
    "submission_title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
    "review_id": "s0amb7oyOxU",
    "input": {
      "title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper is about GAN synthesis of speech based on lip images.\n- This is not the first paper to have done this: e.g. 'Video-Driven Speech Reconstruction using Generative Adversarial Networks' Vougioukas1 et al. 2019 https://arxiv.org/pdf/1906.06301.pdf\n- The architectural modification is the use of a hierarchical structure using local and global features.\n- However, only watching short clip-level (i.e., local) lip movements could be challenging to distinguish the homophones.\n- Define \u201chomophones\u201d here?\n- The most common usage of 'homophones' is to refer to words with different written forms but identical spoken forms, like 'their' and 'there'.\n- It is not clear how global level information is helpful in distinguishing these.\n- Line 111: '...global visual context, can provide additional information that alleviates the ambiguity of homophones'\n- Please explain how, perhaps with an example?\n- Line 171 ', which maximizes the cosine similarity between the generated audio features and the given visual features leading  the generated mel-spectrogram to be synced with the input video.'\n- In order to perform the synchronization between visual inputs and audio outputs, the two features mush share a common representation space.\n- Does this choice improve or impair quality?\n- Per Table 1 synchronization gives a very minor win, while the largest improvement comes from the use of visual attention.\n- Table 2: The order of systems in Table2 is unclear.  Why this sorting?\n- Table 1: It would be useful to compare the pretrained ASR (5.83%) to any other system.\n- Without that comparison, this number isn\u2019t meaningful, and probably should be removed.\n- Line 294: \u2018In addition, for the WER, Lip2Wav is measured by using Google API while VCA-GAN is measured using the ASR model pre-trained on LRW dataset, so they cannot be directly comparable.\n- However, it clearly shows that the synthesized speech correctly contains the right words even in the challenging environments.\u2019\n- It is impossible to draw comparisons here, this mismatched evaluation of intelligibility should be removed.\n- It would be better to either evaluate both using the same system, or leave the comparison out.\n- It would be helpful to have some subjective measure of the value of synchronized images and audio.",
    "review_points_list": [
      "This paper is about GAN synthesis of speech based on lip images.",
      "This is not the first paper to have done this: e.g. 'Video-Driven Speech Reconstruction using Generative Adversarial Networks' Vougioukas1 et al. 2019 https://arxiv.org/pdf/1906.06301.pdf",
      "The architectural modification is the use of a hierarchical structure using local and global features.",
      "However, only watching short clip-level (i.e., local) lip movements could be challenging to distinguish the homophones.",
      "Define \u201chomophones\u201d here?",
      "The most common usage of 'homophones' is to refer to words with different written forms but identical spoken forms, like 'their' and 'there'.",
      "It is not clear how global level information is helpful in distinguishing these.",
      "Line 111: '...global visual context, can provide additional information that alleviates the ambiguity of homophones'",
      "Please explain how, perhaps with an example?",
      "Line 171 ', which maximizes the cosine similarity between the generated audio features and the given visual features leading  the generated mel-spectrogram to be synced with the input video.'",
      "In order to perform the synchronization between visual inputs and audio outputs, the two features mush share a common representation space.",
      "Does this choice improve or impair quality?",
      "Per Table 1 synchronization gives a very minor win, while the largest improvement comes from the use of visual attention.",
      "Table 2: The order of systems in Table2 is unclear.  Why this sorting?",
      "Table 1: It would be useful to compare the pretrained ASR (5.83%) to any other system.",
      "Without that comparison, this number isn\u2019t meaningful, and probably should be removed.",
      "Line 294: \u2018In addition, for the WER, Lip2Wav is measured by using Google API while VCA-GAN is measured using the ASR model pre-trained on LRW dataset, so they cannot be directly comparable.",
      "However, it clearly shows that the synthesized speech correctly contains the right words even in the challenging environments.\u2019",
      "It is impossible to draw comparisons here, this mismatched evaluation of intelligibility should be removed.",
      "It would be better to either evaluate both using the same system, or leave the comparison out.",
      "It would be helpful to have some subjective measure of the value of synchronized images and audio."
    ]
  },
  {
    "paper_id": "2204.01726v1",
    "submission_id": "x6z8J_17LP3",
    "submission_title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
    "review_id": "GsOda9XApO",
    "input": {
      "title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Table 1 presents an ablation study in multi-speaker setting on GRID.\n- It is unclear what the difference between the last row of Table 1 and the last row of Table 6, both representing VCA-GAN but with minor performance difference - good to clarify.\n- While the ablation study shows each component contributes to the performance improvement, but the improvement brought by each component is rather small.\n- More specifically, all three components together improve STOI from 0.726 to 0.736 and PESQ from 1.917 to 1.961.\n- This study makes it less clear why the proposed system works well on all three benchmark data sets.\n- Good to synchronise audio and visual information.\n- The three metrics used in the paper, also commonly used in the literature, are unable to show its whole merit since they are audio only metrics.\n- Watching the demo video in supplementary material does not show the audio and visual sequences are synchronised well though.",
    "review_points_list": [
      "Table 1 presents an ablation study in multi-speaker setting on GRID.",
      "It is unclear what the difference between the last row of Table 1 and the last row of Table 6, both representing VCA-GAN but with minor performance difference - good to clarify.",
      "While the ablation study shows each component contributes to the performance improvement, but the improvement brought by each component is rather small.",
      "More specifically, all three components together improve STOI from 0.726 to 0.736 and PESQ from 1.917 to 1.961.",
      "This study makes it less clear why the proposed system works well on all three benchmark data sets.",
      "Good to synchronise audio and visual information.",
      "The three metrics used in the paper, also commonly used in the literature, are unable to show its whole merit since they are audio only metrics.",
      "Watching the demo video in supplementary material does not show the audio and visual sequences are synchronised well though."
    ]
  },
  {
    "paper_id": "2111.01633v2",
    "submission_id": "yaksQCYcRs",
    "submission_title": "Neural Program Generation Modulo Static Analysis",
    "review_id": "WRH9n4VZxC",
    "input": {
      "title": "Neural Program Generation Modulo Static Analysis",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper considers the challenging and important task of conditional program generation, an area of increasing interest at NeurIPS in recent years.\n- The main weakness identified, that neural models of source code usually struggle with generation of long sections of code, is important.\n- Being able to generate long semantically coherent programs is a key challenge in the field, and this paper presents an approach to program generation that deals with this effectively.\n- The method is explained clearly overall, providing background on static analysis with attribute grammars and a clear explanation of the algorithm for using such static analysis during program generation, compatible with a wide class of models.\n- The appendix provides the details that were omitted from the main paper, including how the training data is prepared, the attributes considered and part of the definition of the attribute grammar, how attributes are encoded, and details about the main and baseline methods.\n- A key element of the approach is the attribute grammar defines a linear path through the program, allowing the model to be trained using pre-computed attributes and requiring on-the-fly computation of attributes during inference.\n- This is explained best in the training and inference appendix.\n- A strength of the approach, and an advantage over prior work, is that the method allows for learning constraints and guiding principles from the input corpus, rather than using the static analysis to provide hard constraints.\n- This gives more freedom to the static analysis designer to include attributes that correspond to best-practices or that appear only in certain contexts, without needing to avoid attributes that don't define hard-and-fast rules about programs.\n- Prior work leveraging attribute grammars for program generation used them to enforce hard constraints.\n- A weakness is that the evaluation static checks match those properties provided by the static analysis attribute grammar.\n- I would be interested to see an evaluation indicating whether the method learns to preserve additional properties beyond those directly measured by the static analysis.\n- Perhaps to that end, please clarify what is meant by 50% evidence.\n- Does this refer to full evidence at 50% of nodes, or 50% evidence at each node?\n- How is the 50% selected?\n- Outside of these concerns, the evaluation is solid.\n- The task and baselines are appropriate, the metrics capture properties of interest, and the evidence from the results tells a quite clear story.\n- Including a measure of spread would improve the results section.\n- Line 71: $\times$ accepts programs as inputs, but in the prediction error expression $\textbf{E}_{(X,Y)\text{~}D}[\times(P_\theta(X\text{|}Y), Y)]$ you incorrectly pass $P_\theta(X\text{|}Y)$, rather than a program, as the first input to $\times$.\n- Nit: Line 165 refers to specification Y rather than specification X.\n- Line 765 has a broken reference simply appearing as \"??\".",
    "review_points_list": [
      "The paper considers the challenging and important task of conditional program generation, an area of increasing interest at NeurIPS in recent years.",
      "The main weakness identified, that neural models of source code usually struggle with generation of long sections of code, is important.",
      "Being able to generate long semantically coherent programs is a key challenge in the field, and this paper presents an approach to program generation that deals with this effectively.",
      "The method is explained clearly overall, providing background on static analysis with attribute grammars and a clear explanation of the algorithm for using such static analysis during program generation, compatible with a wide class of models.",
      "The appendix provides the details that were omitted from the main paper, including how the training data is prepared, the attributes considered and part of the definition of the attribute grammar, how attributes are encoded, and details about the main and baseline methods.",
      "A key element of the approach is the attribute grammar defines a linear path through the program, allowing the model to be trained using pre-computed attributes and requiring on-the-fly computation of attributes during inference.",
      "This is explained best in the training and inference appendix.",
      "A strength of the approach, and an advantage over prior work, is that the method allows for learning constraints and guiding principles from the input corpus, rather than using the static analysis to provide hard constraints.",
      "This gives more freedom to the static analysis designer to include attributes that correspond to best-practices or that appear only in certain contexts, without needing to avoid attributes that don't define hard-and-fast rules about programs.",
      "Prior work leveraging attribute grammars for program generation used them to enforce hard constraints.",
      "A weakness is that the evaluation static checks match those properties provided by the static analysis attribute grammar.",
      "I would be interested to see an evaluation indicating whether the method learns to preserve additional properties beyond those directly measured by the static analysis.",
      "Perhaps to that end, please clarify what is meant by 50% evidence.",
      "Does this refer to full evidence at 50% of nodes, or 50% evidence at each node?",
      "How is the 50% selected?",
      "Outside of these concerns, the evaluation is solid.",
      "The task and baselines are appropriate, the metrics capture properties of interest, and the evidence from the results tells a quite clear story.",
      "Including a measure of spread would improve the results section.",
      "Line 71: $\times$ accepts programs as inputs, but in the prediction error expression $\textbf{E}_{(X,Y)\text{~}D}[\times(P_\theta(X\text{|}Y), Y)]$ you incorrectly pass $P_\theta(X\text{|}Y)$, rather than a program, as the first input to $\times$.",
      "Nit: Line 165 refers to specification Y rather than specification X.",
      "Line 765 has a broken reference simply appearing as \"??\"."
    ]
  },
  {
    "paper_id": "2111.01633v2",
    "submission_id": "yaksQCYcRs",
    "submission_title": "Neural Program Generation Modulo Static Analysis",
    "review_id": "jBMJ7Bl-cUO",
    "input": {
      "title": "Neural Program Generation Modulo Static Analysis",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- + A principled formalism and implementation that incorporates semantic information computed by the compiler into neural code generation models.\n- + Clear motivating examples and well-explained decision rationale\n- + Fair comparison to two most relevant baselines, one syntax-only and one attribute-inspired but not explicitly supervised with symbolic attributes.\n- + Evaluation along multiple important dimensions: method generation accuracy, small expression generation accuracy (as in prior work), and individual language rules.\n- + An explicit discussion of limitations is welcome, not discussed often enough in our community.\n- The paper spends significant space on the NSG formalism and on the particular generative model re-used from Murali et al., which forces it to defer its instantiation for Java, choice of attributes, and their encoding to the Appendix.\n- As a result, Section 4 ends abruptly, confusing the reader. The 'design' paragraph helps set up the scene but, I imagine, would not actually be informative to someone without formal methods background, at least without a single concrete example.\n- Similarly, lack of space likely forced the authors to leave out Appendix H. It provides a valuable complementary perspective on the respective strengths and weaknesses of training approaches, and a quick summary of it would be welcome in the paper.\n- Characterizing [7] as 'next token prediction' is unfair, as that model generates whole ASTs and \u2013 just like this work \u2013 supervised with its production rule decisions at every step.\n- The difference is in (a) size of the target AST, (b) the amount of available in-method context for conditioning and for copy.\n- Phrasing aside, it would be interesting to see how [7] fairs against NSG for _in-method_ expression completion (medium horizon). I imagine NSG would still outperform, as it leverages more explicit conditioning on attributes rather than their neural abstractions.\n- The general line of reasoning that 'semantics are hard to learn from syntax' is not wrong, but: \u2013 It largely depends on the size of the model and its pretraining data, with improvement following a power law.\n- It further improves with multi-language training as the model pick ups non-formalizable yet similarly expressed cross-language cues.\n- Both were shown empirically by [Codex], concurrently with this submission. This does not diminish the paper's contribution, just shows that the two directions keep improving and could further benefit each other.\n- A better Transformer baseline would be [GPT-NEO] or [GPT-J]. Both are pretrained on The Pile corpus, which includes a big portion of Github and more natural language.\n- The former was shown to be a strong baseline for code generation against Codex and in the [APPS paper], with or without finetuning.\n- L404-405: There are many methods to integrate tree structure and generative Transformers. See e.g.: \u2013 Scholak et al. 'DuoRAT: Towards Simpler Text-to-SQL Models', NAACL 2021.\n- Sun et al. 'TreeGen: A Tree-Based Transformer Architecture for Code Generation', AAAI 2020.\n- How much of attribute vocabulary could be shared cross-language? Most popular imperative languages could implement a compiler frontend that computes a version of the attributes in Appendix D.\n- Could this help us effectively train cross-language and cross the gap to language model training?\n- Is the NSG model robust to presence or absence of attributes? For example, suppose the code/linker is in a broken state so that type-based attributes cannot be properly resolved.\n- Does the model break down completely or can it still rely on other features?\n- Follow-up to previous question: In addition to conditioning on attributes, the model could be trained to predict them as an auxiliary task.\n- Multitask learning has improved performance or accelerated training in many domains, code could work just as well.\n- L47: 'whose with grammar'",
    "review_points_list": [
      "+ A principled formalism and implementation that incorporates semantic information computed by the compiler into neural code generation models.",
      "+ Clear motivating examples and well-explained decision rationale",
      "+ Fair comparison to two most relevant baselines, one syntax-only and one attribute-inspired but not explicitly supervised with symbolic attributes.",
      "+ Evaluation along multiple important dimensions: method generation accuracy, small expression generation accuracy (as in prior work), and individual language rules.",
      "+ An explicit discussion of limitations is welcome, not discussed often enough in our community.",
      "The paper spends significant space on the NSG formalism and on the particular generative model re-used from Murali et al., which forces it to defer its instantiation for Java, choice of attributes, and their encoding to the Appendix.",
      "As a result, Section 4 ends abruptly, confusing the reader. The 'design' paragraph helps set up the scene but, I imagine, would not actually be informative to someone without formal methods background, at least without a single concrete example.",
      "Similarly, lack of space likely forced the authors to leave out Appendix H. It provides a valuable complementary perspective on the respective strengths and weaknesses of training approaches, and a quick summary of it would be welcome in the paper.",
      "Characterizing [7] as 'next token prediction' is unfair, as that model generates whole ASTs and \u2013 just like this work \u2013 supervised with its production rule decisions at every step.",
      "The difference is in (a) size of the target AST, (b) the amount of available in-method context for conditioning and for copy.",
      "Phrasing aside, it would be interesting to see how [7] fairs against NSG for _in-method_ expression completion (medium horizon). I imagine NSG would still outperform, as it leverages more explicit conditioning on attributes rather than their neural abstractions.",
      "The general line of reasoning that 'semantics are hard to learn from syntax' is not wrong, but: \u2013 It largely depends on the size of the model and its pretraining data, with improvement following a power law.",
      "It further improves with multi-language training as the model pick ups non-formalizable yet similarly expressed cross-language cues.",
      "Both were shown empirically by [Codex], concurrently with this submission. This does not diminish the paper's contribution, just shows that the two directions keep improving and could further benefit each other.",
      "A better Transformer baseline would be [GPT-NEO] or [GPT-J]. Both are pretrained on The Pile corpus, which includes a big portion of Github and more natural language.",
      "The former was shown to be a strong baseline for code generation against Codex and in the [APPS paper], with or without finetuning.",
      "L404-405: There are many methods to integrate tree structure and generative Transformers. See e.g.: \u2013 Scholak et al. 'DuoRAT: Towards Simpler Text-to-SQL Models', NAACL 2021.",
      "Sun et al. 'TreeGen: A Tree-Based Transformer Architecture for Code Generation', AAAI 2020.",
      "How much of attribute vocabulary could be shared cross-language? Most popular imperative languages could implement a compiler frontend that computes a version of the attributes in Appendix D.",
      "Could this help us effectively train cross-language and cross the gap to language model training?",
      "Is the NSG model robust to presence or absence of attributes? For example, suppose the code/linker is in a broken state so that type-based attributes cannot be properly resolved.",
      "Does the model break down completely or can it still rely on other features?",
      "Follow-up to previous question: In addition to conditioning on attributes, the model could be trained to predict them as an auxiliary task.",
      "Multitask learning has improved performance or accelerated training in many domains, code could work just as well.",
      "L47: 'whose with grammar'"
    ]
  },
  {
    "paper_id": "2202.12150v1",
    "submission_id": "xJYek6zantM",
    "submission_title": "Tighter Expected Generalization Error Bounds via Wasserstein Distance",
    "review_id": "tC63ISQa_zS",
    "input": {
      "title": "Tighter Expected Generalization Error Bounds via Wasserstein Distance",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The literature review is very well done, which helps to put the paper contribution in perspective.\n- While no new techniques are introduced, the authors made a good job in refining and combining different existing techniques.\n- As far as the reviewer is concerned, it seems like the bounds presented in this paper are close to the best that can be done with existing techniques at the proposed level of generality.\n- Applying these bounds beyond toy examples still requires some effort (probably a big one), but the paper reveals a clear path to do so.\n- Overall, this paper represents a solid step towards practical generalization bounds.\n- It would be helpful for the reader if the authors emphasize, perhaps in a graphical manner, the taxonomy of current generalization bounds in terms of performance.\n- Bobkov-G\u00f6tze's theorem has a similar role to the HB inequality, although the former is not presented in the preliminaries.\n- Perhaps it is useful to add the BG theorem to the preliminaries.",
    "review_points_list": [
      "The literature review is very well done, which helps to put the paper contribution in perspective.",
      "While no new techniques are introduced, the authors made a good job in refining and combining different existing techniques.",
      "As far as the reviewer is concerned, it seems like the bounds presented in this paper are close to the best that can be done with existing techniques at the proposed level of generality.",
      "Applying these bounds beyond toy examples still requires some effort (probably a big one), but the paper reveals a clear path to do so.",
      "Overall, this paper represents a solid step towards practical generalization bounds.",
      "It would be helpful for the reader if the authors emphasize, perhaps in a graphical manner, the taxonomy of current generalization bounds in terms of performance.",
      "Bobkov-G\u00f6tze's theorem has a similar role to the HB inequality, although the former is not presented in the preliminaries.",
      "Perhaps it is useful to add the BG theorem to the preliminaries."
    ]
  },
  {
    "paper_id": "2202.12150v1",
    "submission_id": "xJYek6zantM",
    "submission_title": "Tighter Expected Generalization Error Bounds via Wasserstein Distance",
    "review_id": "XNlq14_zx2q",
    "input": {
      "title": "Tighter Expected Generalization Error Bounds via Wasserstein Distance",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The Wasserstien distance is defined using a metric.\n- There is no discussion on how we can find an appropriate metric for the hypothesis space.\n- The concept of using the geometry of the hypothesis space has been used in learning theory before.\n- For instance, for VC classes this seminal work considers the VC classes.\n- Later it was used for obtaining bounds on the Radmacher complexity by chaining technique.\n- What is the impact of the algorithm on the chosen metric?\n- Assume we have a hypothesis space and two different algorithms for learning.\n- Do we need to choose different metrics when analyzing different algorithms for the same hypothesis space?\n- The concept of chaining mutual information bounds has been considered in [13], [11], and a recent work by Asadi, Abbe.\n- What are the main differences of this paper compared to those results?\n- The aforementioned papers also provide bounds that depend on the learning algorithm as well as the geometry of the space.\n- The example of the Gaussian location model implies that in some cases the proposed bounds can give much tighter characterization of the generalization bound.\n- Is there any specific practical problem that your generalization bound can give new insights and analysis?\n- Also, does this generalization bound guide us to design a new training algorithm?\n- Most of the results in the paper depend on the distribution of output conditioned on a single sample, i.e., P(W|Z_i).\n- I think in most problems it should be hard to quantify this kernel.\n- The authors also discuss the bounds that depend on P(W|S) however those bounds are much looser.\n- Is there any way to improve the bounds based on P(W|S)?\n- Another important property of the Wasserstein distance is that it is defined based on \u201cinf\u201d over the coupling.\n- In the paper, there is no discussion on how to find a good coupling?\n- I think the authors should also comment on this aspect.\n- Theorem 2: why do you want to add extra info R?\n- I think we can add extra information by considering a specific coupling.\n- The backward channel: I could not understand what is the main idea behind considering backward channel?\n- I think characterizing the kernel P(S|W) is a very very hard problem.\n- How can we use this bound in practice to gain a new insight?\n- I would appreciate it if the authors comment on why they consider the backward channel in the paper.\n- Line 33, there are three and in a row and it is difficult to understand the sentence.\n- Line 38, what is j?\n- Line 151: in the first sentence it seems there is a typo.\n- What does the word \u201csingle letter\u201d mean in the paper?",
    "review_points_list": [
      "The Wasserstien distance is defined using a metric.",
      "There is no discussion on how we can find an appropriate metric for the hypothesis space.",
      "The concept of using the geometry of the hypothesis space has been used in learning theory before.",
      "For instance, for VC classes this seminal work considers the VC classes.",
      "Later it was used for obtaining bounds on the Radmacher complexity by chaining technique.",
      "What is the impact of the algorithm on the chosen metric?",
      "Assume we have a hypothesis space and two different algorithms for learning.",
      "Do we need to choose different metrics when analyzing different algorithms for the same hypothesis space?",
      "The concept of chaining mutual information bounds has been considered in [13], [11], and a recent work by Asadi, Abbe.",
      "What are the main differences of this paper compared to those results?",
      "The aforementioned papers also provide bounds that depend on the learning algorithm as well as the geometry of the space.",
      "The example of the Gaussian location model implies that in some cases the proposed bounds can give much tighter characterization of the generalization bound.",
      "Is there any specific practical problem that your generalization bound can give new insights and analysis?",
      "Also, does this generalization bound guide us to design a new training algorithm?",
      "Most of the results in the paper depend on the distribution of output conditioned on a single sample, i.e., P(W|Z_i).",
      "I think in most problems it should be hard to quantify this kernel.",
      "The authors also discuss the bounds that depend on P(W|S) however those bounds are much looser.",
      "Is there any way to improve the bounds based on P(W|S)?",
      "Another important property of the Wasserstein distance is that it is defined based on \u201cinf\u201d over the coupling.",
      "In the paper, there is no discussion on how to find a good coupling?",
      "I think the authors should also comment on this aspect.",
      "Theorem 2: why do you want to add extra info R?",
      "I think we can add extra information by considering a specific coupling.",
      "The backward channel: I could not understand what is the main idea behind considering backward channel?",
      "I think characterizing the kernel P(S|W) is a very very hard problem.",
      "How can we use this bound in practice to gain a new insight?",
      "I would appreciate it if the authors comment on why they consider the backward channel in the paper.",
      "Line 33, there are three and in a row and it is difficult to understand the sentence.",
      "Line 38, what is j?",
      "Line 151: in the first sentence it seems there is a typo.",
      "What does the word \u201csingle letter\u201d mean in the paper?"
    ]
  },
  {
    "paper_id": "2202.12150v1",
    "submission_id": "xJYek6zantM",
    "submission_title": "Tighter Expected Generalization Error Bounds via Wasserstein Distance",
    "review_id": "Cck-p6x3QtF",
    "input": {
      "title": "Tighter Expected Generalization Error Bounds via Wasserstein Distance",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The first four main results of the paper are for single-letter and random-subset generalization bounds in the standard and randomized-subsample settings.\n- Note that in the latter setting, twice the typical number of samples are drawn, but coin flips are used to decide which subset of those samples is actually used;\n- the distributions considered differ in whether the results of those coin flips / the identities of the samples are known.\n- The bounds in the standard setting are showcased by considering a particular application regarding estimating the mean of a Gaussian distribution given its covariance matrix and comparing the proposed bounds to previous Wasserstein / mutual information bounds.\n- Since this setting is simple, the precise generalization error can be computed and is plotted as well;\n- the new bounds are much closer to the correct generalization error than previous work.\n- The paper then also discusses how the results above can be made to work with backward channels instead of forward channels\n- e.g. how knowing the hypothesis changes the distribution over samples\n- as well as how they can be converted to other information measures, such as launtum information.\n- I think this is a thorough collection of theory results that pushes the boundaries relative to a variety of prior work.\n- I am a bit concerned about the practicality of these results\n- (see the Limitations and Societal Impact section),\n- but I would still lean towards acceptance.\n- Miscellaneous:\n- - Line 3:\n- \u201cand their analogous in\u201d analogous -> analogues.\n- - Line 153:\n- \u201ctighter than (1)\u201d Took a moment for me to parse this reference, I think it would be clearer if it were more explicit e.g. Inequality (1)\n- or perhaps Formula (1)? Similarly on line 160.\n- - Line 210:\n- \u201cimpact on the samples\u2019 identities\u201d on -> of\n- I have read the author's response and they have addressed my concern about there being cases where these bounds improve on practical cases,\n- so I have adjusted my score accordingly to an accept.",
    "review_points_list": [
      "The first four main results of the paper are for single-letter and random-subset generalization bounds in the standard and randomized-subsample settings.",
      "Note that in the latter setting, twice the typical number of samples are drawn, but coin flips are used to decide which subset of those samples is actually used;",
      "the distributions considered differ in whether the results of those coin flips / the identities of the samples are known.",
      "The bounds in the standard setting are showcased by considering a particular application regarding estimating the mean of a Gaussian distribution given its covariance matrix and comparing the proposed bounds to previous Wasserstein / mutual information bounds.",
      "Since this setting is simple, the precise generalization error can be computed and is plotted as well;",
      "the new bounds are much closer to the correct generalization error than previous work.",
      "The paper then also discusses how the results above can be made to work with backward channels instead of forward channels",
      "e.g. how knowing the hypothesis changes the distribution over samples",
      "as well as how they can be converted to other information measures, such as launtum information.",
      "I think this is a thorough collection of theory results that pushes the boundaries relative to a variety of prior work.",
      "I am a bit concerned about the practicality of these results",
      "(see the Limitations and Societal Impact section),",
      "but I would still lean towards acceptance.",
      "Miscellaneous:",
      "- Line 3:",
      "\u201cand their analogous in\u201d analogous -> analogues.",
      "- Line 153:",
      "\u201ctighter than (1)\u201d Took a moment for me to parse this reference, I think it would be clearer if it were more explicit e.g. Inequality (1)",
      "or perhaps Formula (1)? Similarly on line 160.",
      "- Line 210:",
      "\u201cimpact on the samples\u2019 identities\u201d on -> of",
      "I have read the author's response and they have addressed my concern about there being cases where these bounds improve on practical cases,",
      "so I have adjusted my score accordingly to an accept."
    ]
  },
  {
    "paper_id": "2202.03636v1",
    "submission_id": "tvDBe6K8L5o",
    "submission_title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
    "review_id": "ntnjWgvEqD",
    "input": {
      "title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper's main contribution is the development of a novel algorithm for [task].\n- The authors demonstrate the effectiveness of the proposed method through an empirical evaluation.\n- The proposed algorithm is well-suited for large-scale datasets and can be used in conjunction with other techniques to improve performance.\n- One potential limitation of the proposed method is its reliance on [assumption].\n- This assumption may not hold for all datasets, which could impact the method's performance.\n- Additionally, the authors do not provide a clear explanation of the algorithm's underlying mechanics.\n- Overall, the proposed method shows promise but requires further development to address the limitations mentioned above.",
    "review_points_list": [
      "The paper's main contribution is the development of a novel algorithm for [task].",
      "The authors demonstrate the effectiveness of the proposed method through an empirical evaluation.",
      "The proposed algorithm is well-suited for large-scale datasets and can be used in conjunction with other techniques to improve performance.",
      "One potential limitation of the proposed method is its reliance on [assumption].",
      "This assumption may not hold for all datasets, which could impact the method's performance.",
      "Additionally, the authors do not provide a clear explanation of the algorithm's underlying mechanics.",
      "Overall, the proposed method shows promise but requires further development to address the limitations mentioned above."
    ]
  },
  {
    "paper_id": "2202.03636v1",
    "submission_id": "tvDBe6K8L5o",
    "submission_title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
    "review_id": "EHfdXa9Z1YG",
    "input": {
      "title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The algorithm seems to be novel and differs from previous comparable approaches like CoreSet or BADGE. Related work is adequately cited.\n- The authors show empirically that their algorithm, Cluster-Margin, is both more efficient (O(nlog(n) vs O(n\u221a(n)) than CoreSet and BADGE in practice and more effective.\n- In particular, the algorithm clearly outperforms CoreSet, BADGE, Margin and Random on the Open Images dataset.\n- The algorithm requires 29% less labels than the second-best model in the 100k batch-size setting and 60% less labels in the 1M batch-size setting to achieve the same result (mAP).\n- Cluster-Margin also outperforms all other methods on CIFAR10, CIFAR100 and obtains a similar performance on SVHN.\n- The authors also establish a theoretical guarantee for the Cluster-MarginV algorithm and show that those results hold for the Cluster-Margin algorithm in specific settings.\n- They show that the Cluster-MarginV algorithm has a label complexity bound which improves over the Margin algorithm by a factor beta.\n- This improvement is possible, under specific hypotheses like an optimal volume-based sampler, when the dimensionality of the embedding space is small or when the batch size k is large.\n- The optimal volume-based sampler is approximately equivalent to the Cluster-Margin algorithm.\n- log(k) is an upper bound on the improvement of query complexity for any sampler.\n- The authors are aware and mention that their theoretical results are initial and that equating volume based samplers and the Cluster-Margin algorithm is an open research question.\n- The paper is very clear and well-organized. The authors detail the hyper-parameters and compute details used for the experiments.\n- The Cluster-Margin algorithm is also explained in detail.\n- The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods.\n- The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm.",
    "review_points_list": [
      "The algorithm seems to be novel and differs from previous comparable approaches like CoreSet or BADGE. Related work is adequately cited.",
      "The authors show empirically that their algorithm, Cluster-Margin, is both more efficient (O(nlog(n) vs O(n\u221a(n)) than CoreSet and BADGE in practice and more effective.",
      "In particular, the algorithm clearly outperforms CoreSet, BADGE, Margin and Random on the Open Images dataset.",
      "The algorithm requires 29% less labels than the second-best model in the 100k batch-size setting and 60% less labels in the 1M batch-size setting to achieve the same result (mAP).",
      "Cluster-Margin also outperforms all other methods on CIFAR10, CIFAR100 and obtains a similar performance on SVHN.",
      "The authors also establish a theoretical guarantee for the Cluster-MarginV algorithm and show that those results hold for the Cluster-Margin algorithm in specific settings.",
      "They show that the Cluster-MarginV algorithm has a label complexity bound which improves over the Margin algorithm by a factor beta.",
      "This improvement is possible, under specific hypotheses like an optimal volume-based sampler, when the dimensionality of the embedding space is small or when the batch size k is large.",
      "The optimal volume-based sampler is approximately equivalent to the Cluster-Margin algorithm.",
      "log(k) is an upper bound on the improvement of query complexity for any sampler.",
      "The authors are aware and mention that their theoretical results are initial and that equating volume based samplers and the Cluster-Margin algorithm is an open research question.",
      "The paper is very clear and well-organized. The authors detail the hyper-parameters and compute details used for the experiments.",
      "The Cluster-Margin algorithm is also explained in detail.",
      "The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods.",
      "The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm."
    ]
  },
  {
    "paper_id": "2202.03636v1",
    "submission_id": "tvDBe6K8L5o",
    "submission_title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
    "review_id": "dVwl9BQ3BZ",
    "input": {
      "title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper provides a novel architecture which combines a number of different ideas for synthesising tabular data.\n- The paper is well written and clear.\n- The ideas composition of previous architectures is also interesting.\n- The only notable flaw is that the angle in which privacy is examined is only examined experimentally with no theoretic analysis.\n- However, the large coverage of experiments helps provide evidence for this approach.\n- Although the individual components of the architecture are not novel, their composition appears to be novel.\n- One fact which could hurt novelty is if the composition of AE and GAN has been used before.\n- If so, this should be highlighted in prior work.\n- The use of the invertible generator is also very interesting.\n- The paper seems to be technically sound with a nice and extensive experimental section.\n- The only issue is the lack of runtime and architecture size comparisons.\n- The paper is also well written.\n- Although the heavy notation in the specification of the architecture hurts clarity a bit.\n- The combination of AE, GAN, and invertible generator is neat.\n- The log-density likelihood regularizer/loss function is interesting.\n- It seems that this can also be used to make different partitions of real-world data more likely (i.e., create a specialized generator by rewarding/penalizing different subsets of the training data).\n- The notation of the loss function in (9) suggest that the regularity/sparsity terms only depend on the fake data.\n- If so, why not have this also depend on the training data/real records?\n- What is the runtime compared to baselines?\n- How does your proposed architecture compare in the number of parameters to baselines?\n- Is there a way to relate the log-density regularizer and the fake-real distance other definitions of privacy? (differential privacy?)\n- There seems to be a missing point of comparison when the log-density regularizer is not used.\n- What happens when you don't use an invertible generator?\n- Furthermore, without the invertible generator can the log-density regularizer be substituted by just considering a convex reweighting of the discriminator error for real and fake error?\n- i.e., for $ \beta > 0 $, weight the error of the discriminator for real data being lower than that of an error for fake data",
    "review_points_list": [
      "The paper provides a novel architecture which combines a number of different ideas for synthesising tabular data.",
      "The paper is well written and clear.",
      "The ideas composition of previous architectures is also interesting.",
      "The only notable flaw is that the angle in which privacy is examined is only examined experimentally with no theoretic analysis.",
      "However, the large coverage of experiments helps provide evidence for this approach.",
      "Although the individual components of the architecture are not novel, their composition appears to be novel.",
      "One fact which could hurt novelty is if the composition of AE and GAN has been used before.",
      "If so, this should be highlighted in prior work.",
      "The use of the invertible generator is also very interesting.",
      "The paper seems to be technically sound with a nice and extensive experimental section.",
      "The only issue is the lack of runtime and architecture size comparisons.",
      "The paper is also well written.",
      "Although the heavy notation in the specification of the architecture hurts clarity a bit.",
      "The combination of AE, GAN, and invertible generator is neat.",
      "The log-density likelihood regularizer/loss function is interesting.",
      "It seems that this can also be used to make different partitions of real-world data more likely (i.e., create a specialized generator by rewarding/penalizing different subsets of the training data).",
      "The notation of the loss function in (9) suggest that the regularity/sparsity terms only depend on the fake data.",
      "If so, why not have this also depend on the training data/real records?",
      "What is the runtime compared to baselines?",
      "How does your proposed architecture compare in the number of parameters to baselines?",
      "Is there a way to relate the log-density regularizer and the fake-real distance other definitions of privacy? (differential privacy?)",
      "There seems to be a missing point of comparison when the log-density regularizer is not used.",
      "What happens when you don't use an invertible generator?",
      "Furthermore, without the invertible generator can the log-density regularizer be substituted by just considering a convex reweighting of the discriminator error for real and fake error?",
      "i.e., for $ \beta > 0 $, weight the error of the discriminator for real data being lower than that of an error for fake data"
    ]
  },
  {
    "paper_id": "2202.03636v1",
    "submission_id": "tvDBe6K8L5o",
    "submission_title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
    "review_id": "bmDuAIrEqLV",
    "input": {
      "title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- While the problem and method of anonymising tabular data using a GAN appears not to be new, the proposed technique of using the NLL to guard against privacy techniques seems original.\n- The account of the existing literature is thorough and the contribution of the authors is well defined.\n- The authors give cursory theoretical considerations to several aspects of their approach, in particular the time complexity for the Jacobean determinant calculations and suitably of the algorithms to solve the associated ODE problem.\n- The experimental results are well presented, and the authors' approach appears to significantly outperform other techniques with respect to synthesising tabular data and defending against privacy attacks.\n- The work is very clearly written and concise.",
    "review_points_list": [
      "While the problem and method of anonymising tabular data using a GAN appears not to be new, the proposed technique of using the NLL to guard against privacy techniques seems original.",
      "The account of the existing literature is thorough and the contribution of the authors is well defined.",
      "The authors give cursory theoretical considerations to several aspects of their approach, in particular the time complexity for the Jacobean determinant calculations and suitably of the algorithms to solve the associated ODE problem.",
      "The experimental results are well presented, and the authors' approach appears to significantly outperform other techniques with respect to synthesising tabular data and defending against privacy attacks.",
      "The work is very clearly written and concise."
    ]
  },
  {
    "paper_id": "2106.07148v2",
    "submission_id": "vlf0zTKa5Lh",
    "submission_title": "On the Sample Complexity of Learning under Geometric Stability",
    "review_id": "ObVch5n21p8",
    "input": {
      "title": "On the Sample Complexity of Learning under Geometric Stability",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Exploiting invariance properties in the data is a central theme in machine learning, and is believed to play a major role in neural networks architecture for example. However, theoretical results are severely lacking and I find this work interesting and a great addition to the subject. The paper is well written and all the results are extensively discussed and interpreted.\n- Given the closeness to the paper [1], I would spend more time discussing and comparing the two works. Especially, the main insight of the proof (representation of projection) and the improvement factor were already claimed in [1], so I think the main contribution of this paper is to understand the dependency of this improvement factor on the structure of the group (in fixed $d$). I understand that $d$ fixed and large $d$ are two different regimes with very different techniques/assumptions (source and capacity conditions for example). However I don\u2019t think the high dimensional regime is too naive (learn only low-degree polynomials) as it simply confirm a series of (non-rigorous) work that show that RF/KRR act as shrinkage operators with an inflated effective regularization (which seems to be a good approximation even for moderate/low $d$). In the case of invariant kernel, this effective regularization is divided by $d^\boldsymbol{\\alpha}$ and explain the improvement.\n- As far I understand, [1] considers not only invariance by translation (as written in your introduction), but more generally, subsets of the orthogonal group O(d). This includes group with infinite size. In [1], the 'degeneracy' of the group instead of the cardinality of the group controls the sample size gain (for a group of degeneracy $\\alpha$, gain of factor $d^\\alpha$ in sample complexity). So even for some infinite group, there is only a finite sample size gain. In some cases (some subgroups of permutation of polynomial size, while you can consider exponentially sized groups in your paper), the degeneracy coincides with the size of the group. While [1] fully check the conditions only for invariance by translation, I expect their proof to extend to these cases (with the added requirement $n > O(d^\\alpha)$ if $\\alpha >1$). I think it will be useful to discuss how",
    "review_points_list": [
      "Exploiting invariance properties in the data is a central theme in machine learning, and is believed to play a major role in neural networks architecture for example. However, theoretical results are severely lacking and I find this work interesting and a great addition to the subject. The paper is well written and all the results are extensively discussed and interpreted.",
      "Given the closeness to the paper [1], I would spend more time discussing and comparing the two works. Especially, the main insight of the proof (representation of projection) and the improvement factor were already claimed in [1], so I think the main contribution of this paper is to understand the dependency of this improvement factor on the structure of the group (in fixed $d$). I understand that $d$ fixed and large $d$ are two different regimes with very different techniques/assumptions (source and capacity conditions for example). However I don\u2019t think the high dimensional regime is too naive (learn only low-degree polynomials) as it simply confirm a series of (non-rigorous) work that show that RF/KRR act as shrinkage operators with an inflated effective regularization (which seems to be a good approximation even for moderate/low $d$). In the case of invariant kernel, this effective regularization is divided by $d^\boldsymbol{\\alpha}$ and explain the improvement.",
      "As far I understand, [1] considers not only invariance by translation (as written in your introduction), but more generally, subsets of the orthogonal group O(d). This includes group with infinite size. In [1], the 'degeneracy' of the group instead of the cardinality of the group controls the sample size gain (for a group of degeneracy $\\alpha$, gain of factor $d^\\alpha$ in sample complexity). So even for some infinite group, there is only a finite sample size gain. In some cases (some subgroups of permutation of polynomial size, while you can consider exponentially sized groups in your paper), the degeneracy coincides with the size of the group. While [1] fully check the conditions only for invariance by translation, I expect their proof to extend to these cases (with the added requirement $n > O(d^\\alpha)$ if $\\alpha >1$). I think it will be useful to discuss how"
    ]
  },
  {
    "paper_id": "2106.07148v2",
    "submission_id": "vlf0zTKa5Lh",
    "submission_title": "On the Sample Complexity of Learning under Geometric Stability",
    "review_id": "JTa4m9KV5GV",
    "input": {
      "title": "On the Sample Complexity of Learning under Geometric Stability",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I think this paper explores an interesting question, and provides a concrete set of results that demonstrate the possible improvements of exploiting invariance in the kernel design.\n- The extension in section 5 to stability rather than exact invariance also feels important from a practical perspective.\n- The generalization error bounds in terms of N_{K_G} are perhaps not too surprising, and the more significant technical contributions might be in how to control N_{K_G} and the dimension ratios gamma_d, as carried out in Section 4 and Appendix C.\n- It's not clear how sharp are the obtained generalization bounds---in particular how sharp is the method of bounding N_{K_G} in Lemma 4, and the further restriction of l_n in Theorem 5 such that the second term of this bound dominates the first.\n- I would have appreciated one or two concrete examples where these analyses may be shown to correctly characterize the size of N_{K_G}(lambda) for the relevant choice of lambda.\n- The improvement in generalization error is not in the scaling dependence on n, but rather in how the 'constant' depends on |G| and the dimension d.\n- I think this is fine, but then the analysis in Section 4 is a pointwise analysis over permutations in G, and thus doesn't really track well this dependence on d and |G|.\n- In the bound (17), nu_0 = 1/|G| can indeed be exponentially small in d, but can the leading constant C also be exponentially large in d?\n- If n needs to be exponentially large in d for the statement 'nu_d(l_n) is approximately 1/|G|' to hold, I think this detracts a bit from the relevance of the results.\n- A couple other comments: ...\n- In the proof of Theorem 13, can the authors perhaps also provide a proof of the analogue of Lemma 3 (the approximation error) in this setting?\n- I think the result of Lemma 3 feels more intuitive to me in the setting of exact invariance and where S is a projection onto a subspace, but is less clear to me in the 'geometric stability' setting of Section 5.\n- Can the authors clarify what is the choice of h(sigma) in Proposition 14?",
    "review_points_list": [
      "I think this paper explores an interesting question, and provides a concrete set of results that demonstrate the possible improvements of exploiting invariance in the kernel design.",
      "The extension in section 5 to stability rather than exact invariance also feels important from a practical perspective.",
      "The generalization error bounds in terms of N_{K_G} are perhaps not too surprising, and the more significant technical contributions might be in how to control N_{K_G} and the dimension ratios gamma_d, as carried out in Section 4 and Appendix C.",
      "It's not clear how sharp are the obtained generalization bounds---in particular how sharp is the method of bounding N_{K_G} in Lemma 4, and the further restriction of l_n in Theorem 5 such that the second term of this bound dominates the first.",
      "I would have appreciated one or two concrete examples where these analyses may be shown to correctly characterize the size of N_{K_G}(lambda) for the relevant choice of lambda.",
      "The improvement in generalization error is not in the scaling dependence on n, but rather in how the 'constant' depends on |G| and the dimension d.",
      "I think this is fine, but then the analysis in Section 4 is a pointwise analysis over permutations in G, and thus doesn't really track well this dependence on d and |G|.",
      "In the bound (17), nu_0 = 1/|G| can indeed be exponentially small in d, but can the leading constant C also be exponentially large in d?",
      "If n needs to be exponentially large in d for the statement 'nu_d(l_n) is approximately 1/|G|' to hold, I think this detracts a bit from the relevance of the results.",
      "A couple other comments: ...",
      "In the proof of Theorem 13, can the authors perhaps also provide a proof of the analogue of Lemma 3 (the approximation error) in this setting?",
      "I think the result of Lemma 3 feels more intuitive to me in the setting of exact invariance and where S is a projection onto a subspace, but is less clear to me in the 'geometric stability' setting of Section 5.",
      "Can the authors clarify what is the choice of h(sigma) in Proposition 14?"
    ]
  },
  {
    "paper_id": "2001.06137v1",
    "submission_id": "w3x8K0M6sAz",
    "submission_title": "Topology-Imbalance Learning for Semi-Supervised Node Classification",
    "review_id": "QWDREPkpAg0",
    "input": {
      "title": "Topology-Imbalance Learning for Semi-Supervised Node Classification",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The topology imbalance problem is very interesting and should be paid more attention.\n- It is clearly illustrated by the authors that labelled nodes at different positions should be weighted differently.\n- Figure 3 is quite impressive that the proposed metric Totoro has strong indication about the position of the labelled nodes and the metric value is directly related with the performance of downstream node classifiers.\n- Overall the presentation is good and I can follow most ideas presented.\n- Specifically, I have following concerns:\n- (1) A very important assumption this paper holds is that the decision boundary of Label propagation methods and GNNs methods are aligned.\n- Authors verify this by a visualization in Figure 2(a) but this is very messy and I feel confused about the meaning of the legends (e.g., GCN=T, LP=T).\n- (2) Authors empirically prove that nodes with large Totoro values are far away from the decision boundary and should be set large weights during training.\n- Experiment results indeed demonstrate performance gain.\n- However, it is still non-intuitive.\n- Samples far away from the decision boundary should be easy cases and contribute less to the classification in many models (refer to SVM).\n- Assigning larger weights to the easy samples is not intuitive for me.\n- Also, the weight computed based on Rank of the Totoro metric is quite heuristic.\n- It is fine but it would be better if authors can point out the direct connection between model performance, specific Totoro values, and sample weights.\n- (3) The presentation of the core idea from line 120 to line 122 can be improved.\n- The operation '*' in Eq. (6) and (7) is multiplication?\n- If so, try to replace it with other symbols since '*' represents convolution in many cases.\n- Line 198 'GTA' should be 'GAT'.",
    "review_points_list": [
      "The topology imbalance problem is very interesting and should be paid more attention.",
      "It is clearly illustrated by the authors that labelled nodes at different positions should be weighted differently.",
      "Figure 3 is quite impressive that the proposed metric Totoro has strong indication about the position of the labelled nodes and the metric value is directly related with the performance of downstream node classifiers.",
      "Overall the presentation is good and I can follow most ideas presented.",
      "Specifically, I have following concerns:",
      "(1) A very important assumption this paper holds is that the decision boundary of Label propagation methods and GNNs methods are aligned.",
      "Authors verify this by a visualization in Figure 2(a) but this is very messy and I feel confused about the meaning of the legends (e.g., GCN=T, LP=T).",
      "(2) Authors empirically prove that nodes with large Totoro values are far away from the decision boundary and should be set large weights during training.",
      "Experiment results indeed demonstrate performance gain.",
      "However, it is still non-intuitive.",
      "Samples far away from the decision boundary should be easy cases and contribute less to the classification in many models (refer to SVM).",
      "Assigning larger weights to the easy samples is not intuitive for me.",
      "Also, the weight computed based on Rank of the Totoro metric is quite heuristic.",
      "It is fine but it would be better if authors can point out the direct connection between model performance, specific Totoro values, and sample weights.",
      "(3) The presentation of the core idea from line 120 to line 122 can be improved.",
      "The operation '*' in Eq. (6) and (7) is multiplication?",
      "If so, try to replace it with other symbols since '*' represents convolution in many cases.",
      "Line 198 'GTA' should be 'GAT'."
    ]
  },
  {
    "paper_id": "2001.06137v1",
    "submission_id": "w3x8K0M6sAz",
    "submission_title": "Topology-Imbalance Learning for Semi-Supervised Node Classification",
    "review_id": "f51DM7zcQ6K",
    "input": {
      "title": "Topology-Imbalance Learning for Semi-Supervised Node Classification",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Overall the paper is well written and easy to follow.\n- I am not sure if topo imbalance problem proposed by the author is entirely new. Seems natural in Label Propagation type problem.  Please confirm.\n- Ok\n- The empirical study seems promising. I appreciate the scientific and transparent experimental section.\n- The approach proposed by the authors seems more like a loss type of heuristic, lacking analysis why it needs to be designed in this form and what is the trade off by applying such loss, etc.\n- In the empirical study section, it seems that the benchmarks are not convincing enough. The ReNode approach may benefit from comparisons with other stronger benchmarks with potential to solve the topo imbalance problem. (please confirm if does not exist)",
    "review_points_list": [
      "Overall the paper is well written and easy to follow.",
      "I am not sure if topo imbalance problem proposed by the author is entirely new. Seems natural in Label Propagation type problem.  Please confirm.",
      "Ok",
      "The empirical study seems promising. I appreciate the scientific and transparent experimental section.",
      "The approach proposed by the authors seems more like a loss type of heuristic, lacking analysis why it needs to be designed in this form and what is the trade off by applying such loss, etc.",
      "In the empirical study section, it seems that the benchmarks are not convincing enough. The ReNode approach may benefit from comparisons with other stronger benchmarks with potential to solve the topo imbalance problem. (please confirm if does not exist)"
    ]
  },
  {
    "paper_id": "2001.06137v1",
    "submission_id": "w3x8K0M6sAz",
    "submission_title": "Topology-Imbalance Learning for Semi-Supervised Node Classification",
    "review_id": "qtP-eFDf87S",
    "input": {
      "title": "Topology-Imbalance Learning for Semi-Supervised Node Classification",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- While I like some aspects of the write-up, I find there's some blind spots in the work as it currently stands, which limit its significance:\n- The proposed method has a strong assumption that the data is homophilous (edges are more likely to link nodes sharing the same label, so clusters of the same label are present in the data).  Should this not hold, all nodes are on the graph's class boundary (per the Totoro PPR definition).\n- The authors acknowledge this limitation, but the phenomenon needs to be studied in my opinion.\n- In both absolute *and* relative terms, the performance gains by applying ReNode for all datasets is actually quite low.\n- The performance gains don't really address imbalance much better than other proposed approaches (e.g. consider Table 3).\n- Furthermore it seems like there's no baseline in Table 3 that considers both kinds of imbalance (TINL and QINL)?\n- There seems to be no study of alternative reweighting strategies to validate the choice of PPR.\n- There are probably dozens of alternatives possible here -- this choice needs to be defended.\n- Understanding how ReNode works with other models (than GCN) is necessary to understand its significance.\n- For example, Figure 5 shows that there may be some difference in sensitivity between methods, but we don't know how ReNode effects different graph convolutions.\n- Generally high, but aside from the issues mentioned in significance, there were other obvious analysis that need to be performed:\n- A more thorough analysis of imbalance and its effect on performance (only two imbalance ratios [5,10] are used...it'd be nice to understand how ReNode (and baselines) performed as imbalance was more smoothly varied.\n- There's an assertion that  GAT is the least sensitive to topological imbalance (Figure 5), but quite frankly, I don't know if I believe this\n- GAT (as the most parameterized model) is much more capable of overfitting.  Instead, I wonder if the hyper-parameters were not adequately searched for all 3 methods for each imbalance setting.\n- There's a number of work out there which considers quantity and graph imbalance, so the onus is on ReNode to show that its unique.\n- I'm curious if related work for graphs *really* doesn't consider topology imbalance.\n- To be more specific, some of the regularizations to tackle quantity imbalance might end up encoding topology indirectly (e.g. via unlabeled nodes, edge generation, or something else).\n- There has also been a lot of work studying PPR & GNNs lately, but the authors seem to do an ok job surveying the field.\n- The paper is well written and easy to understand.\n- The authors establish the quality of the proposed method, ReNode, as generally high.\n- However, aside from the issues mentioned in significance, there were other obvious analysis that need to be performed.\n- Understanding how ReNode works with other models (than GCN) is necessary to understand its significance.",
    "review_points_list": [
      "While I like some aspects of the write-up, I find there's some blind spots in the work as it currently stands, which limit its significance:",
      "The proposed method has a strong assumption that the data is homophilous (edges are more likely to link nodes sharing the same label, so clusters of the same label are present in the data).  Should this not hold, all nodes are on the graph's class boundary (per the Totoro PPR definition).",
      "The authors acknowledge this limitation, but the phenomenon needs to be studied in my opinion.",
      "In both absolute *and* relative terms, the performance gains by applying ReNode for all datasets is actually quite low.",
      "The performance gains don't really address imbalance much better than other proposed approaches (e.g. consider Table 3).",
      "Furthermore it seems like there's no baseline in Table 3 that considers both kinds of imbalance (TINL and QINL)?",
      "There seems to be no study of alternative reweighting strategies to validate the choice of PPR.",
      "There are probably dozens of alternatives possible here -- this choice needs to be defended.",
      "Understanding how ReNode works with other models (than GCN) is necessary to understand its significance.",
      "For example, Figure 5 shows that there may be some difference in sensitivity between methods, but we don't know how ReNode effects different graph convolutions.",
      "Generally high, but aside from the issues mentioned in significance, there were other obvious analysis that need to be performed:",
      "A more thorough analysis of imbalance and its effect on performance (only two imbalance ratios [5,10] are used...it'd be nice to understand how ReNode (and baselines) performed as imbalance was more smoothly varied.",
      "There's an assertion that  GAT is the least sensitive to topological imbalance (Figure 5), but quite frankly, I don't know if I believe this",
      "GAT (as the most parameterized model) is much more capable of overfitting.  Instead, I wonder if the hyper-parameters were not adequately searched for all 3 methods for each imbalance setting.",
      "There's a number of work out there which considers quantity and graph imbalance, so the onus is on ReNode to show that its unique.",
      "I'm curious if related work for graphs *really* doesn't consider topology imbalance.",
      "To be more specific, some of the regularizations to tackle quantity imbalance might end up encoding topology indirectly (e.g. via unlabeled nodes, edge generation, or something else).",
      "There has also been a lot of work studying PPR & GNNs lately, but the authors seem to do an ok job surveying the field.",
      "The paper is well written and easy to understand.",
      "The authors establish the quality of the proposed method, ReNode, as generally high.",
      "However, aside from the issues mentioned in significance, there were other obvious analysis that need to be performed.",
      "Understanding how ReNode works with other models (than GCN) is necessary to understand its significance."
    ]
  },
  {
    "paper_id": "2101.02195v2",
    "submission_id": "vYZmTEDFoqP",
    "submission_title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
    "review_id": "leZsF61Q4S",
    "input": {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- All references here can be found in the original paper.\n- Ideas in this paper do not seem too novel to me.\n- LSVI-UCB-Batch algorithm is literally LSVI-UCB with Q-function updates only at pre-specified batch grids.\n- The analysis for the upper bound mostly combines techniques from Jin et al. 2020 and Abbasi-Yadkori et al. 2011.\n- The message that we can use uniform batch grids is somewhat interesting, but it's quite predictive from Han et al. 2020 that uses uniform batches in SBUCB.\n- LSVI-UCB-RareSwitch uses the determinant based criterion from Abbasi-Yadkori et al. 2011 to determine when to switch the policy.\n- They basically changed the OFUL updates in the rarely switching OFUL algorithm (Abbasi-Yadkori et al. 2011) to LSVI-UCB.\n- Providing a reasonable lower bound in this setting could be the most novel contribution, but unfortunately the authors defer that to future work.\n- Overall this paper tells a clean and complete story (except missing the lower bound for the rare policy switch model) with all arguments well supported.\n- Theory. I briefly checked the proofs in the appendix (which is generally well-organized), and the high-level ideas look correct to me.\n- Experiments. It is good to see that the numerical experiments matches what the theory predicts (when the linear MDP assumption holds).\n- It would be good to have some extra experiments showing how the proposed algorithms perform under different levels of model misspecification.\n- The paper is clearly written and well organized.\n- It's very easy to follow, and provides enough information to reproduce experimental results.\n- RL under adaptivity constraints is a relevant problem in many practices, so I agree that the problem studied here can be valuable.\n- However, as a theoretical paper, its technical contributions look somewhat limited.\n- Also, since the paper assumes linear MDP throughout and it often does not hold in practice, the current experiment results may not be strong enough to convince practitioners that the batch schedules / policy update criterion's work well in practice.",
    "review_points_list": [
      "All references here can be found in the original paper.",
      "Ideas in this paper do not seem too novel to me.",
      "LSVI-UCB-Batch algorithm is literally LSVI-UCB with Q-function updates only at pre-specified batch grids.",
      "The analysis for the upper bound mostly combines techniques from Jin et al. 2020 and Abbasi-Yadkori et al. 2011.",
      "The message that we can use uniform batch grids is somewhat interesting, but it's quite predictive from Han et al. 2020 that uses uniform batches in SBUCB.",
      "LSVI-UCB-RareSwitch uses the determinant based criterion from Abbasi-Yadkori et al. 2011 to determine when to switch the policy.",
      "They basically changed the OFUL updates in the rarely switching OFUL algorithm (Abbasi-Yadkori et al. 2011) to LSVI-UCB.",
      "Providing a reasonable lower bound in this setting could be the most novel contribution, but unfortunately the authors defer that to future work.",
      "Overall this paper tells a clean and complete story (except missing the lower bound for the rare policy switch model) with all arguments well supported.",
      "Theory. I briefly checked the proofs in the appendix (which is generally well-organized), and the high-level ideas look correct to me.",
      "Experiments. It is good to see that the numerical experiments matches what the theory predicts (when the linear MDP assumption holds).",
      "It would be good to have some extra experiments showing how the proposed algorithms perform under different levels of model misspecification.",
      "The paper is clearly written and well organized.",
      "It's very easy to follow, and provides enough information to reproduce experimental results.",
      "RL under adaptivity constraints is a relevant problem in many practices, so I agree that the problem studied here can be valuable.",
      "However, as a theoretical paper, its technical contributions look somewhat limited.",
      "Also, since the paper assumes linear MDP throughout and it often does not hold in practice, the current experiment results may not be strong enough to convince practitioners that the batch schedules / policy update criterion's work well in practice."
    ]
  },
  {
    "paper_id": "2101.02195v2",
    "submission_id": "vYZmTEDFoqP",
    "submission_title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
    "review_id": "zxOAQW2o5v",
    "input": {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The result seems well explored.\n- The first result reduces the number of batches to achieve the same regret as with T batches.\n- The second result reduces the number of policy switches.\n- As I see, the result is interesting.\n- The related work is well explained.\n- The related work to this problem exist in different setups, and thus the overall result is not that surprising.\n- The detailed proof changes with the problem setup.\n- The detailed technical novelty compared to the related works would have improved the paper.",
    "review_points_list": [
      "The result seems well explored.",
      "The first result reduces the number of batches to achieve the same regret as with T batches.",
      "The second result reduces the number of policy switches.",
      "As I see, the result is interesting.",
      "The related work is well explained.",
      "The related work to this problem exist in different setups, and thus the overall result is not that surprising.",
      "The detailed proof changes with the problem setup.",
      "The detailed technical novelty compared to the related works would have improved the paper."
    ]
  },
  {
    "paper_id": "2101.02195v2",
    "submission_id": "vYZmTEDFoqP",
    "submission_title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
    "review_id": "4KQtGhc8LN",
    "input": {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is well written.\n- The main results are well explained and it is easy to follow proof steps.\n- It is nice to match/improve the regret bound for the LSVI-UCB algorithms constrained by the policy adaptivity.\n- For the LSVI-UVB-RareSwitch, it would be better if you could explain the bound on the adaptivity in terms of $B$, and the role of $B.\n- When specialized to the tabular setting, the provided regret bounds both scale poorly in $S$, $A, if we compare them with.\n- This seems to be the state of the art, but it is worth remaking this issue.\n- As shown in Theorem 4.1 and Theorem 5.1, the regret bounds match the LSVI-UCB's regret with full adaptivity when $B=O(\").\n- In practice, $B$ is often the constraint that is fixed and we are free to choose algorithm parameters, e.g. $T.\n- It would be useful if you could add comments on how to select algorithm parameters like $T. What does the PAC bounds look like?\n- It is nice to see the effect of batch size $B on the regret in the computational experiment.\n- Did you try other schemes adjusting $B like diminishing $B as the learned policy goes to be near-optimal?\n- This would add more flexibility of algorithms so that practitioners like.\n- Originality and Significance.\n- The proposed LSVI-UCB algorithms are featured by adaptively switching policies.\n- The method is new in the function approximation setting and the proofs offer a way to deal with samples from adaptively switched policies.\n- The authors also remark a similar work that could be a case of their results.\n- It would be helpful for readers to differentiate them if you could add more comments on the technical side.",
    "review_points_list": [
      "The paper is well written.",
      "The main results are well explained and it is easy to follow proof steps.",
      "It is nice to match/improve the regret bound for the LSVI-UCB algorithms constrained by the policy adaptivity.",
      "For the LSVI-UVB-RareSwitch, it would be better if you could explain the bound on the adaptivity in terms of $B$, and the role of $B.",
      "When specialized to the tabular setting, the provided regret bounds both scale poorly in $S$, $A, if we compare them with.",
      "This seems to be the state of the art, but it is worth remaking this issue.",
      "As shown in Theorem 4.1 and Theorem 5.1, the regret bounds match the LSVI-UCB's regret with full adaptivity when $B=O(\").",
      "In practice, $B$ is often the constraint that is fixed and we are free to choose algorithm parameters, e.g. $T.",
      "It would be useful if you could add comments on how to select algorithm parameters like $T. What does the PAC bounds look like?",
      "It is nice to see the effect of batch size $B on the regret in the computational experiment.",
      "Did you try other schemes adjusting $B like diminishing $B as the learned policy goes to be near-optimal?",
      "This would add more flexibility of algorithms so that practitioners like.",
      "Originality and Significance.",
      "The proposed LSVI-UCB algorithms are featured by adaptively switching policies.",
      "The method is new in the function approximation setting and the proofs offer a way to deal with samples from adaptively switched policies.",
      "The authors also remark a similar work that could be a case of their results.",
      "It would be helpful for readers to differentiate them if you could add more comments on the technical side."
    ]
  },
  {
    "paper_id": "2101.02195v2",
    "submission_id": "vYZmTEDFoqP",
    "submission_title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
    "review_id": "7bjqWav1j56",
    "input": {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- In Theorem 4.2, by comparing the regret lower bound with the obtained bound of LSVI-UCB-Batch in Theorem 4.1, the authors claim that the dependency of the total regret on the number of batches for the batch learning model is optimal.\n- However, the main term $\text{sqrt}{d^3 H^3 T}$ does not match the lower bound of $d H \text{sqrt}{T}$.\n- It would be helpful if the authors could provide a more general analysis of the batch learning model under other RL algorithms, especially those that match the lower bound.\n- The focus of this paper is the adaptivity constrained model.\n- However, all the two algorithms proposed are based on the LSVI-UCB algorithm proposed in Jin et al. (2020).\n- Theorem 4.1 of this work extends Theorem 3.1 in Jin et al. (2020) by comparing the difference between $\text{Lambda}_h^{b_k}$ and $\text{Lambda}_h^{k}$ in Lemma A.3.\n- Theorem 5.1 of this work further combines the idea of Lemma B.2 (Lemma 12, Abbasi-Yadkori et al. 2011).\n- This work provides a new aspect of the LSVI-UCB algorithm, but the novelty is limited if only the LSVI-UCB algorithm is considered.",
    "review_points_list": [
      "In Theorem 4.2, by comparing the regret lower bound with the obtained bound of LSVI-UCB-Batch in Theorem 4.1, the authors claim that the dependency of the total regret on the number of batches for the batch learning model is optimal.",
      "However, the main term $\text{sqrt}{d^3 H^3 T}$ does not match the lower bound of $d H \text{sqrt}{T}$.",
      "It would be helpful if the authors could provide a more general analysis of the batch learning model under other RL algorithms, especially those that match the lower bound.",
      "The focus of this paper is the adaptivity constrained model.",
      "However, all the two algorithms proposed are based on the LSVI-UCB algorithm proposed in Jin et al. (2020).",
      "Theorem 4.1 of this work extends Theorem 3.1 in Jin et al. (2020) by comparing the difference between $\text{Lambda}_h^{b_k}$ and $\text{Lambda}_h^{k}$ in Lemma A.3.",
      "Theorem 5.1 of this work further combines the idea of Lemma B.2 (Lemma 12, Abbasi-Yadkori et al. 2011).",
      "This work provides a new aspect of the LSVI-UCB algorithm, but the novelty is limited if only the LSVI-UCB algorithm is considered."
    ]
  },
  {
    "paper_id": "2307.02078v1",
    "submission_id": "yewqeLly5D8",
    "submission_title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
    "review_id": "JNlDotd6RZK",
    "input": {
      "title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- One limitation of the approach is that the documents are split into windows to construct the graphs.\n- This means that the model would disregard the co-occurrences of terms in the same document but in different positions of the document.\n- This means that the model may link together words close-by, but lose more distant co-occurrences.\n- It would be nice to see an experiment where this trade-off is compared.\n- Also, the experiments are tested with a window of 5.\n- I believe this is to limit the complexity of the method, as considering a bigger window size would require building a large number of subgraphs.\n- How much is the model scalable in terms of the window size?\n- One limitation of standard topic models is that they require limited size of the vocabulary.\n- By adding the complexity of creating subgraphs for each part of a document, I believe the model would require a limited vocabulary,\n- However, I could not find a comment on the practical scalability in the paper.\n- In terms of topic diversity, the results show that the proposed model performs worse when increasing the number of topics.\n- Vice versa, a simple LDA is able to consistently achieving good or even better results with the increase of the number of topics.\n- Additional comment on the results would be beneficial to understand this.",
    "review_points_list": [
      "One limitation of the approach is that the documents are split into windows to construct the graphs.",
      "This means that the model would disregard the co-occurrences of terms in the same document but in different positions of the document.",
      "This means that the model may link together words close-by, but lose more distant co-occurrences.",
      "It would be nice to see an experiment where this trade-off is compared.",
      "Also, the experiments are tested with a window of 5.",
      "I believe this is to limit the complexity of the method, as considering a bigger window size would require building a large number of subgraphs.",
      "How much is the model scalable in terms of the window size?",
      "One limitation of standard topic models is that they require limited size of the vocabulary.",
      "By adding the complexity of creating subgraphs for each part of a document, I believe the model would require a limited vocabulary,",
      "However, I could not find a comment on the practical scalability in the paper.",
      "In terms of topic diversity, the results show that the proposed model performs worse when increasing the number of topics.",
      "Vice versa, a simple LDA is able to consistently achieving good or even better results with the increase of the number of topics.",
      "Additional comment on the results would be beneficial to understand this."
    ]
  },
  {
    "paper_id": "2307.02078v1",
    "submission_id": "yewqeLly5D8",
    "submission_title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
    "review_id": "M-BulZuVFY",
    "input": {
      "title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The model is quite clearly defined. It is novel and interesting. All modeling choices are motivated by previous works, and the additional construction of a document graph using word neighborhood is highly relevant: the BOW hypothesis is one of the main weaknesses of previous topic models.\n- Even if incorporating the sequential information was already studied (e.g., HTMM), there is still room for improvement.\n- The graph that one can construct at the end when the parameters are learned (Figure 4, left) is particularly interesting for interpretability.\n- It seems that it can really improve the understanding of the topic content.\n- The approach seems to outperform (recent) prior works in term of topic coherence, diversity, and clustering.\n- Not a rejection criterion per se, but your approach is beaten in few configurations, e.g. NMI on TMN and 20NG with a high number of topic for clustering and TD score for 20NG, BNC and Reuters, again with high number of topics.\n- Do you have an intuition on why your method performs slightly worst when the number of topics grows?\n- Even if you provide the effect of the window size in supp.mat. on the 20NG dataset, it would be interesting to provide further experiments regarding this parameter, on more datasets.\n- Similarly, you should provide more experiments to test the effect of the word embedding used on the performances of your model.\n- L35: \u201cFor example, LDA discovers word such as \u201cstation\u201d in a topic which does not seem to be that insightful. \u00bb You do not introduce this example. Where does it come from? You cannot drop examples like that and expect that the reader will understand your point.\n- L 108: \u201cWe also filter out the stop words, and words and dependency edges with low frequency among the whole collection to reduce noise.\u201d: it should be in the experimental section, and you should add the parameters you use (which stop words, the threshold for selection, etc\u2026). All your experiments must be reproduceable.\n- The paper is not well written. There is a lot of grammatical errors, typos, and some sentences are not clear.\n- 2.2 : there should be a way to simplify the notation, this subsection is hard to follow.\n- In the abstract and in the experiment section, you should use present tense, not past tense.\n- Why using AVI rather than the usual expression (VAE)?\n- L 100: \u201c we can donate a document\u201d this is not clear, donate does not fit here\n- L 104: \u201cwith nodes on the vocabulary\u201d : should be \u201cin the vocabulary\u201d\n- L 128:  just based \uf0e0 based solely on the topic assignment ?\n- L 136 : should be cast into \uf0e0 cast as\n- L 210 : \u201cNow, the only challenge to compute the ELBO loss Ld is at the last term in Eqaution: \u201c this sentence is not grammatically correct and Eqaution should be Equation\n- L 58 : Graphic Neural Topic Modeling \uf0e0 Graph Neural Topic Modeling. This is the name of your method!\n- I've read the authors' response and the other reviews. The authors answered most of my remarks and I choose to keep my score of 7.\n- I'd be interested to read the reactions of the other reviewers before, but they never came...",
    "review_points_list": [
      "The model is quite clearly defined. It is novel and interesting. All modeling choices are motivated by previous works, and the additional construction of a document graph using word neighborhood is highly relevant: the BOW hypothesis is one of the main weaknesses of previous topic models.",
      "Even if incorporating the sequential information was already studied (e.g., HTMM), there is still room for improvement.",
      "The graph that one can construct at the end when the parameters are learned (Figure 4, left) is particularly interesting for interpretability.",
      "It seems that it can really improve the understanding of the topic content.",
      "The approach seems to outperform (recent) prior works in term of topic coherence, diversity, and clustering.",
      "Not a rejection criterion per se, but your approach is beaten in few configurations, e.g. NMI on TMN and 20NG with a high number of topic for clustering and TD score for 20NG, BNC and Reuters, again with high number of topics.",
      "Do you have an intuition on why your method performs slightly worst when the number of topics grows?",
      "Even if you provide the effect of the window size in supp.mat. on the 20NG dataset, it would be interesting to provide further experiments regarding this parameter, on more datasets.",
      "Similarly, you should provide more experiments to test the effect of the word embedding used on the performances of your model.",
      "L35: \u201cFor example, LDA discovers word such as \u201cstation\u201d in a topic which does not seem to be that insightful. \u00bb You do not introduce this example. Where does it come from? You cannot drop examples like that and expect that the reader will understand your point.",
      "L 108: \u201cWe also filter out the stop words, and words and dependency edges with low frequency among the whole collection to reduce noise.\u201d: it should be in the experimental section, and you should add the parameters you use (which stop words, the threshold for selection, etc\u2026). All your experiments must be reproduceable.",
      "The paper is not well written. There is a lot of grammatical errors, typos, and some sentences are not clear.",
      "2.2 : there should be a way to simplify the notation, this subsection is hard to follow.",
      "In the abstract and in the experiment section, you should use present tense, not past tense.",
      "Why using AVI rather than the usual expression (VAE)?",
      "L 100: \u201c we can donate a document\u201d this is not clear, donate does not fit here",
      "L 104: \u201cwith nodes on the vocabulary\u201d : should be \u201cin the vocabulary\u201d",
      "L 128:  just based \uf0e0 based solely on the topic assignment ?",
      "L 136 : should be cast into \uf0e0 cast as",
      "L 210 : \u201cNow, the only challenge to compute the ELBO loss Ld is at the last term in Eqaution: \u201c this sentence is not grammatically correct and Eqaution should be Equation",
      "L 58 : Graphic Neural Topic Modeling \uf0e0 Graph Neural Topic Modeling. This is the name of your method!",
      "I've read the authors' response and the other reviews. The authors answered most of my remarks and I choose to keep my score of 7.",
      "I'd be interested to read the reactions of the other reviewers before, but they never came..."
    ]
  },
  {
    "paper_id": "2307.02078v1",
    "submission_id": "yewqeLly5D8",
    "submission_title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
    "review_id": "bIAJR9CCh8r",
    "input": {
      "title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Representing each document as a directed word graph is interesting, which could potentially capture complex and nonlinear relationships among words. The idea of integrating GCN with the topic model can also be found in [1-3]. The difference is that the reconstruction of the directed graph given a set of sampled latent topic assignments and the word generation given the reconstructed graph and topic assignment, which can be the strength and could be also the weakness. The strength resides in that it could potentially learn the semantic correlation between words. However, my concern is also about the graph reconstruction. I am a bit confused by G_d and G^o_d (the placeholder) in regards to the generative process. It would be good to explain it with the example in figure 1 (a). For instance, is it assumed that the relative position of words is fixed in G^o_d, the reconstruction is just about deciding if there is a directed edge between two words. However, eq (4) seems to say E^o_d is observed, and line 107 indicates it is different from E_d. Further clarity is needed here.\n- In the proposed model, each topic is also a distribution over edges between pairs of words. Presumably, there is a one to one making between a topic as a distribution over words and that as a distribution over edges. I wonder if the topic assignment to edges is independent of the topic assigned to a word, or they are dependent.\n- Both topic coherence scores and downstream clustering tasks are used in the experiment, and the proposed GNTM is compared with several neural topic models, showing promising results. Here are my comments: In regards to the topic coherence scores, it is not unknown that consistent document preprocessing can significantly affect the topic coherence scores, reporting how documents are preprocessed is necessary. There is a lack of oblation study on the impact of representing topics as a distribution over edges. Besides GraphBTM, it might also be good to consider those GCN based topic models, like [1-3]. It is good to show how sensitive the model is to the window size used in construction G_d.",
    "review_points_list": [
      "Representing each document as a directed word graph is interesting, which could potentially capture complex and nonlinear relationships among words. The idea of integrating GCN with the topic model can also be found in [1-3]. The difference is that the reconstruction of the directed graph given a set of sampled latent topic assignments and the word generation given the reconstructed graph and topic assignment, which can be the strength and could be also the weakness. The strength resides in that it could potentially learn the semantic correlation between words. However, my concern is also about the graph reconstruction. I am a bit confused by G_d and G^o_d (the placeholder) in regards to the generative process. It would be good to explain it with the example in figure 1 (a). For instance, is it assumed that the relative position of words is fixed in G^o_d, the reconstruction is just about deciding if there is a directed edge between two words. However, eq (4) seems to say E^o_d is observed, and line 107 indicates it is different from E_d. Further clarity is needed here.",
      "In the proposed model, each topic is also a distribution over edges between pairs of words. Presumably, there is a one to one making between a topic as a distribution over words and that as a distribution over edges. I wonder if the topic assignment to edges is independent of the topic assigned to a word, or they are dependent.",
      "Both topic coherence scores and downstream clustering tasks are used in the experiment, and the proposed GNTM is compared with several neural topic models, showing promising results. Here are my comments: In regards to the topic coherence scores, it is not unknown that consistent document preprocessing can significantly affect the topic coherence scores, reporting how documents are preprocessed is necessary. There is a lack of oblation study on the impact of representing topics as a distribution over edges. Besides GraphBTM, it might also be good to consider those GCN based topic models, like [1-3]. It is good to show how sensitive the model is to the window size used in construction G_d."
    ]
  },
  {
    "paper_id": "2307.02078v1",
    "submission_id": "yewqeLly5D8",
    "submission_title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
    "review_id": "bV04QEFKTVp",
    "input": {
      "title": "Topic Modeling Revisited: A Document Graph-based Neural Network Perspective",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- paper is well formulated and easy to read/follow\n- the proposed methodology is well detailed and described\n- novel approach of introducing document-graph structure in the generative process of topic modeling\n- novel AVI approach of learning over GNTM model\n- Unclear contribution since several existing works in topic modeling have already investigated incorporating word-dependency relations in the generative process of neural topic modeling\n- comparison to the baselines methods used is not fair and incomplete\n- the baseline methods do not use pretrained word embeddings (e.g. word2vec), however the proposed model GNTM utilizes external knowledge via pre-trained embeddings word2vec. Therefore, comparison is unfair\n- missing comparison with the topic models that incorporates word dependency relations such as [1, 2, 3]\n- experimental setup is unclear; how are the scores of baseline methods obtained?\n- Additional experiments required to evaluate the quality of document representations such as text classification and unsupervised IR.\n- DocNADE and its variants such as iDocNADE/iDocNADEe have shown to outperform [3] ProdLDA; however the score of iDocNADE are low. Please provide experimental details and reasoning on the deviation of scores from original works?\n- The proposed GNTM model incorporates word dependency relations, while the baseline methods do not and only assumes bow. Therefore, additional comparisons/experiments with the topic models variants such as [1, 2, 3] that incorporate (sequential/syntactic/contextualized) word-dependent relations are required for completeness.",
    "review_points_list": [
      "paper is well formulated and easy to read/follow",
      "the proposed methodology is well detailed and described",
      "novel approach of introducing document-graph structure in the generative process of topic modeling",
      "novel AVI approach of learning over GNTM model",
      "Unclear contribution since several existing works in topic modeling have already investigated incorporating word-dependency relations in the generative process of neural topic modeling",
      "comparison to the baselines methods used is not fair and incomplete",
      "the baseline methods do not use pretrained word embeddings (e.g. word2vec), however the proposed model GNTM utilizes external knowledge via pre-trained embeddings word2vec. Therefore, comparison is unfair",
      "missing comparison with the topic models that incorporates word dependency relations such as [1, 2, 3]",
      "experimental setup is unclear; how are the scores of baseline methods obtained?",
      "Additional experiments required to evaluate the quality of document representations such as text classification and unsupervised IR.",
      "DocNADE and its variants such as iDocNADE/iDocNADEe have shown to outperform [3] ProdLDA; however the score of iDocNADE are low. Please provide experimental details and reasoning on the deviation of scores from original works?",
      "The proposed GNTM model incorporates word dependency relations, while the baseline methods do not and only assumes bow. Therefore, additional comparisons/experiments with the topic models variants such as [1, 2, 3] that incorporate (sequential/syntactic/contextualized) word-dependent relations are required for completeness."
    ]
  },
  {
    "paper_id": "2105.02358v2",
    "submission_id": "wRXzOa2z5T",
    "submission_title": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "review_id": "dNAUsm3s6f8",
    "input": {
      "title": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Strengths:\n- - The paper reaches very good results in tabular datasets. They reach the best results in 4 out of 10 datasets, better than any other method. When separated into the three main tasks: binary classification, multi-class classification, and regression, they reach the best results in binary classification, joined best results in multi-class classification, and joined second-best results in regression. Overall, it can be considered that the method works at least on par (in my opinion, slightly better) than the two main gradient boosting methods: XGBoost and CatBoost.\n- - Understanding the method is quite straightforward, the authors do a good job at explaining it.\n- - The authors do a good job at giving insights after they present their method.\n- Weaknesses:\n- 1) Massively overstated claim.\n- The first sentence of the paper 'We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.' gives the impression that this is the first paper that tries to model some sort of relation between the data, instead of input-output relation which is commonly done in deep learning.\n- However, this is a very misleading statement, contextual relations between the data in neural networks have been going on at least since RNNs were invented.\n- More interestingly, there are entire fields in deep learning whose sole purpose is to model the relations between the data.\n- Those fields fall under the umbrella of similarity learning, and are specialized in different directions like metric learning, person re-identification, face recognition etc.\n- In all those fields, the idea is that the embedding/results of a data point are not dependent only on itself, but also on the other data points.\n- Typically that was done using some sort of contrastive/triplet loss, but nowadays that is done by using the relations between all the pairs in the dataset.\n- That could be done as some sort of ranking, proxy, or contextual procedure.\n- 2) Misleading statement -\n- line 62-63: We provide the model with the entire dataset \u2013 all datapoints \u2013 as input.\n- line 72: NPTs take as input the entire dataset $X \\\\in R^{n\u00d7d}$.\n- My first impression was how is this scalable, and I was expecting a sophisticated way of somehow memorizing the dataset or using some hashing procedure that allows that.\n- To my disappointment, it quickly became clear that the authors, in fact, do not use the entire dataset.\n- Instead, they use minibatches of the dataset (lines 173-179), something that is done in virtually every other deep learning paper (including the ones I mentioned above).\n- 3) Very poor results in CIFAR10/MNIST:\n- I was initially disappointed that when it comes to image datasets, the method is tested only in very simple and long-outdated datasets, like MNIST and CIFAR-10.\n- I would have expected that 'a general framework' that goes 'beyond deep learning' would actually do some more realistic evaluations in more complicated datasets.\n- However, I was even more disappointed to see the results in CIFAR10, where the method reaches only 68.2%.\n- CIFAR10 is a dataset where SOTA is over 99.5% reached by a method that uses Transformers [H].\n- Even compared to only non-parametric methods (which the method claims to be, although it is a mix between parametric and non-parametric), the results in other works are considerably better (75.89% [I]).\n- Similarly, the method reaches only 98.3% accuracy in MNIST, way below other papers that reach 99.8% (including more than a decade old work of [J]) or the non-parametric method of [I] that reaches 99.44%.\n- Verdict: If the paper would have been presented for what it is: a paper that combines ideas from other fields to progress the task of classification/regression in tabular datasets, this paper would have been an interesting one, and probably deserving of getting published at NeurIPS.\n- However, the claims of the paper are massively exaggerated, the related work does not mention other papers that roughly do the same thing (in more complicated tasks), making the abstract, introduction, and conclusions completely misleading.\n- Adding to that, the weak results in image datasets, I do not see the paper being fit for publishing at the NeurIPS conference.\n- My low score is quite heavily weighted towards the misleading parts of the paper and it promising far more than it shows.",
    "review_points_list": [
      "Strengths:",
      "- The paper reaches very good results in tabular datasets. They reach the best results in 4 out of 10 datasets, better than any other method. When separated into the three main tasks: binary classification, multi-class classification, and regression, they reach the best results in binary classification, joined best results in multi-class classification, and joined second-best results in regression. Overall, it can be considered that the method works at least on par (in my opinion, slightly better) than the two main gradient boosting methods: XGBoost and CatBoost.",
      "- Understanding the method is quite straightforward, the authors do a good job at explaining it.",
      "- The authors do a good job at giving insights after they present their method.",
      "Weaknesses:",
      "1) Massively overstated claim.",
      "The first sentence of the paper 'We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.' gives the impression that this is the first paper that tries to model some sort of relation between the data, instead of input-output relation which is commonly done in deep learning.",
      "However, this is a very misleading statement, contextual relations between the data in neural networks have been going on at least since RNNs were invented.",
      "More interestingly, there are entire fields in deep learning whose sole purpose is to model the relations between the data.",
      "Those fields fall under the umbrella of similarity learning, and are specialized in different directions like metric learning, person re-identification, face recognition etc.",
      "In all those fields, the idea is that the embedding/results of a data point are not dependent only on itself, but also on the other data points.",
      "Typically that was done using some sort of contrastive/triplet loss, but nowadays that is done by using the relations between all the pairs in the dataset.",
      "That could be done as some sort of ranking, proxy, or contextual procedure.",
      "2) Misleading statement -",
      "line 62-63: We provide the model with the entire dataset \u2013 all datapoints \u2013 as input.",
      "line 72: NPTs take as input the entire dataset $X \\\\in R^{n\u00d7d}$.",
      "My first impression was how is this scalable, and I was expecting a sophisticated way of somehow memorizing the dataset or using some hashing procedure that allows that.",
      "To my disappointment, it quickly became clear that the authors, in fact, do not use the entire dataset.",
      "Instead, they use minibatches of the dataset (lines 173-179), something that is done in virtually every other deep learning paper (including the ones I mentioned above).",
      "3) Very poor results in CIFAR10/MNIST:",
      "I was initially disappointed that when it comes to image datasets, the method is tested only in very simple and long-outdated datasets, like MNIST and CIFAR-10.",
      "I would have expected that 'a general framework' that goes 'beyond deep learning' would actually do some more realistic evaluations in more complicated datasets.",
      "However, I was even more disappointed to see the results in CIFAR10, where the method reaches only 68.2%.",
      "CIFAR10 is a dataset where SOTA is over 99.5% reached by a method that uses Transformers [H].",
      "Even compared to only non-parametric methods (which the method claims to be, although it is a mix between parametric and non-parametric), the results in other works are considerably better (75.89% [I]).",
      "Similarly, the method reaches only 98.3% accuracy in MNIST, way below other papers that reach 99.8% (including more than a decade old work of [J]) or the non-parametric method of [I] that reaches 99.44%.",
      "Verdict: If the paper would have been presented for what it is: a paper that combines ideas from other fields to progress the task of classification/regression in tabular datasets, this paper would have been an interesting one, and probably deserving of getting published at NeurIPS.",
      "However, the claims of the paper are massively exaggerated, the related work does not mention other papers that roughly do the same thing (in more complicated tasks), making the abstract, introduction, and conclusions completely misleading.",
      "Adding to that, the weak results in image datasets, I do not see the paper being fit for publishing at the NeurIPS conference.",
      "My low score is quite heavily weighted towards the misleading parts of the paper and it promising far more than it shows."
    ]
  },
  {
    "paper_id": "2105.02358v2",
    "submission_id": "wRXzOa2z5T",
    "submission_title": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "review_id": "xKrH_ooJx-D",
    "input": {
      "title": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This is a high-quality paper that is exceptionally well-written, well-structured and very clear in its exposition.\n- The idea is novel and well-positioned against related work (with some exceptions).\n- I think that this paper is relevant for the NeurIPS community and I expect that it will inspire a range of follow-up works.\n- The idea of using self-attention to take into account all other data points (including their labels, if available) both during training and inference is intriguing, and given the rising popularity Transformer-style architectures in the machine learning community, I expect that this method will spawn lots of interest despite its shortcomings in its current form, such as its computational complexity and its limited usefulness/applicability in any of the studied tasks in the paper.\n- The experimental evaluation is carried out with great care and the authors come up with several insightful experiments to study whether the attention between datapoint (ABD) mechanism learns to make use of features/labels of similar data points.\n- Overall, I think that this paper can be accepted and that it would make a great addition to the conference program.\n- I do, however, want to highlight several limitations of the method and issues with the paper below, which I encourage the authors to take into account in the next version of the paper.\n- The core contribution of the paper is the attention between datapoint (ABD) mechanism and its application for semi-supervised learning in tabular data.\n- While the other model contribution, namely attention between attributes (ABA) is interesting, it is not properly experimentally evaluated (e.g. against an alternative, simpler architecture component, such as an MLP on the flattened representation, or simply the identity, i.e. no module at all).\n- The reader might wonder why this module is introduced in the first place, and its necessity is unclear.\n- I could imagine that this module could simply be left out for the tabular tasks without affecting performance in a significant way.\n- For the visual task, it could likely be replaced with a CNN encoder as the Embedding function (Figure 2b), as done in prior work [1], which would reduce computational complexity and likely not hurt performance.\n- I encourage the authors to revisit the choice of why ABA is introduced in the first place.\n- One core limitation of the method is its computational complexity.\n- The authors highlight strongly competitive results on tabular data, but this only takes into account the validation error / accuracy.\n- I strongly suspect that the proposed method is by far not competitive with the shown baselines when taking into account computation time and memory requirement.\n- These limitations should be highlighted in the main paper (not in the appendix) by showing a direct comparison in terms of wall clock training/inference time (and memory) to give researchers and practitioners a clear picture about the practicality of the technique.\n- I agree with the authors that alleviating this limitation is a promising avenue for future work.\n- In terms of related work, I encourage the authors to use the additional page for the camera-ready version (should the paper be accepted) to include the discussion on graph-based semi-supervised learning using graph neural networks in the main paper (instead of the appendix), as the closest relatives to the proposed model come from this research area.\n- This includes a highly-cited paper [1] which similarly introduces an attention mechanism that aggregates information across data points for semi-supervised learning and few-shot learning in an architecture that is closely related to the one presented here.\n- This paper is currently not cited/discussed, but I think it should be closely contrasted against the proposed technique.\n- Other suggestions for improvement:\n- The CIFAR-10 and MNIST results are currently very difficult to interpret in the absence of any baselines.\n- A comparison against a variant of the model that does not use attention between data points (in the main paper) would significantly strengthen this section.\n- The statement that 'they can learn to completely ignore other inputs, essentially collapsing into a standard parametric model' in the context of perturbed features (Section 4.3) should be verified, e.g. by comparing the attention weights with and without perturbation of the features.",
    "review_points_list": [
      "This is a high-quality paper that is exceptionally well-written, well-structured and very clear in its exposition.",
      "The idea is novel and well-positioned against related work (with some exceptions).",
      "I think that this paper is relevant for the NeurIPS community and I expect that it will inspire a range of follow-up works.",
      "The idea of using self-attention to take into account all other data points (including their labels, if available) both during training and inference is intriguing, and given the rising popularity Transformer-style architectures in the machine learning community, I expect that this method will spawn lots of interest despite its shortcomings in its current form, such as its computational complexity and its limited usefulness/applicability in any of the studied tasks in the paper.",
      "The experimental evaluation is carried out with great care and the authors come up with several insightful experiments to study whether the attention between datapoint (ABD) mechanism learns to make use of features/labels of similar data points.",
      "Overall, I think that this paper can be accepted and that it would make a great addition to the conference program.",
      "I do, however, want to highlight several limitations of the method and issues with the paper below, which I encourage the authors to take into account in the next version of the paper.",
      "The core contribution of the paper is the attention between datapoint (ABD) mechanism and its application for semi-supervised learning in tabular data.",
      "While the other model contribution, namely attention between attributes (ABA) is interesting, it is not properly experimentally evaluated (e.g. against an alternative, simpler architecture component, such as an MLP on the flattened representation, or simply the identity, i.e. no module at all).",
      "The reader might wonder why this module is introduced in the first place, and its necessity is unclear.",
      "I could imagine that this module could simply be left out for the tabular tasks without affecting performance in a significant way.",
      "For the visual task, it could likely be replaced with a CNN encoder as the Embedding function (Figure 2b), as done in prior work [1], which would reduce computational complexity and likely not hurt performance.",
      "I encourage the authors to revisit the choice of why ABA is introduced in the first place.",
      "One core limitation of the method is its computational complexity.",
      "The authors highlight strongly competitive results on tabular data, but this only takes into account the validation error / accuracy.",
      "I strongly suspect that the proposed method is by far not competitive with the shown baselines when taking into account computation time and memory requirement.",
      "These limitations should be highlighted in the main paper (not in the appendix) by showing a direct comparison in terms of wall clock training/inference time (and memory) to give researchers and practitioners a clear picture about the practicality of the technique.",
      "I agree with the authors that alleviating this limitation is a promising avenue for future work.",
      "In terms of related work, I encourage the authors to use the additional page for the camera-ready version (should the paper be accepted) to include the discussion on graph-based semi-supervised learning using graph neural networks in the main paper (instead of the appendix), as the closest relatives to the proposed model come from this research area.",
      "This includes a highly-cited paper [1] which similarly introduces an attention mechanism that aggregates information across data points for semi-supervised learning and few-shot learning in an architecture that is closely related to the one presented here.",
      "This paper is currently not cited/discussed, but I think it should be closely contrasted against the proposed technique.",
      "Other suggestions for improvement:",
      "The CIFAR-10 and MNIST results are currently very difficult to interpret in the absence of any baselines.",
      "A comparison against a variant of the model that does not use attention between data points (in the main paper) would significantly strengthen this section.",
      "The statement that 'they can learn to completely ignore other inputs, essentially collapsing into a standard parametric model' in the context of perturbed features (Section 4.3) should be verified, e.g. by comparing the attention weights with and without perturbation of the features."
    ]
  },
  {
    "paper_id": "2105.02358v2",
    "submission_id": "wRXzOa2z5T",
    "submission_title": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "review_id": "iUE_ZKFh4le",
    "input": {
      "title": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The intersection of non-parametric models and deep parametric models is an area that has garnered much attention (and rightfully so, in my opinion).\n- Works such as Neural Processes, Deep Gaussian Processes, and Deep Kernel Learning have had significant impact.\n- The proposed NTP model advances these previous works in significant, not incremental, ways.\n- It provides a novel architecture, which has capabilities not achieved by previous works.\n- In particular, I believe the paper offers the following merits:\n- * **Innovative Architecture** - The NPT architecture is novel and interesting and offers new perspectives on the ways to design and parameterize deep transformer-like architectures.\n- * **Inspiration for Future Work** - The paper has the potential to lead to future work both in advancing NPT-like methodologies as well as applications of NPT to a wide array of other tasks.\n- * **Empirical Analysis** - The authors carefully design experiments to test the performance of the model in different characteristics of the model. These provide insights into how the model behaves in particular circumstances.\n- * **Well written** - The manuscript is very clearly written and is a pleasure to read. The figures and illustrations are clear and informative.\n- The authors carefully design experiments to test the performance of the model in different characteristics of the model.\n- These provide insights into how the model behaves in particular circumstances.\n- The description of the model is also clear and well motivated.\n- Overall, I think that this is a very strong paper.\n- Here are some ways in which I think it could be improved:\n- * **Retrieve and Edit** - There are seemingly some similarities between approaches such as retrieve-and-edit models that retrieve relevant instances and transform them to make a prediction for a target instance, e.g. (Hashimoto et al, 2018), inter-alia.\n- * **Transductive, Case-based, Label Propagation** - Classic approaches such as transductive learning or case-based reasoning I believe are also quite relevant to NPT.\n- * **Structured Prediction** - Many structured prediction tasks can naturally extended their associated models / objectives / energy functions to the dataset level, it might be interest to understand relationships between NPT and structured prediction models.\n- * **Differences from Interpolation** - I am a bit unsure that I follow the argument in lines 170-172: 'For example, NPTs could learn to assign test datapoints to clusters of training datapoints, and predict on those points using interpolation of the training targets in their respective cluster.'\n- Isn't an interpolative to put all interpolation weight on the unmasked 'copy' of the masked instance?\n- * **Theoretical results** - I don't think this is a requirement of the paper.\n- Thanks very much to the authors for their response.\n- I believe this is a interesting paper with many notable merits.",
    "review_points_list": [
      "The intersection of non-parametric models and deep parametric models is an area that has garnered much attention (and rightfully so, in my opinion).",
      "Works such as Neural Processes, Deep Gaussian Processes, and Deep Kernel Learning have had significant impact.",
      "The proposed NTP model advances these previous works in significant, not incremental, ways.",
      "It provides a novel architecture, which has capabilities not achieved by previous works.",
      "In particular, I believe the paper offers the following merits:",
      "* **Innovative Architecture** - The NPT architecture is novel and interesting and offers new perspectives on the ways to design and parameterize deep transformer-like architectures.",
      "* **Inspiration for Future Work** - The paper has the potential to lead to future work both in advancing NPT-like methodologies as well as applications of NPT to a wide array of other tasks.",
      "* **Empirical Analysis** - The authors carefully design experiments to test the performance of the model in different characteristics of the model. These provide insights into how the model behaves in particular circumstances.",
      "* **Well written** - The manuscript is very clearly written and is a pleasure to read. The figures and illustrations are clear and informative.",
      "The authors carefully design experiments to test the performance of the model in different characteristics of the model.",
      "These provide insights into how the model behaves in particular circumstances.",
      "The description of the model is also clear and well motivated.",
      "Overall, I think that this is a very strong paper.",
      "Here are some ways in which I think it could be improved:",
      "* **Retrieve and Edit** - There are seemingly some similarities between approaches such as retrieve-and-edit models that retrieve relevant instances and transform them to make a prediction for a target instance, e.g. (Hashimoto et al, 2018), inter-alia.",
      "* **Transductive, Case-based, Label Propagation** - Classic approaches such as transductive learning or case-based reasoning I believe are also quite relevant to NPT.",
      "* **Structured Prediction** - Many structured prediction tasks can naturally extended their associated models / objectives / energy functions to the dataset level, it might be interest to understand relationships between NPT and structured prediction models.",
      "* **Differences from Interpolation** - I am a bit unsure that I follow the argument in lines 170-172: 'For example, NPTs could learn to assign test datapoints to clusters of training datapoints, and predict on those points using interpolation of the training targets in their respective cluster.'",
      "Isn't an interpolative to put all interpolation weight on the unmasked 'copy' of the masked instance?",
      "* **Theoretical results** - I don't think this is a requirement of the paper.",
      "Thanks very much to the authors for their response.",
      "I believe this is a interesting paper with many notable merits."
    ]
  },
  {
    "paper_id": "1203.0567v1",
    "submission_id": "te8iyHjbPQd",
    "submission_title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
    "review_id": "lYA2RwVKMX6",
    "input": {
      "title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is quite clearly written, easy to follow, and the authors describe their contributions in a transparent fashion.\n- My only reservations are related to the significance of the results, particularly for the machine learning community.\n- There are plenty of ways of getting around the aforementioned under-estimation bias,\n- It is arguably more common to use quantile regression as a sub-routine in a larger conformal prediction type of procedure,  rather than to pay particular attention to the coverage of the quantile regressor itself.\n- The results here are of a fundamentally statistical nature, which I still think is fine for NeurIPS,\n- Considering that quantile regression is a now-classical and well-studied topic, I would be surprised if similar insights did not already exist in the statistics literature,\n- But I am not sufficiently familiar with that literature.\n- Taking these points together, my recommendation is a borderline accept.\n- I thank the authors for their response.\n- Having read through the authors' response and the other reviews, my overall opinion remains unchanged.",
    "review_points_list": [
      "The paper is quite clearly written, easy to follow, and the authors describe their contributions in a transparent fashion.",
      "My only reservations are related to the significance of the results, particularly for the machine learning community.",
      "There are plenty of ways of getting around the aforementioned under-estimation bias,",
      "It is arguably more common to use quantile regression as a sub-routine in a larger conformal prediction type of procedure,  rather than to pay particular attention to the coverage of the quantile regressor itself.",
      "The results here are of a fundamentally statistical nature, which I still think is fine for NeurIPS,",
      "Considering that quantile regression is a now-classical and well-studied topic, I would be surprised if similar insights did not already exist in the statistics literature,",
      "But I am not sufficiently familiar with that literature.",
      "Taking these points together, my recommendation is a borderline accept.",
      "I thank the authors for their response.",
      "Having read through the authors' response and the other reviews, my overall opinion remains unchanged."
    ]
  },
  {
    "paper_id": "1203.0567v1",
    "submission_id": "te8iyHjbPQd",
    "submission_title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
    "review_id": "cuWsBd_GgQ",
    "input": {
      "title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- As far as I know, the analysis is new and sheds light on a known but unexplained phenomenon.\n- It builds on recent advances in high-dimensional M-estimation but introduces a novel concentration argument to deal with the additional learnable bias, which was not considered in previous works.\n- This paper is a complete and sound piece of work addressing an important problem of quantile regression.\n- I am not familiar with all the technical elements of the proofs in the Appendix, so I can't say too much about their correctness.\n- The proof overviews in the main text are helpful.\n- The noise smoothness assumptions of the main theorem seem reasonable and those of theorem 3 allow to consider more general data distributions.\n- The experimental results nicely match the proposed theory in realistic situations.\n- The paper is very well organized and written which makes it very pleasant to read.\n- There is a very good literature review that positions it with respect to prior work.\n- These results will be of interest for the machine learning communities working on quantile regression to potentially build on these results and extend them to more general settings.\n- Assumption A should specify what is \u03b1 , is it \u03b1\u2208(0.5,1) ?\n- Line 162: small should be more precise\n- Line 192: I think it should say 'under-coverage' instead of 'over-coverage'\n- Line 245: K-sub-Gaussian should be defined\n- Line 252: small should be more precise\n- arXiv versions should be replaced by their peer-reviewed versions when possible, e.g.: Y. Romano, E. Patterson, and E. J. Cand\u00e8s. Conformalized quantile regression. arXiv preprint 471 arXiv:1905.03222, 2019.\n- 57 is incomplete",
    "review_points_list": [
      "As far as I know, the analysis is new and sheds light on a known but unexplained phenomenon.",
      "It builds on recent advances in high-dimensional M-estimation but introduces a novel concentration argument to deal with the additional learnable bias, which was not considered in previous works.",
      "This paper is a complete and sound piece of work addressing an important problem of quantile regression.",
      "I am not familiar with all the technical elements of the proofs in the Appendix, so I can't say too much about their correctness.",
      "The proof overviews in the main text are helpful.",
      "The noise smoothness assumptions of the main theorem seem reasonable and those of theorem 3 allow to consider more general data distributions.",
      "The experimental results nicely match the proposed theory in realistic situations.",
      "The paper is very well organized and written which makes it very pleasant to read.",
      "There is a very good literature review that positions it with respect to prior work.",
      "These results will be of interest for the machine learning communities working on quantile regression to potentially build on these results and extend them to more general settings.",
      "Assumption A should specify what is \u03b1 , is it \u03b1\u2208(0.5,1) ?",
      "Line 162: small should be more precise",
      "Line 192: I think it should say 'under-coverage' instead of 'over-coverage'",
      "Line 245: K-sub-Gaussian should be defined",
      "Line 252: small should be more precise",
      "arXiv versions should be replaced by their peer-reviewed versions when possible, e.g.: Y. Romano, E. Patterson, and E. J. Cand\u00e8s. Conformalized quantile regression. arXiv preprint 471 arXiv:1905.03222, 2019.",
      "57 is incomplete"
    ]
  },
  {
    "paper_id": "1203.0567v1",
    "submission_id": "te8iyHjbPQd",
    "submission_title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
    "review_id": "thLBKPn9Ole",
    "input": {
      "title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The authors give a clear contribution in explaining the under-coverage phenomenon in linear quantile regression.\n- The analysis, as the authors claim, is unlike existing results in that it captures the sign of the bias, and is the first rigorous theoretical justification on the under-coverage phenomenon.\n- The study can open up more research questions along this line.\n- Only the classical linear quantile regression algorithm with pinball loss is analyzed, and it is unclear whether the insight is generalizable.\n- As the authors are aware, there are many advanced uncertainty estimation algorithms (e.g., conformal-prediction-based approaches, quantile regression forests etc.), and the paper would be much stronger if similar insights could be drawn to these advanced approaches.\n- For instance, the bias term which can have either under- or over-coverage effect seems to be very specific to the linear model, and it is unclear any generalization of such insights could be drawn to other models.\n- Theorem 1 requires that both n and d go to infinity.\n- It is unclear how easy/meaningful it is to 'model' an increasing data dimension beyond linear (although I understand the authors' motivation in letting d go to infinity to highlight the nontrivial impact of the under-coverage bias).\n- The authors are aware of the limitation.\n- The paper is well-written.\n- It is easy to read.\n- It is easy to understand the main theorems.\n- Intuitions are well presented.\n- In line 192, 'the over-coverage should get more severe as \u03ba\u2014the measure of over-parametrization in this problem\u2014gets larger'.\n- Should 'over-coverage' be under-coverage bias?\n- The authors present a rigorous theoretical study on the under-coverage of uncertainty estimation algorithms in learning quantiles, focusing on linear quantile regression.\n- The analysis giving rise to the sign of the coverage bias seems novel.\n- Except for the sign of the coverage bias, the finite-sample estimation error of linear quantile regression is well-studied, e.g., in Steinwart et al., 2011.",
    "review_points_list": [
      "The authors give a clear contribution in explaining the under-coverage phenomenon in linear quantile regression.",
      "The analysis, as the authors claim, is unlike existing results in that it captures the sign of the bias, and is the first rigorous theoretical justification on the under-coverage phenomenon.",
      "The study can open up more research questions along this line.",
      "Only the classical linear quantile regression algorithm with pinball loss is analyzed, and it is unclear whether the insight is generalizable.",
      "As the authors are aware, there are many advanced uncertainty estimation algorithms (e.g., conformal-prediction-based approaches, quantile regression forests etc.), and the paper would be much stronger if similar insights could be drawn to these advanced approaches.",
      "For instance, the bias term which can have either under- or over-coverage effect seems to be very specific to the linear model, and it is unclear any generalization of such insights could be drawn to other models.",
      "Theorem 1 requires that both n and d go to infinity.",
      "It is unclear how easy/meaningful it is to 'model' an increasing data dimension beyond linear (although I understand the authors' motivation in letting d go to infinity to highlight the nontrivial impact of the under-coverage bias).",
      "The authors are aware of the limitation.",
      "The paper is well-written.",
      "It is easy to read.",
      "It is easy to understand the main theorems.",
      "Intuitions are well presented.",
      "In line 192, 'the over-coverage should get more severe as \u03ba\u2014the measure of over-parametrization in this problem\u2014gets larger'.",
      "Should 'over-coverage' be under-coverage bias?",
      "The authors present a rigorous theoretical study on the under-coverage of uncertainty estimation algorithms in learning quantiles, focusing on linear quantile regression.",
      "The analysis giving rise to the sign of the coverage bias seems novel.",
      "Except for the sign of the coverage bias, the finite-sample estimation error of linear quantile regression is well-studied, e.g., in Steinwart et al., 2011."
    ]
  },
  {
    "paper_id": "2107.04086v3",
    "submission_id": "wGmOLwb8ClT",
    "submission_title": "Robust Counterfactual Explanations on Graph Neural Networks",
    "review_id": "6ZB0hoQSY-G",
    "input": {
      "title": "Robust Counterfactual Explanations on Graph Neural Networks",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Though some concepts like LDB and counterfactual have been used in other domains, to my knowledge, the framework proposed by this paper is original in the field of GNN explanation.\n- However, the definition of counterfactual explanation seems broken.\n- This paper is well-written and the authors make their idea and the workflow clear.\n- However, the reviewer thinks more experiments are needed to demonstrate the effectiveness of the proposed method.\n- In addition, the definition of noise in this paper is not very clear, without which robustness can\u2019t be clarified as well.\n- the structure of this paper is well organized.\n- It starts with the intuition and then get into the tech details.\n- Each module(e.g. LDBs, counterfactual subgraph) is introduced following the pipeline of the proposed method, which makes the paper structure quite clear.\n- The experiments are conducted on three small datasets and consider several relevant baselines for GNN explanation.\n- The results are significantly better than the baseline approaches on criteria fidelity and AUC save for the AUC performance on Mutagenicity.\n- According to the previous, the definition of counterfactual explanation is \u201cif X had been different, Y would not have occurred\u201d, rather than \u201cif X had not occurred, Y would not have occurred\u201d as claimed by the authors.\n- That means, the authors only replace the subgraph (X) with the empty set, rather than the reference subgraph (that could be the empty or other subgraphs).\n- Moreover, recent studies find that simply replacing the features of interest (X) with the empty could cause the distribution shift from the complete graphs.\n- Please explain or clarify these points.\n- I cannot find a clear definition of noise in this paper.\n- I think it\u2019s necessary to clarify it when you are discussing robustness.\n- For example, what does 30% noise mean?\n- Adding random noise whose magnitude is 30% of the original node feature or randomly adding or deleting 30% edges or do them both?\n- It will be much better if the authors can explain more why the robustness defined in this paper is important.\n- In CV domain, after adding noise to the panda picture, the NN cannot classify it correctly anymore while human being can\u2019t see any difference between the original pic and the noisy one.\n- However, taking mutagenicity for example, adding or deleting 30% of edges will make the original molecule a totally different one even though the GNN doesn\u2019t change the prediction(noxious or not).\n- In these case, it\u2019s reasonable for an explainer to identify a different subgraph.\n- So, I don\u2019t think evaluating the robustness based on the similarity between S\u2019 and S makes perfect sense to me.\n- The effectiveness of the counterfactual part needs to be demonstrated by setting \u03bb=1or deleting the counterfactual term L_opp in eq(8).",
    "review_points_list": [
      "Though some concepts like LDB and counterfactual have been used in other domains, to my knowledge, the framework proposed by this paper is original in the field of GNN explanation.",
      "However, the definition of counterfactual explanation seems broken.",
      "This paper is well-written and the authors make their idea and the workflow clear.",
      "However, the reviewer thinks more experiments are needed to demonstrate the effectiveness of the proposed method.",
      "In addition, the definition of noise in this paper is not very clear, without which robustness can\u2019t be clarified as well.",
      "the structure of this paper is well organized.",
      "It starts with the intuition and then get into the tech details.",
      "Each module(e.g. LDBs, counterfactual subgraph) is introduced following the pipeline of the proposed method, which makes the paper structure quite clear.",
      "The experiments are conducted on three small datasets and consider several relevant baselines for GNN explanation.",
      "The results are significantly better than the baseline approaches on criteria fidelity and AUC save for the AUC performance on Mutagenicity.",
      "According to the previous, the definition of counterfactual explanation is \u201cif X had been different, Y would not have occurred\u201d, rather than \u201cif X had not occurred, Y would not have occurred\u201d as claimed by the authors.",
      "That means, the authors only replace the subgraph (X) with the empty set, rather than the reference subgraph (that could be the empty or other subgraphs).",
      "Moreover, recent studies find that simply replacing the features of interest (X) with the empty could cause the distribution shift from the complete graphs.",
      "Please explain or clarify these points.",
      "I cannot find a clear definition of noise in this paper.",
      "I think it\u2019s necessary to clarify it when you are discussing robustness.",
      "For example, what does 30% noise mean?",
      "Adding random noise whose magnitude is 30% of the original node feature or randomly adding or deleting 30% edges or do them both?",
      "It will be much better if the authors can explain more why the robustness defined in this paper is important.",
      "In CV domain, after adding noise to the panda picture, the NN cannot classify it correctly anymore while human being can\u2019t see any difference between the original pic and the noisy one.",
      "However, taking mutagenicity for example, adding or deleting 30% of edges will make the original molecule a totally different one even though the GNN doesn\u2019t change the prediction(noxious or not).",
      "In these case, it\u2019s reasonable for an explainer to identify a different subgraph.",
      "So, I don\u2019t think evaluating the robustness based on the similarity between S\u2019 and S makes perfect sense to me.",
      "The effectiveness of the counterfactual part needs to be demonstrated by setting \u03bb=1or deleting the counterfactual term L_opp in eq(8)."
    ]
  },
  {
    "paper_id": "2107.04086v3",
    "submission_id": "wGmOLwb8ClT",
    "submission_title": "Robust Counterfactual Explanations on Graph Neural Networks",
    "review_id": "avis1Em2VYO",
    "input": {
      "title": "Robust Counterfactual Explanations on Graph Neural Networks",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper, in my opinion, has some important and strong aspects that make it interesting for the community.\n- The method is general and can be used for the three tasks usually studied in graph learning.\n- Once the boundaries are found, the method consists of moving a sample from one boundary to another to change the prediction outcome. This makes the method efficient at explanation time.\n- The experiments are conducted on datasets that are standard for this task.\n- In general, one seeks to make a few modifications as possible to the input sample. This aspect is totally absent in this research and, in my opinion, is of fundamental importance in this area.\n- One should seek the smallest modification possible and then measure fidelity, sparsity, and size of the recommendation.\n- You want to extract a decision region with 2 properties: Should be induced by a subset of LDBs in H Should cover many instances in the training set, and all covered graphs should be predicted as the same classification\n- It is not really clear how LDBs are related to GNNs (not really considering that you base your formulation only on GCNs) as it could be used, in principle, in all deep learning mechanisms, right?\n- The authors should comment on how the method described in 4.2 is related to GNN.\n- The end of 4.1 and 4.2 is a bit hard to follow, but we see here that it involves sampling from H, which does not seem very robust because then the partitioning of O^d is dependent on which LDBs were sampled from H.\n- The authors even say that r(P,c) induced by LDBs in the tilde{H} might have instances that aren\u2019t predicted as the correct class.\n- Do the explanations change depending on the sampling of H? If so, this is not robust at all.\n- Existing methods are not robust to noise bc they optimize for individual inputs. They cite PGExplainer here as one of the methods.\n- Unless I did not understand correctly, it does not optimize for individual inputs and instead learns a general explanation model applied to individual inputs.\n- The comparison to CF-GNNExplainer is not fair, in my opinion.\n- The method they propose is similar to CF-GNNExplainer in the way they build the CF explanation, but it has not been properly acknowledged in the paper.\n- Both methods learn a sparsifier that is then thresholded to find the edges to be removed.\n- CF-GNNExplorer is not an adversarial attack method.\n- The baselines mentioned in the experiments are not CF explanations at all. This makes the experiment not convincing at all.\n- The methods are for explaining node classifiers and not graph classification models.\n- I would appreciate some thoughts on this point.\n- There is no experimental support for this claim.\n- There is no human experiment, so they cannot claim that their method aligns well with human intuition.\n- What is \u201cexplainability\u201d? How do you quantify \u201cexplainability\u201d? What is \u201cstable\u201d?\n- There is no support for this claim at all anywhere.\n- Have you tried to reuse the network without retraining? Does it work as claimed in the paper?\n- The claim that the method is robust to noise is weakly supported by the experiments, in my opinion.\n- The error bars are separate, so it is not convincing.\n- The robustness feature is poorly motivated in the paper. Why is it essential to be robust? If you don\u2019t mention proper motivations, the problem definition feels rather artificial.\n- The requirements require \u2192 Can you change this? It sounds a little bit odd.\n- The sentence at lines 143-144 is broken; I cannot parse it.\n- Does FC actually mean FC + nonlinearity?\n- Please define the positive and negative side at Line 273.\n- What is the operator in Eq. (9)?\n- In references [1], [15], and [46] the year is missing",
    "review_points_list": [
      "The paper, in my opinion, has some important and strong aspects that make it interesting for the community.",
      "The method is general and can be used for the three tasks usually studied in graph learning.",
      "Once the boundaries are found, the method consists of moving a sample from one boundary to another to change the prediction outcome. This makes the method efficient at explanation time.",
      "The experiments are conducted on datasets that are standard for this task.",
      "In general, one seeks to make a few modifications as possible to the input sample. This aspect is totally absent in this research and, in my opinion, is of fundamental importance in this area.",
      "One should seek the smallest modification possible and then measure fidelity, sparsity, and size of the recommendation.",
      "You want to extract a decision region with 2 properties: Should be induced by a subset of LDBs in H Should cover many instances in the training set, and all covered graphs should be predicted as the same classification",
      "It is not really clear how LDBs are related to GNNs (not really considering that you base your formulation only on GCNs) as it could be used, in principle, in all deep learning mechanisms, right?",
      "The authors should comment on how the method described in 4.2 is related to GNN.",
      "The end of 4.1 and 4.2 is a bit hard to follow, but we see here that it involves sampling from H, which does not seem very robust because then the partitioning of O^d is dependent on which LDBs were sampled from H.",
      "The authors even say that r(P,c) induced by LDBs in the tilde{H} might have instances that aren\u2019t predicted as the correct class.",
      "Do the explanations change depending on the sampling of H? If so, this is not robust at all.",
      "Existing methods are not robust to noise bc they optimize for individual inputs. They cite PGExplainer here as one of the methods.",
      "Unless I did not understand correctly, it does not optimize for individual inputs and instead learns a general explanation model applied to individual inputs.",
      "The comparison to CF-GNNExplainer is not fair, in my opinion.",
      "The method they propose is similar to CF-GNNExplainer in the way they build the CF explanation, but it has not been properly acknowledged in the paper.",
      "Both methods learn a sparsifier that is then thresholded to find the edges to be removed.",
      "CF-GNNExplorer is not an adversarial attack method.",
      "The baselines mentioned in the experiments are not CF explanations at all. This makes the experiment not convincing at all.",
      "The methods are for explaining node classifiers and not graph classification models.",
      "I would appreciate some thoughts on this point.",
      "There is no experimental support for this claim.",
      "There is no human experiment, so they cannot claim that their method aligns well with human intuition.",
      "What is \u201cexplainability\u201d? How do you quantify \u201cexplainability\u201d? What is \u201cstable\u201d?",
      "There is no support for this claim at all anywhere.",
      "Have you tried to reuse the network without retraining? Does it work as claimed in the paper?",
      "The claim that the method is robust to noise is weakly supported by the experiments, in my opinion.",
      "The error bars are separate, so it is not convincing.",
      "The robustness feature is poorly motivated in the paper. Why is it essential to be robust? If you don\u2019t mention proper motivations, the problem definition feels rather artificial.",
      "The requirements require \u2192 Can you change this? It sounds a little bit odd.",
      "The sentence at lines 143-144 is broken; I cannot parse it.",
      "Does FC actually mean FC + nonlinearity?",
      "Please define the positive and negative side at Line 273.",
      "What is the operator in Eq. (9)?",
      "In references [1], [15], and [46] the year is missing"
    ]
  },
  {
    "paper_id": "2107.04086v3",
    "submission_id": "wGmOLwb8ClT",
    "submission_title": "Robust Counterfactual Explanations on Graph Neural Networks",
    "review_id": "PEofb80aTe9",
    "input": {
      "title": "Robust Counterfactual Explanations on Graph Neural Networks",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- the topic of robust counterfactual explanations in GNN is certainly of high interest for the community and it has been, yet, poorly explored.\n- As such the work has very high potential impact.\n- The proposed approach tackles specifically the issue of providing subgraph explanation with robustness.\n- The proposed method is neat and the paper is easy to follow.\n- However, I have a few concerns.\n- 1, the paper mentioned that the explanation align well with human intuition by removing the set of edges. It is less convincing to claim the human intuition here without user studies.\n- 2, the extracting decision optimization algorithm has been leveraged by SCSC. I'm curious how to make the overall efficiency much better? More complexity analysis would be more trustworthy.\n- 3, In order to better evaluation the explainability, how to quantify the significance of G_{E/S} changes? In [1], it use 'faithfulness' to justify the explainability. I hope similar notion could be adopted as explainability evaluation.\n- [1] ERASER : A Benchmark to Evaluate Rationalized NLP Models",
    "review_points_list": [
      "the topic of robust counterfactual explanations in GNN is certainly of high interest for the community and it has been, yet, poorly explored.",
      "As such the work has very high potential impact.",
      "The proposed approach tackles specifically the issue of providing subgraph explanation with robustness.",
      "The proposed method is neat and the paper is easy to follow.",
      "However, I have a few concerns.",
      "1, the paper mentioned that the explanation align well with human intuition by removing the set of edges. It is less convincing to claim the human intuition here without user studies.",
      "2, the extracting decision optimization algorithm has been leveraged by SCSC. I'm curious how to make the overall efficiency much better? More complexity analysis would be more trustworthy.",
      "3, In order to better evaluation the explainability, how to quantify the significance of G_{E/S} changes? In [1], it use 'faithfulness' to justify the explainability. I hope similar notion could be adopted as explainability evaluation.",
      "[1] ERASER : A Benchmark to Evaluate Rationalized NLP Models"
    ]
  },
  {
    "paper_id": "2111.02666v3",
    "submission_id": "wtLW-Amuds",
    "submission_title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
    "review_id": "qRcecezV-yA",
    "input": {
      "title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This work provides an interesting and highly valuable method for building RL models robust to various types of noise and capable of out-of-distribution generalization, which is a highly important and widely pursued goal in RL.\n- To attain this goal, the authors propose a novel framework based on Set Transformers (a Transformer-based method for permutation-invariant neural-network analysis of datasets) and also based on concepts of cellular automata.\n- The authors rigorously examine the properties of their new framework using a diverse set of standard RL tasks.\n- The authors show impressive capabilities of their model to perform robustly in the presence of large-scale occlusions, noise, and out-of-distribution versions of the tasks.\n- The text could use numerous clarifications and a comprehensive methods section, which would make the paper accessible for the broader NeurIPS community.\n- Although the paper seems to have all necessary details to be understood, these details appear to be scattered throughout the text (and throughout the referenced literature) which may require reading the paper 2-3 times to cover all the important details.\n- To this end, I suggest making use of unlimited supplementary space to write comprehensive methods, which \u2013 alone \u2013 would be sufficient to reproduce the model and the experiments.\n- The paper may also benefit from adding the details of the framework to the main text.\n- Notably, the Set Transformers used here are much less known than the original Transformer framework, so it may be unfair to expect the reader to know its details or to be motivated to read a separate paper on it.\n- As a good example, the Set Transformers paper itself starts with a brief yet sufficient introduction of the Transformer framework and explains the design choices for the Set Transformer framework immediately upon their appearance.\n- One of such design choices \u2013 not explained in the current paper \u2013 is that SensoryNeuron, just like Set Transformers, attends the input data (Keys, Values) to a (smaller) set of separately learned/handcrafted Query embeddings \u2013 unlike the usual Transformers attending the elements of input data to each other.\n- It is important to explain that learnable/predefined Query embeddings do not depend on the input and decrease the algorithm\u2019s complexity from quadratic to linear.\n- The mathematical notation here can also be simplified, as it appears to have multiple variables which are not used.\n- For example, the matrices Wq, Wk, and Wv were originally introduced in Transformer to be used in multi-head attention, where the different versions of these matrices were applied to the same Queries, Keys, and Values.\n- It appears that the framework presented in this submission does not use multi-head attention, so these matrices can be sacrificed for the clarity of how the main model is explained.\n- The matrix Wk, in particular, can be described together with the function Fk as both of them are implemented using neural networks \u2013 stacked with each other and thus forming a single neural network.\n- The same goes for Fv and Wv \u2013 both being implemented as the identity matrices and thus unnecessary.\n- Decomposition of Q and Wq is justified as it decreases the overall number of parameters and because Q is not learnable \u2013 but this distinction does not need to be immediately introduced.\n- Focusing rather on the roles of Q, K, and V would make the paper more accessible for the broad audience.\n- Finally, I found the neuroscience-oriented introduction to this paper questionable.\n- While the paper presents interesting, significant, and valuable results regarding the robustness of the algorithm to various types of noise, taking advantage of additional data, and out-of-distribution generalization \u2013 these results are summarized only at the end of the paper.\n- At the same time, a reader who starts reading the text from the beginning may falsely think that the purpose of the paper is to learn from/replicate a biological phenomenon.\n- Moreover, the introduction \u2013 in light of the details of the proposed model \u2013 may leave a reader with the impression that the brain information processing is permutation invariant, like in zero-shot learning, whereas it\u2019s not.\n- The neuroscience papers cited in the introduction, such as reverse biking or learning to \u201csee\u201d using an electrode matrix, clearly mention the significant time tolls of acquiring such skills.\n- Although the presented work may have future neuroscience implications, the line of the argument currently presented here does not make a solid connection to neuroscience and, as such, needs to be revised/shrunk.\n- Instead, the paper can start from the useful properties of the model, as outlined in the conclusions.\n- The same goes for the abstract.\n- I would like to highlight here that all my suggestions above do not devalue the significance and the validity of the results.\n- They are intended to highlight that a thorough explanation of the design choices, a broader overview of the Transformer, and starting from the robustness/generalization properties will make the paper a more accessible read for the broad NeurIPS audience.\n- Even in the current format, this work is publishable in NeurIPS.\n- Minor comments:\n- -Why is the pass-through function Fv needed at all? Could you please briefly comment on the intuition why Fv, normally being a learnable parameter in Transformers, does not require learning here?\n- -Could you please also compute the statistical significance for the RL results? Given the huge standard deviation in the CartPole task, the SensoryNeuron\u2019s apparent slight deficiency may not be statistically significant, which will strengthen your claim.\n- -Can you please explain why you spatially arrange the messages Mt in the AtariPong task? Naively, it would seem that, as you permute your sensory neurons, the 2D arrangement of their messages will be also random and the convolution on top of these messages will learn the same as the feedforward connectivity?",
    "review_points_list": [
      "This work provides an interesting and highly valuable method for building RL models robust to various types of noise and capable of out-of-distribution generalization, which is a highly important and widely pursued goal in RL.",
      "To attain this goal, the authors propose a novel framework based on Set Transformers (a Transformer-based method for permutation-invariant neural-network analysis of datasets) and also based on concepts of cellular automata.",
      "The authors rigorously examine the properties of their new framework using a diverse set of standard RL tasks.",
      "The authors show impressive capabilities of their model to perform robustly in the presence of large-scale occlusions, noise, and out-of-distribution versions of the tasks.",
      "The text could use numerous clarifications and a comprehensive methods section, which would make the paper accessible for the broader NeurIPS community.",
      "Although the paper seems to have all necessary details to be understood, these details appear to be scattered throughout the text (and throughout the referenced literature) which may require reading the paper 2-3 times to cover all the important details.",
      "To this end, I suggest making use of unlimited supplementary space to write comprehensive methods, which \u2013 alone \u2013 would be sufficient to reproduce the model and the experiments.",
      "The paper may also benefit from adding the details of the framework to the main text.",
      "Notably, the Set Transformers used here are much less known than the original Transformer framework, so it may be unfair to expect the reader to know its details or to be motivated to read a separate paper on it.",
      "As a good example, the Set Transformers paper itself starts with a brief yet sufficient introduction of the Transformer framework and explains the design choices for the Set Transformer framework immediately upon their appearance.",
      "One of such design choices \u2013 not explained in the current paper \u2013 is that SensoryNeuron, just like Set Transformers, attends the input data (Keys, Values) to a (smaller) set of separately learned/handcrafted Query embeddings \u2013 unlike the usual Transformers attending the elements of input data to each other.",
      "It is important to explain that learnable/predefined Query embeddings do not depend on the input and decrease the algorithm\u2019s complexity from quadratic to linear.",
      "The mathematical notation here can also be simplified, as it appears to have multiple variables which are not used.",
      "For example, the matrices Wq, Wk, and Wv were originally introduced in Transformer to be used in multi-head attention, where the different versions of these matrices were applied to the same Queries, Keys, and Values.",
      "It appears that the framework presented in this submission does not use multi-head attention, so these matrices can be sacrificed for the clarity of how the main model is explained.",
      "The matrix Wk, in particular, can be described together with the function Fk as both of them are implemented using neural networks \u2013 stacked with each other and thus forming a single neural network.",
      "The same goes for Fv and Wv \u2013 both being implemented as the identity matrices and thus unnecessary.",
      "Decomposition of Q and Wq is justified as it decreases the overall number of parameters and because Q is not learnable \u2013 but this distinction does not need to be immediately introduced.",
      "Focusing rather on the roles of Q, K, and V would make the paper more accessible for the broad audience.",
      "Finally, I found the neuroscience-oriented introduction to this paper questionable.",
      "While the paper presents interesting, significant, and valuable results regarding the robustness of the algorithm to various types of noise, taking advantage of additional data, and out-of-distribution generalization \u2013 these results are summarized only at the end of the paper.",
      "At the same time, a reader who starts reading the text from the beginning may falsely think that the purpose of the paper is to learn from/replicate a biological phenomenon.",
      "Moreover, the introduction \u2013 in light of the details of the proposed model \u2013 may leave a reader with the impression that the brain information processing is permutation invariant, like in zero-shot learning, whereas it\u2019s not.",
      "The neuroscience papers cited in the introduction, such as reverse biking or learning to \u201csee\u201d using an electrode matrix, clearly mention the significant time tolls of acquiring such skills.",
      "Although the presented work may have future neuroscience implications, the line of the argument currently presented here does not make a solid connection to neuroscience and, as such, needs to be revised/shrunk.",
      "Instead, the paper can start from the useful properties of the model, as outlined in the conclusions.",
      "The same goes for the abstract.",
      "I would like to highlight here that all my suggestions above do not devalue the significance and the validity of the results.",
      "They are intended to highlight that a thorough explanation of the design choices, a broader overview of the Transformer, and starting from the robustness/generalization properties will make the paper a more accessible read for the broad NeurIPS audience.",
      "Even in the current format, this work is publishable in NeurIPS.",
      "Minor comments:",
      "-Why is the pass-through function Fv needed at all? Could you please briefly comment on the intuition why Fv, normally being a learnable parameter in Transformers, does not require learning here?",
      "-Could you please also compute the statistical significance for the RL results? Given the huge standard deviation in the CartPole task, the SensoryNeuron\u2019s apparent slight deficiency may not be statistically significant, which will strengthen your claim.",
      "-Can you please explain why you spatially arrange the messages Mt in the AtariPong task? Naively, it would seem that, as you permute your sensory neurons, the 2D arrangement of their messages will be also random and the convolution on top of these messages will learn the same as the feedforward connectivity?"
    ]
  },
  {
    "paper_id": "2111.02666v3",
    "submission_id": "wtLW-Amuds",
    "submission_title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
    "review_id": "rX6mmPiA1G3",
    "input": {
      "title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper presents a novel combination and application of well-known techniques and concepts.\n- The Transformer's attention mechanism is known to be permutation invariant and applied here to reinforcement learning.\n- The AttentionNeuron architecture is an angle of collective intelligence, where each of N sensory neurons sees one of each of N observation components.\n- The transformation from input to sensory neuron layer is a generalized permutation matrix, as long as the magnitude of each column remains the same.\n- The connection to collective intelligence seems limited to a high-level abstraction.\n- The method seems complex at first and to some degree needlessly complex.\n- The discussion of design choices such as temporal memory and reduced network size is helpful.\n- The solution is actually quite thoughtful, and the issues are more of presentation than content\n- The experiments are impressive - the approach is used successfully in four different testbeds, some with multiple environments.\n- Though there are some caveats with each one, particularly Atari Pong, the core message of permutation invariance seems to persist through the results.\n- The core message is really strengthened by forgoing complicated cuts of data and various statistics to just show performance on a large range of testbeds.\n- The only improvement may be ablations or analysis to rule out confounds.\n- The related work is quite ambitious and confusing.\n- Many of the related work sections are only related loosely, and the authors say as much at the end of each paragraph.\n- The paper presents an interesting layer architecture that achieves a clear and concrete goal.\n- The related work section is confusing and to some degree feels overpromising.\n- The notation is difficult to follow, partially because it is mostly in paragraph form.\n- Figure 2 is quite helpful.\n- The 1:1 nature of the observation components to sensory neurons could be visually clearer, but the rest of the text on page 4 is dense with words and notation combined.\n- The results are very hefty and presented as one long laundry list.\n- It would be nice to see unifying stats or even unifying qualitative analysis earlier than Section 5.\n- The method can potentially be applied to many more and more immediate applications\n- If the language focused on what this paper is really doing, as opposed to big ideas that sound kind of like what it is doing, it would come across better.\n- The paper is significant because of the applications listed in section 5",
    "review_points_list": [
      "The paper presents a novel combination and application of well-known techniques and concepts.",
      "The Transformer's attention mechanism is known to be permutation invariant and applied here to reinforcement learning.",
      "The AttentionNeuron architecture is an angle of collective intelligence, where each of N sensory neurons sees one of each of N observation components.",
      "The transformation from input to sensory neuron layer is a generalized permutation matrix, as long as the magnitude of each column remains the same.",
      "The connection to collective intelligence seems limited to a high-level abstraction.",
      "The method seems complex at first and to some degree needlessly complex.",
      "The discussion of design choices such as temporal memory and reduced network size is helpful.",
      "The solution is actually quite thoughtful, and the issues are more of presentation than content",
      "The experiments are impressive - the approach is used successfully in four different testbeds, some with multiple environments.",
      "Though there are some caveats with each one, particularly Atari Pong, the core message of permutation invariance seems to persist through the results.",
      "The core message is really strengthened by forgoing complicated cuts of data and various statistics to just show performance on a large range of testbeds.",
      "The only improvement may be ablations or analysis to rule out confounds.",
      "The related work is quite ambitious and confusing.",
      "Many of the related work sections are only related loosely, and the authors say as much at the end of each paragraph.",
      "The paper presents an interesting layer architecture that achieves a clear and concrete goal.",
      "The related work section is confusing and to some degree feels overpromising.",
      "The notation is difficult to follow, partially because it is mostly in paragraph form.",
      "Figure 2 is quite helpful.",
      "The 1:1 nature of the observation components to sensory neurons could be visually clearer, but the rest of the text on page 4 is dense with words and notation combined.",
      "The results are very hefty and presented as one long laundry list.",
      "It would be nice to see unifying stats or even unifying qualitative analysis earlier than Section 5.",
      "The method can potentially be applied to many more and more immediate applications",
      "If the language focused on what this paper is really doing, as opposed to big ideas that sound kind of like what it is doing, it would come across better.",
      "The paper is significant because of the applications listed in section 5"
    ]
  },
  {
    "paper_id": "2111.02666v3",
    "submission_id": "wtLW-Amuds",
    "submission_title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
    "review_id": "j82ZbnJy39",
    "input": {
      "title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Studies novel and biologically inspired permutation invariance question in the context of RL.\n- Experiments evaluate on several diverse environments (CartPole, Ant, Pong, CarRacing).\n- Writing is generally clear.\n- It is not clear to me that permutation invariance is a practically significant or useful property.\n- Minimal ablations/comparisons to other methods in the experiments section.\n- The direction toward permutation-invariant representations for sensory observations is underexplored and novel in the RL setting.\n- The authors investigate a few different interesting properties of the method.\n- However, the baselines are weak.\n- I think this work is promising: it investigated a little-studied problem, permutation invariance to sensory observations in RL, motivated by observations in neuroscience regarding malleability in human brains.\n- The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods.\n- The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm.\n- At a high level, I think this work is promising: it investigated a little-studied problem, permutation invariance to sensory observations in RL, motivated by observations in neuroscience regarding malleability in human brains.\n- While it is not clear to me why this is a useful property from an engineering perspective (i.e. as to why this can't be mostly tackled by the sensor interface), I think the direction studying biologically plausible RL/models is exciting.\n- Furthermore, I think additional practical benefits gained as a result of permutation invariance (such as better generalization) are particularly exciting/important.\n- My main concerns are about the lack of comparisons/ablations to other methods, as the authors propose a method with several components but only compare to a feedforward network.\n- I think a particularly exciting experiment is Section 4.4/Table 4 because it promises that generalization could be improved via permutation invariance.\n- However, the experiment only compares to a minimal baseline.\n- I think comparing to other representation learning methods, such as bisimulation or contrastive learning, would make this experiment more exciting.\n- I think the authors propose a method with several components but only compare to a feedforward network.\n- I don't necessarily believe that my choices below would outperform the proposed method, but I think they would strengthen the paper and better contextualize the presented numbers.\n- To propose a non-exhaustive but simple set of choices:\n- Transformer/vision transformer: a transformer operating directly on the sensory inputs/image patches without positional embeddings is permutation invariant in the same sense as AttentionNeuron, but is a simpler approach.\n- LSTM after image processing: similarly to my above comment, this would clarify if this recurrent reorganization should be done at a low-level or a high-level.\n- Other generalization methods: as I mentioned above, I think a particularly exciting experiment is Section 4.4/Table 4 because it promises that generalization could be improved via permutation invariance.\n- However, the experiment only compares to a minimal baseline.\n- I think comparing to other representation learning methods, such as bisimulation or contrastive learning, would make this experiment more exciting.\n- I would like to see comparisons to these methods and the authors' additional responses to my concerns.\n- If the above issues are addressed, particularly if the comparisons to the transformer and high-level LSTM are included, I think this work could be considered for publication.",
    "review_points_list": [
      "Studies novel and biologically inspired permutation invariance question in the context of RL.",
      "Experiments evaluate on several diverse environments (CartPole, Ant, Pong, CarRacing).",
      "Writing is generally clear.",
      "It is not clear to me that permutation invariance is a practically significant or useful property.",
      "Minimal ablations/comparisons to other methods in the experiments section.",
      "The direction toward permutation-invariant representations for sensory observations is underexplored and novel in the RL setting.",
      "The authors investigate a few different interesting properties of the method.",
      "However, the baselines are weak.",
      "I think this work is promising: it investigated a little-studied problem, permutation invariance to sensory observations in RL, motivated by observations in neuroscience regarding malleability in human brains.",
      "The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods.",
      "The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm.",
      "At a high level, I think this work is promising: it investigated a little-studied problem, permutation invariance to sensory observations in RL, motivated by observations in neuroscience regarding malleability in human brains.",
      "While it is not clear to me why this is a useful property from an engineering perspective (i.e. as to why this can't be mostly tackled by the sensor interface), I think the direction studying biologically plausible RL/models is exciting.",
      "Furthermore, I think additional practical benefits gained as a result of permutation invariance (such as better generalization) are particularly exciting/important.",
      "My main concerns are about the lack of comparisons/ablations to other methods, as the authors propose a method with several components but only compare to a feedforward network.",
      "I think a particularly exciting experiment is Section 4.4/Table 4 because it promises that generalization could be improved via permutation invariance.",
      "However, the experiment only compares to a minimal baseline.",
      "I think comparing to other representation learning methods, such as bisimulation or contrastive learning, would make this experiment more exciting.",
      "I think the authors propose a method with several components but only compare to a feedforward network.",
      "I don't necessarily believe that my choices below would outperform the proposed method, but I think they would strengthen the paper and better contextualize the presented numbers.",
      "To propose a non-exhaustive but simple set of choices:",
      "Transformer/vision transformer: a transformer operating directly on the sensory inputs/image patches without positional embeddings is permutation invariant in the same sense as AttentionNeuron, but is a simpler approach.",
      "LSTM after image processing: similarly to my above comment, this would clarify if this recurrent reorganization should be done at a low-level or a high-level.",
      "Other generalization methods: as I mentioned above, I think a particularly exciting experiment is Section 4.4/Table 4 because it promises that generalization could be improved via permutation invariance.",
      "However, the experiment only compares to a minimal baseline.",
      "I think comparing to other representation learning methods, such as bisimulation or contrastive learning, would make this experiment more exciting.",
      "I would like to see comparisons to these methods and the authors' additional responses to my concerns.",
      "If the above issues are addressed, particularly if the comparisons to the transformer and high-level LSTM are included, I think this work could be considered for publication."
    ]
  },
  {
    "paper_id": "2111.02666v3",
    "submission_id": "wtLW-Amuds",
    "submission_title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
    "review_id": "cuElRNecla",
    "input": {
      "title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The present result is an extension of the set transformer (ICML 2019).\n- Although the title includes \"reinforcement learning\", actual learning takes evolutionary optimization and supervised learning (behavioral cloning).\n- I wish the role of the previous action input to the attention network is better discussed.\n- My guess is that the network compares the changes in the observations and action taken to infers which channel carries the critical information for control.\n- It would be interesting to analyze which channels actually gathered attention (e.g., the patches including the car and road boundary?) and how that is affected if the past action is not presented.\n- The math is clearly presented.\n- In equation (1), most RL folks imagine Q as action values and V as state value, so their meanings as Query and Value may be better explained earlier.\n- Although the architecture is reported to be inspired from sensory neurons, it has only remote link to neuroscience.\n- Although not tested in this paper, permutation-invariant feature would be helpful in the context of multi-agent learning, where an agent need to work with a variable number of agents coming and going in random order.",
    "review_points_list": [
      "The present result is an extension of the set transformer (ICML 2019).",
      "Although the title includes \"reinforcement learning\", actual learning takes evolutionary optimization and supervised learning (behavioral cloning).",
      "I wish the role of the previous action input to the attention network is better discussed.",
      "My guess is that the network compares the changes in the observations and action taken to infers which channel carries the critical information for control.",
      "It would be interesting to analyze which channels actually gathered attention (e.g., the patches including the car and road boundary?) and how that is affected if the past action is not presented.",
      "The math is clearly presented.",
      "In equation (1), most RL folks imagine Q as action values and V as state value, so their meanings as Query and Value may be better explained earlier.",
      "Although the architecture is reported to be inspired from sensory neurons, it has only remote link to neuroscience.",
      "Although not tested in this paper, permutation-invariant feature would be helpful in the context of multi-agent learning, where an agent need to work with a variable number of agents coming and going in random order."
    ]
  },
  {
    "paper_id": "2110.14096v1",
    "submission_id": "ySFGlFjgIfN",
    "submission_title": "Towards Robust Bisimulation Metric Learning",
    "review_id": "mzhYCQxRL-C",
    "input": {
      "title": "Towards Robust Bisimulation Metric Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper is well written and answers many of the questions that arise while reading it immediately.\n- The analysis of certain issues in DBC is very nice to have.\n- The theoretical section was (mostly) easy to follow and was informative.\n- The theory section has a clear added value in analysing the issues with DBC and proposing improvements.\n- For example, compact spaces are assumed for continuous MDPs, and this assumption is violated in DBC. This motivates the authors to propose normalizing the representations to a closed ball.\n- Additionally, there is a degenerate solution when rewards are sparse, which motivates the authors to propose intrinsic rewards to prevent this from occurring.\n- An ablation study is included (though it is incomplete).\n- The paper does a good job of reviewing the related work, only missing a few related references (included below).\n- The choice of inverse dynamics as a regularizer is not well-motivated.\n- Why choose an inverse dynamics model and not a different self-supervised loss?\n- Why not a different combination of approaches?\n- The motivation given for inverse dynamics could also be applied to reconstructions or contrastive coding.\n- Additionally, there are no experimental results showing that an inverse dynamics model is best; as such this choice is the weakest part of the paper.\n- As there is no experimental comparison to other additions to DBC (e.g. different self-supervised learning approaches), it is unclear why inverse dynamics is the preferred method to solving the issues with DBC.\n- The paper focuses heavily on one method; DBC, and as such is somewhat limited in scope.\n- The paper, while well-written, is not fully self-contained - certain terms are used without being defined.\n- What is a \u2018compact subset of $\text{S}$\u2019?\n- What is a diameter of a state space?\n- What is sup?\n- The supremum?\n- What is supp?\n- Also the supremum?\n- What is $\text{H}$ in line 259?\n- What is a low dispersion reward?\n- According to the paper, DBC suffers from common metric learning problems, but there are no references included to other metric learning papers that exhibit similar issues.\n- If the intrinsic reward is based on the forward model error, it should not be useful under state collapse, as the error would be 0.\n- The non-stationarity introduced by IR might be an issue in more complex environments.\n- Line 294: Claim: \u201cHence, we expect less disruption to the BSM encoding than other auxiliary tasks (e.g. reconstruction)\u201d\n- reconstruction is not the only possible auxiliary task.\n- What happens when using a better auxiliary loss, such as action-dependent contrastive coding?\n- Ablation study is incomplete -- DBC-ID and DBC-normed-ID are not included.\n- \u201c(Left) Our method significantly improves upon DBC [36] and is on par with SAC\u201d this looks like it isn\u2019t true, as SAC achieves a higher performance in the end.\n- The method isn\u2019t evaluated on the same benchmarks as DBC, with comparable levels of distraction (e.g. natural video). As a result, it is unclear what the performance improvement would be in those settings.\n- \u201ctwo states that only differ in rewards far in the future should still not be bisimilar\u201d Standard bisimilarity is defined over immediate, not future rewards. Could you elaborate?\n- The assumption that the reward is normalized between 0 and 1 -- is this taken into account in the experiments?\n- The sentences 193-195 are unclear, could you clarify?\n- Eq. 11: x \\in $R^d$ is this d overloaded?\n- Eq. 11: what is $B_c$? The ball with radius c?\n- It looks like c is not defined.\n- Eq. 12: Does $M(B_c)$ mean the space of probability distributions over the ball with radius c?\n- Figure 1: what is magnitude? The magnitude of the embedding?\n- Below are some suggested references on state representation learning that could be relevant to include in related work.\n- Plots are hard to read in black and white.\n- While the motivation for requiring more informative rewards is clear (as mentioned in strengths), it is not motivated why this takes the form of intrinsic rewards, as compared to e.g. reward propagation.\n- SAC does not fail in sparse-cartpole with $N_m=1$, it just takes longer to converge.\n- The score is mostly based on the following:\n- The choice of inverse dynamics (vs action-dependent contrastive coding or another type of auxiliary loss) should be properly motivated, either theoretically or experimentally.\n- Missing references should be included.\n- Evaluation should be performed on the same benchmarks as DBC, e.g. with natural video distraction.\n- I would raise the score if these are addressed.\n- Biza et al, \u201cOnline Abstraction with MDP Homomorphisms for Deep Learning\u201d\n- Grimm et al, \u201cThe value equivalence principle for model-based reinforcement learning\u201d\n- Le Lan et al, \u201cMetrics and continuity in reinforcement learning\u201d\n- Ravindran & Barto, \u201cApproximate Homomorphisms: A framework for non-exact minimization in Markov Decision Processes\u201d\n- Taylor et al, \u201cBounding Performance Loss in Approximate MDP Homomorphisms\u201d\n- Van der Pol et al, \u201cPlannable Approximations to MDP homomorphisms: equivariance under actions\u201d\n- Agarwal et al, \u201cContrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning\u201d\n- Corneil et al, \u201cEfficient model-based deep reinforcement learning with variational state tabulation\u201d\n- Dadashi et al, Offline Reinforcement Learning with Pseudometric Learning\n- Francois-Lavet et al, \u201cCombined reinforcement learning via abstract representations\u201d\n- Watter et al, \u201cEmbed to control: A locally linear latent dynamics model for control from raw images\u201d\n- Thank you for the rebuttal, and for the update on the experiments.\n- I think adding the new experiments to the paper will make the paper more convincing.\n- When reviewing, I only had a few issues with this paper: the lack of motivation for the choice of inverse dynamics (as opposed to another auxiliary loss), the missing evaluations on benchmarks with natural video distractions (for a fair comparison to DBC), the somewhat limited scope (due to the focus on DBC), some missing references, and the paper not being fully self-contained.\n- In the author response, the authors motivate their choice of inverse dynamics, clarify that their results can be useful for bisimulation metric learning in general, promise to add the references and amend the paper where it is unclear, and in a second reply give an update on benchmarks with natural video distractions, with positive results.\n- The authors thus address my original concerns.\n- However, reviewer pEeB raised concerns about the theoretical analysis that are important to address.\n- I have therefore decided keep my original score.",
    "review_points_list": [
      "This paper is well written and answers many of the questions that arise while reading it immediately.",
      "The analysis of certain issues in DBC is very nice to have.",
      "The theoretical section was (mostly) easy to follow and was informative.",
      "The theory section has a clear added value in analysing the issues with DBC and proposing improvements.",
      "For example, compact spaces are assumed for continuous MDPs, and this assumption is violated in DBC. This motivates the authors to propose normalizing the representations to a closed ball.",
      "Additionally, there is a degenerate solution when rewards are sparse, which motivates the authors to propose intrinsic rewards to prevent this from occurring.",
      "An ablation study is included (though it is incomplete).",
      "The paper does a good job of reviewing the related work, only missing a few related references (included below).",
      "The choice of inverse dynamics as a regularizer is not well-motivated.",
      "Why choose an inverse dynamics model and not a different self-supervised loss?",
      "Why not a different combination of approaches?",
      "The motivation given for inverse dynamics could also be applied to reconstructions or contrastive coding.",
      "Additionally, there are no experimental results showing that an inverse dynamics model is best; as such this choice is the weakest part of the paper.",
      "As there is no experimental comparison to other additions to DBC (e.g. different self-supervised learning approaches), it is unclear why inverse dynamics is the preferred method to solving the issues with DBC.",
      "The paper focuses heavily on one method; DBC, and as such is somewhat limited in scope.",
      "The paper, while well-written, is not fully self-contained - certain terms are used without being defined.",
      "What is a \u2018compact subset of $\text{S}$\u2019?",
      "What is a diameter of a state space?",
      "What is sup?",
      "The supremum?",
      "What is supp?",
      "Also the supremum?",
      "What is $\text{H}$ in line 259?",
      "What is a low dispersion reward?",
      "According to the paper, DBC suffers from common metric learning problems, but there are no references included to other metric learning papers that exhibit similar issues.",
      "If the intrinsic reward is based on the forward model error, it should not be useful under state collapse, as the error would be 0.",
      "The non-stationarity introduced by IR might be an issue in more complex environments.",
      "Line 294: Claim: \u201cHence, we expect less disruption to the BSM encoding than other auxiliary tasks (e.g. reconstruction)\u201d",
      "reconstruction is not the only possible auxiliary task.",
      "What happens when using a better auxiliary loss, such as action-dependent contrastive coding?",
      "Ablation study is incomplete -- DBC-ID and DBC-normed-ID are not included.",
      "\u201c(Left) Our method significantly improves upon DBC [36] and is on par with SAC\u201d this looks like it isn\u2019t true, as SAC achieves a higher performance in the end.",
      "The method isn\u2019t evaluated on the same benchmarks as DBC, with comparable levels of distraction (e.g. natural video). As a result, it is unclear what the performance improvement would be in those settings.",
      "\u201ctwo states that only differ in rewards far in the future should still not be bisimilar\u201d Standard bisimilarity is defined over immediate, not future rewards. Could you elaborate?",
      "The assumption that the reward is normalized between 0 and 1 -- is this taken into account in the experiments?",
      "The sentences 193-195 are unclear, could you clarify?",
      "Eq. 11: x \\in $R^d$ is this d overloaded?",
      "Eq. 11: what is $B_c$? The ball with radius c?",
      "It looks like c is not defined.",
      "Eq. 12: Does $M(B_c)$ mean the space of probability distributions over the ball with radius c?",
      "Figure 1: what is magnitude? The magnitude of the embedding?",
      "Below are some suggested references on state representation learning that could be relevant to include in related work.",
      "Plots are hard to read in black and white.",
      "While the motivation for requiring more informative rewards is clear (as mentioned in strengths), it is not motivated why this takes the form of intrinsic rewards, as compared to e.g. reward propagation.",
      "SAC does not fail in sparse-cartpole with $N_m=1$, it just takes longer to converge.",
      "The score is mostly based on the following:",
      "The choice of inverse dynamics (vs action-dependent contrastive coding or another type of auxiliary loss) should be properly motivated, either theoretically or experimentally.",
      "Missing references should be included.",
      "Evaluation should be performed on the same benchmarks as DBC, e.g. with natural video distraction.",
      "I would raise the score if these are addressed.",
      "Biza et al, \u201cOnline Abstraction with MDP Homomorphisms for Deep Learning\u201d",
      "Grimm et al, \u201cThe value equivalence principle for model-based reinforcement learning\u201d",
      "Le Lan et al, \u201cMetrics and continuity in reinforcement learning\u201d",
      "Ravindran & Barto, \u201cApproximate Homomorphisms: A framework for non-exact minimization in Markov Decision Processes\u201d",
      "Taylor et al, \u201cBounding Performance Loss in Approximate MDP Homomorphisms\u201d",
      "Van der Pol et al, \u201cPlannable Approximations to MDP homomorphisms: equivariance under actions\u201d",
      "Agarwal et al, \u201cContrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning\u201d",
      "Corneil et al, \u201cEfficient model-based deep reinforcement learning with variational state tabulation\u201d",
      "Dadashi et al, Offline Reinforcement Learning with Pseudometric Learning",
      "Francois-Lavet et al, \u201cCombined reinforcement learning via abstract representations\u201d",
      "Watter et al, \u201cEmbed to control: A locally linear latent dynamics model for control from raw images\u201d",
      "Thank you for the rebuttal, and for the update on the experiments.",
      "I think adding the new experiments to the paper will make the paper more convincing.",
      "When reviewing, I only had a few issues with this paper: the lack of motivation for the choice of inverse dynamics (as opposed to another auxiliary loss), the missing evaluations on benchmarks with natural video distractions (for a fair comparison to DBC), the somewhat limited scope (due to the focus on DBC), some missing references, and the paper not being fully self-contained.",
      "In the author response, the authors motivate their choice of inverse dynamics, clarify that their results can be useful for bisimulation metric learning in general, promise to add the references and amend the paper where it is unclear, and in a second reply give an update on benchmarks with natural video distractions, with positive results.",
      "The authors thus address my original concerns.",
      "However, reviewer pEeB raised concerns about the theoretical analysis that are important to address.",
      "I have therefore decided keep my original score."
    ]
  },
  {
    "paper_id": "2110.14096v1",
    "submission_id": "ySFGlFjgIfN",
    "submission_title": "Towards Robust Bisimulation Metric Learning",
    "review_id": "SZKIGUy4SwH",
    "input": {
      "title": "Towards Robust Bisimulation Metric Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The authors modify the previous metrics and provide a generalized metric. The analysis of the new metric compared with the old one is solid.\n- In the experiments, the newly proposed method shows better robustness against sparsity compared with DBC.\n- The authors further show the policy-dependence may cause caution collapse in on-policy bisimulation.\n- The experiments have simple/similar dynamics. It would be of beneficial for readers to know whether the approach would still gain obvious edges when the dynamics of the task is complex, such as a sparse version of the spider or walker in gym.\n- Now the main challenge seems only coming from the dimension of the noise.",
    "review_points_list": [
      "The authors modify the previous metrics and provide a generalized metric. The analysis of the new metric compared with the old one is solid.",
      "In the experiments, the newly proposed method shows better robustness against sparsity compared with DBC.",
      "The authors further show the policy-dependence may cause caution collapse in on-policy bisimulation.",
      "The experiments have simple/similar dynamics. It would be of beneficial for readers to know whether the approach would still gain obvious edges when the dynamics of the task is complex, such as a sparse version of the spider or walker in gym.",
      "Now the main challenge seems only coming from the dimension of the noise."
    ]
  },
  {
    "paper_id": "2110.14096v1",
    "submission_id": "ySFGlFjgIfN",
    "submission_title": "Towards Robust Bisimulation Metric Learning",
    "review_id": "VXCRwpoCGT2",
    "input": {
      "title": "Towards Robust Bisimulation Metric Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Learning good latent representation is important for RL problems with high dimensional observations, and recent approaches based on bisimulation metrics have shown promising results.\n- This paper analyzes the existence of unique on-policy bisimulation metrics and their connection to value function bounds.\n- Based on the properties, the paper proposes a normalization method to fix some embedding issues of prior methods in sparse reward environments, and it further proposes improvements using intrinsic rewards and inverse dynamics.\n- The ideas to analyze on-policy bisumlation metrics is very reasonable, and the proposed methods seem to be both theoretical and practical appealing, but the paper (particular Section 3) is very difficult to read, and many key concepts are either missing or hard to follow.\n- One of the key contributions is the convergence condition of bisimulation metric in learning.\n- However, there is never a formal statement of what is the 'sufficient condition for the existence of a unique bisimulation metric'.\n- The sufficient conditions seem to refer to cT<1, p=1, and the support of the approximate dynamics is closed. But later in (11) the main condition seems to refer to the boundedness of the metric.\n- The boundedness of the unique metric is mentioned in Theorem 3, but as a property instead of a sufficient condition.\n- After checking the proof of Theorem 3, the boundedness is indeed not used in proving the existence of the metric.\n- So why does the boundedness of metric becomes the sufficient condition? Does the boundedness have any relationship to the closedness of the support?\n- Even more confusing, in all discussions after Section 3 the condition is referred to 'embedding normalization' without any definition nor explanation.\n- Does normalization mean the requirements (11)? How does the normalization actually work in learning with parameterized embedding functions?\n- There are some missing definitions and typos which highly affect readability.\n- There is no definition of the diameter before Lemma 1, no definition for the different forms of metrics used in Theorem 4 besides in the appendix.\n- In the paragraph before Theorem 3, one metric d with hat but another one without hat.\n- What is the metric d appearing in (11)?\n- There is no indication on what the sup in (14) over on which doesn't seem to be over policies.\n- The idea of investigating the danger of on-policy bisimulation with sparse rewards in 3.2.3 is very interesting, but the discussion is very unclear and may need further explanation.\n- The section claims that the challenges of degeneration is only an issue with on-policy bisimulation, but there is no comparison with policy-independent bisimulation.\n- If policy-independent bisimulation doesn't have the issue, then why prior methods don't work?\n- The paper also claims that this degeneration might be the reason for poor performance of DBC and the proposed methods fix the degeneration, but the simulation results actually show exploding behaviors instead of collapsing.\n- How does the analysis explains exploding behaviors of DBC?\n- The proposed intrinsic rewards and inverse dynamics methods seem reasonable, but there does not seem to have any connection to the analysis in 3.2.3 and why these methods can resolve the degeneration issue.\n- As said above, I like the ideas of the paper but would not recommend to accept it given the issues mentioned above.\n- I will consider raising the score if the paper can be revised to address the above points.",
    "review_points_list": [
      "Learning good latent representation is important for RL problems with high dimensional observations, and recent approaches based on bisimulation metrics have shown promising results.",
      "This paper analyzes the existence of unique on-policy bisimulation metrics and their connection to value function bounds.",
      "Based on the properties, the paper proposes a normalization method to fix some embedding issues of prior methods in sparse reward environments, and it further proposes improvements using intrinsic rewards and inverse dynamics.",
      "The ideas to analyze on-policy bisumlation metrics is very reasonable, and the proposed methods seem to be both theoretical and practical appealing, but the paper (particular Section 3) is very difficult to read, and many key concepts are either missing or hard to follow.",
      "One of the key contributions is the convergence condition of bisimulation metric in learning.",
      "However, there is never a formal statement of what is the 'sufficient condition for the existence of a unique bisimulation metric'.",
      "The sufficient conditions seem to refer to cT<1, p=1, and the support of the approximate dynamics is closed. But later in (11) the main condition seems to refer to the boundedness of the metric.",
      "The boundedness of the unique metric is mentioned in Theorem 3, but as a property instead of a sufficient condition.",
      "After checking the proof of Theorem 3, the boundedness is indeed not used in proving the existence of the metric.",
      "So why does the boundedness of metric becomes the sufficient condition? Does the boundedness have any relationship to the closedness of the support?",
      "Even more confusing, in all discussions after Section 3 the condition is referred to 'embedding normalization' without any definition nor explanation.",
      "Does normalization mean the requirements (11)? How does the normalization actually work in learning with parameterized embedding functions?",
      "There are some missing definitions and typos which highly affect readability.",
      "There is no definition of the diameter before Lemma 1, no definition for the different forms of metrics used in Theorem 4 besides in the appendix.",
      "In the paragraph before Theorem 3, one metric d with hat but another one without hat.",
      "What is the metric d appearing in (11)?",
      "There is no indication on what the sup in (14) over on which doesn't seem to be over policies.",
      "The idea of investigating the danger of on-policy bisimulation with sparse rewards in 3.2.3 is very interesting, but the discussion is very unclear and may need further explanation.",
      "The section claims that the challenges of degeneration is only an issue with on-policy bisimulation, but there is no comparison with policy-independent bisimulation.",
      "If policy-independent bisimulation doesn't have the issue, then why prior methods don't work?",
      "The paper also claims that this degeneration might be the reason for poor performance of DBC and the proposed methods fix the degeneration, but the simulation results actually show exploding behaviors instead of collapsing.",
      "How does the analysis explains exploding behaviors of DBC?",
      "The proposed intrinsic rewards and inverse dynamics methods seem reasonable, but there does not seem to have any connection to the analysis in 3.2.3 and why these methods can resolve the degeneration issue.",
      "As said above, I like the ideas of the paper but would not recommend to accept it given the issues mentioned above.",
      "I will consider raising the score if the paper can be revised to address the above points."
    ]
  },
  {
    "paper_id": "2110.14096v1",
    "submission_id": "ySFGlFjgIfN",
    "submission_title": "Towards Robust Bisimulation Metric Learning",
    "review_id": "V8UcgHcomxH",
    "input": {
      "title": "Towards Robust Bisimulation Metric Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper focuses on learning more robust and efficient representations for deep reinforcement learning algorithms.\n- The analysis and resulting method build on previous work of the Deep Bisimulation for Control (DBC) algorithm but improves for environments with sparse rewards in particular.\n- The authors analyzed theoretically embedding collapse and explosion could happen with sparse rewards and proposed remedies to fix them.\n- The authors showed that the resulting algorithm was not only better with sparse rewards but also more stable when the observations are noisy.\n- The topic is definitely closely related and significant to the community.\n- Although it builds on DBC, the analysis and proposed algorithm are novel to the best of my knowledge.\n- The paper is also well-written and easy to follow in general.\n- Can you talk more about embedding collapsing and exploration?\n- The experiments are conducted on sparse cartpole and continuous mountain cars, which are relatively low-dimension, maybe evaluate and compare the methods in MuJuCo tasks with sparse rewards?\n- Fig.3, why the proposed method is a bit worse or almost the same as SAC? Could you explain more? Especially for cartpole-swingup-sparse, why with IR it is still a bit worse than SAC?",
    "review_points_list": [
      "This paper focuses on learning more robust and efficient representations for deep reinforcement learning algorithms.",
      "The analysis and resulting method build on previous work of the Deep Bisimulation for Control (DBC) algorithm but improves for environments with sparse rewards in particular.",
      "The authors analyzed theoretically embedding collapse and explosion could happen with sparse rewards and proposed remedies to fix them.",
      "The authors showed that the resulting algorithm was not only better with sparse rewards but also more stable when the observations are noisy.",
      "The topic is definitely closely related and significant to the community.",
      "Although it builds on DBC, the analysis and proposed algorithm are novel to the best of my knowledge.",
      "The paper is also well-written and easy to follow in general.",
      "Can you talk more about embedding collapsing and exploration?",
      "The experiments are conducted on sparse cartpole and continuous mountain cars, which are relatively low-dimension, maybe evaluate and compare the methods in MuJuCo tasks with sparse rewards?",
      "Fig.3, why the proposed method is a bit worse or almost the same as SAC? Could you explain more? Especially for cartpole-swingup-sparse, why with IR it is still a bit worse than SAC?"
    ]
  },
  {
    "paper_id": "2103.12024v2",
    "submission_id": "yaxePRTOhqk",
    "submission_title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
    "review_id": "oBHl0nZcTc",
    "input": {
      "title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- To achieve the main result, a useful Lemma 3.1 is shown (this is a minor variation of a Lemma from BKZ): this result avoids the sampling error term $O(1/\\sqrt{n})$ by showing a moment bound of a 'shifted' generalization error.\n- It would be interesting if the authors would elaborate further about the intuition on this refinement, and what are its consequences for the excess risk bounds (the technical proofs are clear, but nevertheless it might be useful a brief discussion).\n- The proof of the main result (Thm 1.1) is obtained nontrivially from Lemma 3.1.\n- This involves a (seemingly novel) decomposition of the excess risk and the (shifted) generalization error, along with various estimates based on concentration by the Bernstein condition.\n- Theorem 1.2 provides what they call a variance-type bound (not sure why is it called like this), which bounds the risk in terms of the empirical risk and a deviation bound which leverages faster rates by uniform stability.\n- This bound seems useful in realizable settings, where the empirical risk can be made arbitrarily small.\n- The consequences for strongly convex stochastic optimization appear in Proposition 2.1 and its following discussion.\n- As mentioned above, this general bound for stable algorithms answers a question raised in Shalev-Shwartz et al (2009) which is quite interesting on its own.\n- In this result, the most problematic term is $\\sqrt{L^2\\overline{\\Delta}/\\lambda}$, where $\\overline{\\Delta}$ is a bound on the empirical error: this leads to optimal excess risk in this case for GD with O(n^2) iterations.\n- It is worth pointing out here that a similar iteration requirement appears in the nonsmooth case, as shown in Bassily, Feldman Guzman and Talwar (2020).\n- That paper also includes a lower bound on stability, showing that for GD $\\Omega(n^2)$ steps needed to obtain optimal rates with uniform stability.\n- It is reasonable to conjecture that a similar lower bound may hold in the strongly convex setting, and in fact some limitations of GD in terms of excess risk have already been shown in recent work by Amir, Koren and Livni (https://arxiv.org/pdf/2102.01117.pdf).\n- The latter result is shown for GD on the regularized ERM, but I suspect ideas there might be useful for the strongly convex setting as well.\n- Finally, I would suggest the authors to include a more detailed statement of the last paragraph before section 3 (lines 204-206 in page 6).\n- Here it is claimed a new high-probability excess risk bound for GD on smooth and strongly convex losses. For future use of this result, it might be a good idea to have a proper statement.",
    "review_points_list": [
      "To achieve the main result, a useful Lemma 3.1 is shown (this is a minor variation of a Lemma from BKZ): this result avoids the sampling error term $O(1/\\sqrt{n})$ by showing a moment bound of a 'shifted' generalization error.",
      "It would be interesting if the authors would elaborate further about the intuition on this refinement, and what are its consequences for the excess risk bounds (the technical proofs are clear, but nevertheless it might be useful a brief discussion).",
      "The proof of the main result (Thm 1.1) is obtained nontrivially from Lemma 3.1.",
      "This involves a (seemingly novel) decomposition of the excess risk and the (shifted) generalization error, along with various estimates based on concentration by the Bernstein condition.",
      "Theorem 1.2 provides what they call a variance-type bound (not sure why is it called like this), which bounds the risk in terms of the empirical risk and a deviation bound which leverages faster rates by uniform stability.",
      "This bound seems useful in realizable settings, where the empirical risk can be made arbitrarily small.",
      "The consequences for strongly convex stochastic optimization appear in Proposition 2.1 and its following discussion.",
      "As mentioned above, this general bound for stable algorithms answers a question raised in Shalev-Shwartz et al (2009) which is quite interesting on its own.",
      "In this result, the most problematic term is $\\sqrt{L^2\\overline{\\Delta}/\\lambda}$, where $\\overline{\\Delta}$ is a bound on the empirical error: this leads to optimal excess risk in this case for GD with O(n^2) iterations.",
      "It is worth pointing out here that a similar iteration requirement appears in the nonsmooth case, as shown in Bassily, Feldman Guzman and Talwar (2020).",
      "That paper also includes a lower bound on stability, showing that for GD $\\Omega(n^2)$ steps needed to obtain optimal rates with uniform stability.",
      "It is reasonable to conjecture that a similar lower bound may hold in the strongly convex setting, and in fact some limitations of GD in terms of excess risk have already been shown in recent work by Amir, Koren and Livni (https://arxiv.org/pdf/2102.01117.pdf).",
      "The latter result is shown for GD on the regularized ERM, but I suspect ideas there might be useful for the strongly convex setting as well.",
      "Finally, I would suggest the authors to include a more detailed statement of the last paragraph before section 3 (lines 204-206 in page 6).",
      "Here it is claimed a new high-probability excess risk bound for GD on smooth and strongly convex losses. For future use of this result, it might be a good idea to have a proper statement."
    ]
  },
  {
    "paper_id": "2103.12024v2",
    "submission_id": "yaxePRTOhqk",
    "submission_title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
    "review_id": "UhYn9K_sbsv",
    "input": {
      "title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- To my knowledge, this type of result was only proved in particular cases and this is the first general result on fast rates of convergence obtained via stability.\n- The article is clear and well written\n- I think this is significant for the Neurips community.\n- This article may generalize to a lot of learning problems and may become a tool to prove fast rates in a lot of different settings\n- This may be an important step to assess the rate of convergence of algorithms and to have theoretical rates that match the rates that we witness in practice\n- The authors also illustrate this generalization capacity on the problem of stochastic convex optimization with strongly convex losses",
    "review_points_list": [
      "To my knowledge, this type of result was only proved in particular cases and this is the first general result on fast rates of convergence obtained via stability.",
      "The article is clear and well written",
      "I think this is significant for the Neurips community.",
      "This article may generalize to a lot of learning problems and may become a tool to prove fast rates in a lot of different settings",
      "This may be an important step to assess the rate of convergence of algorithms and to have theoretical rates that match the rates that we witness in practice",
      "The authors also illustrate this generalization capacity on the problem of stochastic convex optimization with strongly convex losses"
    ]
  },
  {
    "paper_id": "2103.12024v2",
    "submission_id": "yaxePRTOhqk",
    "submission_title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
    "review_id": "x7BqHnLrYha",
    "input": {
      "title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper uses tools developed by Bousquet et al. [8] heavily to prove their results.\n- It is rather difficult to find a strongly convex function that is Lipschitz continuous and bounded.\n- An easy way to justify these conditions together is to assume that \\mathcal{X} is a compact set.\n- The contribution is of good value from a theoretical point of view.\n- As the authors pointed out, the generalized Bernstein assumption might be useful when considering optimization under PL-condition which is widely studied in recent years due to its usefulness for modern ML applications.\n- Overall, the paper makes a good contribution and might serve as a first step to thoroughly explore the new assumption in different settings.",
    "review_points_list": [
      "The paper uses tools developed by Bousquet et al. [8] heavily to prove their results.",
      "It is rather difficult to find a strongly convex function that is Lipschitz continuous and bounded.",
      "An easy way to justify these conditions together is to assume that \\mathcal{X} is a compact set.",
      "The contribution is of good value from a theoretical point of view.",
      "As the authors pointed out, the generalized Bernstein assumption might be useful when considering optimization under PL-condition which is widely studied in recent years due to its usefulness for modern ML applications.",
      "Overall, the paper makes a good contribution and might serve as a first step to thoroughly explore the new assumption in different settings."
    ]
  },
  {
    "paper_id": "2010.16344v2",
    "submission_id": "zHj5fx11jQC",
    "submission_title": "Marginalised Gaussian Processes with Nested Sampling",
    "review_id": "gO4uFCPVOwB",
    "input": {
      "title": "Marginalised Gaussian Processes with Nested Sampling",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This is a paper to bring the technical of nested sampling that is well-used in physics and cosmology to the machine learning GP community.\n- The paper is not very original and significant, given that nested sampling has been introduced previously in N(eur)IPS [Murray, 2005], and the authors of that paper have very deep knowledge of GP and are experts in this area.\n- It is also widely accepted that margalised (or Bayesian) GP is *the correct* thing to do, though it can be somewhat challenging to execute in practice.\n- Nevertheless, this is a worthwhile paper, illustrating the significance of Bayesian GP, esp in the context of the recently introduced spectral mixture kernel (SKM) which has multiple modes as illustrated by the authors.\n- Also useful is to bring to attention the DYNESTY package.\n- If the authors were to release their code, I can see that the code will be used frequently.\n- Section 4 is a particularly helpful advice for anyone attempting to use SKM.\n- The paper is also well-written.\n- I prefer the term 'Bayesian Gaussian Processes' rather than 'Marginalised Gaussian Processes' in the title, but this is a matter of personal preference.\n- Given that the study is limited to SKM, this should be in the title.\n- Reorganised the paper to put SMK material in section 2 together with section 4.\n- It is paramount to cite and discuss [Murray, 2005].\n- I beg to differ from the sentence from line 232 to 233. The pattern for NS is too clear on along the axis in Figure 4, and then it blurs out along the edges. This is not what the generating function gives.\n- It is nevertheless reassuring that all these are better than NKN.\n- Lines 262 to 263 tries to give something negative a positive light. What I read is that for simpler kernels/covariance-functions like the RBF/squared-exponential that are typically used in practice, there is no requirement for NS.\n- Lines 284 to 286 requires a correct heading.",
    "review_points_list": [
      "This is a paper to bring the technical of nested sampling that is well-used in physics and cosmology to the machine learning GP community.",
      "The paper is not very original and significant, given that nested sampling has been introduced previously in N(eur)IPS [Murray, 2005], and the authors of that paper have very deep knowledge of GP and are experts in this area.",
      "It is also widely accepted that margalised (or Bayesian) GP is *the correct* thing to do, though it can be somewhat challenging to execute in practice.",
      "Nevertheless, this is a worthwhile paper, illustrating the significance of Bayesian GP, esp in the context of the recently introduced spectral mixture kernel (SKM) which has multiple modes as illustrated by the authors.",
      "Also useful is to bring to attention the DYNESTY package.",
      "If the authors were to release their code, I can see that the code will be used frequently.",
      "Section 4 is a particularly helpful advice for anyone attempting to use SKM.",
      "The paper is also well-written.",
      "I prefer the term 'Bayesian Gaussian Processes' rather than 'Marginalised Gaussian Processes' in the title, but this is a matter of personal preference.",
      "Given that the study is limited to SKM, this should be in the title.",
      "Reorganised the paper to put SMK material in section 2 together with section 4.",
      "It is paramount to cite and discuss [Murray, 2005].",
      "I beg to differ from the sentence from line 232 to 233. The pattern for NS is too clear on along the axis in Figure 4, and then it blurs out along the edges. This is not what the generating function gives.",
      "It is nevertheless reassuring that all these are better than NKN.",
      "Lines 262 to 263 tries to give something negative a positive light. What I read is that for simpler kernels/covariance-functions like the RBF/squared-exponential that are typically used in practice, there is no requirement for NS.",
      "Lines 284 to 286 requires a correct heading."
    ]
  },
  {
    "paper_id": "2010.16344v2",
    "submission_id": "zHj5fx11jQC",
    "submission_title": "Marginalised Gaussian Processes with Nested Sampling",
    "review_id": "72jbfYte5tG",
    "input": {
      "title": "Marginalised Gaussian Processes with Nested Sampling",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The problem the authors consider is an important one in the Bayesian community.\n- Apart from some minors (see below), I found the paper also well written.\n- The experiments seem rigorously designed.\n- While I find the paper interesting and the experiments insightful, my main concern is the novelty of the proposed method.\n- While it is true, that as the authors state NS isn\u2019t well known in the ML community, it has been discussed to evaluate the marginal likelihood integral (see e.g. the thesis of Ian Murray [https://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf], which btw contains some nice illustrations to explain NS intuitively) and was also used in several works (of whose to be fair the manuscript is citing several).\n- The main contribution of the current work comes from more rigorously comparing NS to the dominant solutions (ML-II, HMC), to solve the integral of interest, and using NS for complicated priors (priors with spectral mixture kernels) in the GP regression setting.\n- Overall, I think the paper is and edge case, where on the pro-side stands a rigour empirical study to showcase the advantages of NS as a widely used model in the community, and on the contra limited novelty.\n- In my opinion, the former slightly dominates, hence my rating.\n- After reading the authors response and the other reviews, I think, I was potentially underestimating the novelty of the work slightly.\n- Hence, I will increase my score to 7.",
    "review_points_list": [
      "The problem the authors consider is an important one in the Bayesian community.",
      "Apart from some minors (see below), I found the paper also well written.",
      "The experiments seem rigorously designed.",
      "While I find the paper interesting and the experiments insightful, my main concern is the novelty of the proposed method.",
      "While it is true, that as the authors state NS isn\u2019t well known in the ML community, it has been discussed to evaluate the marginal likelihood integral (see e.g. the thesis of Ian Murray [https://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf], which btw contains some nice illustrations to explain NS intuitively) and was also used in several works (of whose to be fair the manuscript is citing several).",
      "The main contribution of the current work comes from more rigorously comparing NS to the dominant solutions (ML-II, HMC), to solve the integral of interest, and using NS for complicated priors (priors with spectral mixture kernels) in the GP regression setting.",
      "Overall, I think the paper is and edge case, where on the pro-side stands a rigour empirical study to showcase the advantages of NS as a widely used model in the community, and on the contra limited novelty.",
      "In my opinion, the former slightly dominates, hence my rating.",
      "After reading the authors response and the other reviews, I think, I was potentially underestimating the novelty of the work slightly.",
      "Hence, I will increase my score to 7."
    ]
  },
  {
    "paper_id": "2010.16344v2",
    "submission_id": "zHj5fx11jQC",
    "submission_title": "Marginalised Gaussian Processes with Nested Sampling",
    "review_id": "_sX2v5REA2O",
    "input": {
      "title": "Marginalised Gaussian Processes with Nested Sampling",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper addresses a problem of marginalizing GP hyperparameters.\n- The proposed technique is nested sampling of the hyperparameters.\n- Nested sampling (NS henceforth) was developed previously, however the application of the technique to GPs is original work.\n- The authors thoroughly cite related work.\n- NS improves GP behavior compared to a point estimate of the hyperparameters.\n- NS takes significantly less computation time than exploring the posterior with HMC.\n- Performance comparison between NS and HMC seems to depend largely on the kernel and the dataset.\n- There is also a comparison with Neural Kernel Network.\n- The claims are supported by experiments.\n- The paper is very well-written and organized.\n- The supplementary material provides settings to reproduce the results.\n- p. 3 l. 79: complete \u2013 typo\n- It could also benefit to have larger pictures or larger fonts in the pictures, because they have to be ridiculously zoomed-in to be seen.\n- Also, please provide a clearer explanation of the choice of the uniform prior for frequencies larger than the fundamental.\n- The results are important because they provide a relatively low-cost (compared to HMC) technique to marginalize SM kernel hyperparameters which improves the performance of the GP.\n- The SM kernel is indeed a very expressive family.\n- Overall, very good paper.",
    "review_points_list": [
      "The paper addresses a problem of marginalizing GP hyperparameters.",
      "The proposed technique is nested sampling of the hyperparameters.",
      "Nested sampling (NS henceforth) was developed previously, however the application of the technique to GPs is original work.",
      "The authors thoroughly cite related work.",
      "NS improves GP behavior compared to a point estimate of the hyperparameters.",
      "NS takes significantly less computation time than exploring the posterior with HMC.",
      "Performance comparison between NS and HMC seems to depend largely on the kernel and the dataset.",
      "There is also a comparison with Neural Kernel Network.",
      "The claims are supported by experiments.",
      "The paper is very well-written and organized.",
      "The supplementary material provides settings to reproduce the results.",
      "p. 3 l. 79: complete \u2013 typo",
      "It could also benefit to have larger pictures or larger fonts in the pictures, because they have to be ridiculously zoomed-in to be seen.",
      "Also, please provide a clearer explanation of the choice of the uniform prior for frequencies larger than the fundamental.",
      "The results are important because they provide a relatively low-cost (compared to HMC) technique to marginalize SM kernel hyperparameters which improves the performance of the GP.",
      "The SM kernel is indeed a very expressive family.",
      "Overall, very good paper."
    ]
  },
  {
    "paper_id": "2010.16344v2",
    "submission_id": "zHj5fx11jQC",
    "submission_title": "Marginalised Gaussian Processes with Nested Sampling",
    "review_id": "znLeJ4qdXDC",
    "input": {
      "title": "Marginalised Gaussian Processes with Nested Sampling",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper highlights the method of nested sampling, popular in physics, as a method for marginalizing over hyperparameters of spectral mixture kernels. This is a valuable contribution.\n- The conducted empirical study is convincing in showing that the method is indeed promising in this new setting.\n- The method is only benchmarked in low dimensions but may be useful in the time series modeling problems which are quite broad and important themselves.\n- As far as I can tell, the contribution is novel.\n- The claims are supported by an extensive empirical evaluation.\n- On the downside, the implementation is not provided and said to be proprietary.\n- The paper is very well written.\n- The most serious problem of the sort spotted by me in the paper is fonts being too small in some figures (especially in Figure 2).\n- The method suggested by the paper and the corresponding empirical study will be valuable for machine learning practitioners who use low-dimensional Gaussian processes.\n- Mostly, this work is relevant for time series modeling.\n- Regrettably, this positive impact is somewhat limited by the fact that the code will not be openly released for people to be able to quickly and easily apply the method to their problems.\n- Line 17. I suggest giving a reference or briefly discussing the abbreviation ML-II, it might not be a universally known one.\n- Line 39. \u2018lies not its\u2019->\u2019lies not in its\u2019.\n- Line 44. \u2018some the\u2019-> \u2018some\u2019.\n- Lines 67-68. Phrase \u2018the final form of the posterior predictive in a marginalised GOs is a mixture of Gaussians\u2019 is not rigorous because it is only approximately so, I suggest reflecting this in text.\n- Line 75. \u2018consider\u2019->\u2018considered\u2019.\n- Line 76. \u2018demonstrate\u2019->\u2018demonstrated\u2019.\n- Line 79. \u2018complet\u2019->\u2018complex\u2019.\n- All figures except Figure 3 need larger fonts.",
    "review_points_list": [
      "The paper highlights the method of nested sampling, popular in physics, as a method for marginalizing over hyperparameters of spectral mixture kernels. This is a valuable contribution.",
      "The conducted empirical study is convincing in showing that the method is indeed promising in this new setting.",
      "The method is only benchmarked in low dimensions but may be useful in the time series modeling problems which are quite broad and important themselves.",
      "As far as I can tell, the contribution is novel.",
      "The claims are supported by an extensive empirical evaluation.",
      "On the downside, the implementation is not provided and said to be proprietary.",
      "The paper is very well written.",
      "The most serious problem of the sort spotted by me in the paper is fonts being too small in some figures (especially in Figure 2).",
      "The method suggested by the paper and the corresponding empirical study will be valuable for machine learning practitioners who use low-dimensional Gaussian processes.",
      "Mostly, this work is relevant for time series modeling.",
      "Regrettably, this positive impact is somewhat limited by the fact that the code will not be openly released for people to be able to quickly and easily apply the method to their problems.",
      "Line 17. I suggest giving a reference or briefly discussing the abbreviation ML-II, it might not be a universally known one.",
      "Line 39. \u2018lies not its\u2019->\u2019lies not in its\u2019.",
      "Line 44. \u2018some the\u2019-> \u2018some\u2019.",
      "Lines 67-68. Phrase \u2018the final form of the posterior predictive in a marginalised GOs is a mixture of Gaussians\u2019 is not rigorous because it is only approximately so, I suggest reflecting this in text.",
      "Line 75. \u2018consider\u2019->\u2018considered\u2019.",
      "Line 76. \u2018demonstrate\u2019->\u2018demonstrated\u2019.",
      "Line 79. \u2018complet\u2019->\u2018complex\u2019.",
      "All figures except Figure 3 need larger fonts."
    ]
  },
  {
    "paper_id": "2102.09381v1",
    "submission_id": "zoQJBVrhnn3",
    "submission_title": "Learning to Safely Exploit a Non-Stationary Opponent",
    "review_id": "WCvxBjNajAW",
    "input": {
      "title": "Learning to Safely Exploit a Non-Stationary Opponent",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- There appears to be a major problem with the paper.\n- The main algorithm uses an RNR solver based on an LP formulation that is provided in the appendix, but this formulation is incorrect.\n- In particular, the first LP on page 579 on page 14 is incorrect, since the vector y_a is defined incorrectly on line 578.\n- It should be y_a(j) = p pi_fix(a)  for j != a, not y_a(j) = 0 for j != a (the j=a case is correct).\n- There also should not be a need for the second LP, since player 1\u2019s strategy should correspond to the dual variables from the first LP.\n- The second LP does appear correct, though the second set of constraints (pi^2(a) <= p pi_fix(a) + 1 \u2013 p) is redundant and implied by the final two constraints.\n- Since the algorithms are based on an incorrect formulation, I assume all the experiments are incorrect, so I can\u2019t really assess them further.\n- I think that overall the problem is important and the approach is promising.\n- 37: dominated actions are not the only forms of irrationality in games. In fact, dominated actions are very rare in realistic poker games. See \u201cMistakes in Games\u201d Ganzfried DAI 2019.\n- 38: a NE -> an NE\n- naive -> na\u00efve\n- 90: Seems weird to cite a recent 2020 paper for the result that two-player zero-sum games can be solved in polynomial time by an LP.\n- 109: States that the approaches can be extended to multiplayer games, but RNR is defined only for two-player zero-sum games, so it should be elaborated on how the approach is applicable to multiplayer games.\n- Section 3.1 seems to just present the standard definition of a normal-form game. I\u2019m not sure why it is called a \u201cmeta normal-form game.\u201d What is the \u201cmeta\u201d part referring to that differentiates from a standard NFG?\n- Footnote i refers to stochastic games, while the model in the paper is just for normal-form games. Why is there a reference to stochastic games here?\n- The game model described is for normal-form games, but experiments are on Kuhn poker, which is an extensive-form imperfect-information game.\n- 152: life-time -> lifetime\n- 156: why use a maximum-a-posteriori approach instead of alternatives?\n- 163: rolls-out -> rolls out\n- It would be helpful if some more information about the PPO algorithm is included, since it is the opposing agent as well as one of the benchmark algorithms.\n- I think it might be helpful to include a static Nash equilibrium strategy as an additional benchmark strategy (there are multiple equilibria in this game, so perhaps some of the extreme point NE can be used).\n- Does \u201cstds\u201d mean \u201cstandard deviations\u201d?\n- In Table 1 the standard deviations seem very large compared to the payoff values, which indicate that additional experiments should be run.\n- Additional comments after the responses and discussion:\n- Based on the response it is likely that the authors probably used the correct LP formulation in the experiments and just wrote it wrong in the paper.\n- The authors claim that they have done additional experiments that achieve the same result. I'm not sure if we can just take their word. I think it is necessary to obtain statistical significance, and the current results do not come near this. In the response to 8Pgk they state that they also have comparisons against other benchmark agents SAM and MCCFR that show superiority of the new approach. But again I'm not sure if we can take into account results not in the paper. I would lean towards marginal acceptance if results showing statistical significance as well as comparisons against SAM and MCCFR were included.\n- I would still be interested in seeing a comparison against static NE strategies (there are multiple NE). I think this is a very important benchmark to include.\n- Author response: 'Mistakes in Games (Ganzfried 2019): Thank you for pointing out this interesting work. We will reword \u201cdominated actions\u201d to \u201cdominated strategies\u201d and discuss the difference between the two terms with reference to the work.'",
    "review_points_list": [
      "There appears to be a major problem with the paper.",
      "The main algorithm uses an RNR solver based on an LP formulation that is provided in the appendix, but this formulation is incorrect.",
      "In particular, the first LP on page 579 on page 14 is incorrect, since the vector y_a is defined incorrectly on line 578.",
      "It should be y_a(j) = p pi_fix(a)  for j != a, not y_a(j) = 0 for j != a (the j=a case is correct).",
      "There also should not be a need for the second LP, since player 1\u2019s strategy should correspond to the dual variables from the first LP.",
      "The second LP does appear correct, though the second set of constraints (pi^2(a) <= p pi_fix(a) + 1 \u2013 p) is redundant and implied by the final two constraints.",
      "Since the algorithms are based on an incorrect formulation, I assume all the experiments are incorrect, so I can\u2019t really assess them further.",
      "I think that overall the problem is important and the approach is promising.",
      "37: dominated actions are not the only forms of irrationality in games. In fact, dominated actions are very rare in realistic poker games. See \u201cMistakes in Games\u201d Ganzfried DAI 2019.",
      "38: a NE -> an NE",
      "naive -> na\u00efve",
      "90: Seems weird to cite a recent 2020 paper for the result that two-player zero-sum games can be solved in polynomial time by an LP.",
      "109: States that the approaches can be extended to multiplayer games, but RNR is defined only for two-player zero-sum games, so it should be elaborated on how the approach is applicable to multiplayer games.",
      "Section 3.1 seems to just present the standard definition of a normal-form game. I\u2019m not sure why it is called a \u201cmeta normal-form game.\u201d What is the \u201cmeta\u201d part referring to that differentiates from a standard NFG?",
      "Footnote i refers to stochastic games, while the model in the paper is just for normal-form games. Why is there a reference to stochastic games here?",
      "The game model described is for normal-form games, but experiments are on Kuhn poker, which is an extensive-form imperfect-information game.",
      "152: life-time -> lifetime",
      "156: why use a maximum-a-posteriori approach instead of alternatives?",
      "163: rolls-out -> rolls out",
      "It would be helpful if some more information about the PPO algorithm is included, since it is the opposing agent as well as one of the benchmark algorithms.",
      "I think it might be helpful to include a static Nash equilibrium strategy as an additional benchmark strategy (there are multiple equilibria in this game, so perhaps some of the extreme point NE can be used).",
      "Does \u201cstds\u201d mean \u201cstandard deviations\u201d?",
      "In Table 1 the standard deviations seem very large compared to the payoff values, which indicate that additional experiments should be run.",
      "Additional comments after the responses and discussion:",
      "Based on the response it is likely that the authors probably used the correct LP formulation in the experiments and just wrote it wrong in the paper.",
      "The authors claim that they have done additional experiments that achieve the same result. I'm not sure if we can just take their word. I think it is necessary to obtain statistical significance, and the current results do not come near this. In the response to 8Pgk they state that they also have comparisons against other benchmark agents SAM and MCCFR that show superiority of the new approach. But again I'm not sure if we can take into account results not in the paper. I would lean towards marginal acceptance if results showing statistical significance as well as comparisons against SAM and MCCFR were included.",
      "I would still be interested in seeing a comparison against static NE strategies (there are multiple NE). I think this is a very important benchmark to include.",
      "Author response: 'Mistakes in Games (Ganzfried 2019): Thank you for pointing out this interesting work. We will reword \u201cdominated actions\u201d to \u201cdominated strategies\u201d and discuss the difference between the two terms with reference to the work.'"
    ]
  },
  {
    "paper_id": "2102.09381v1",
    "submission_id": "zoQJBVrhnn3",
    "submission_title": "Learning to Safely Exploit a Non-Stationary Opponent",
    "review_id": "yxFkhuxyzNE",
    "input": {
      "title": "Learning to Safely Exploit a Non-Stationary Opponent",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The problem of opponent modeling in games has been extensively studied before.\n- The idea to use the Dirichlet process (DP) to solve it may be new and is well motivated in the paper.\n- The paper is reasonably clear and understandable.\n- However, it would be further improved by use of examples to demonstrate the notions and algorithms on a specific tiny game.\n- The notation and basic claims in the paper seem sound.\n- My main concern is with the positioning with respect to the related work and the quality of the empirical evaluation.\n- Opponent modelling in games is a classical problem with many solutions.\n- This paper reviews a part of this literature, but it does not provide theoretical or empirical arguments on superiority of the proposed approach to the existing ones.\n- A natural candidate for the context of the paper would be, for example, the implicit agent modelling framework [A], but at least a compelling argument of superiority to [10,15,49] either theoretically or empirically should be provided.\n- There is no discussion of the computational complexity of the proposed method.\n- It looks much more expensive than some of the alternatives mentioned above and the evaluation is only on a tiny game.\n- How well scalable would the approach be to larger games?\n- The experiments are performed on one small game against one opponent on 5 runs of the system, without any statistical analysis of the confidence intervals.\n- It is not sufficient to make any conclusions.\n- The proposed approach might be a good idea.\n- However, without a clear comparison to competing methods, I do not think someone would choose to implement and use it on some practical problem.\n- From the theoretical perspective, I understand the paper as an application of an existing method of DP to the problem of opponent modelling.\n- Hence, there is no theoretical advancement.\n- If it is not so, I assume the authors will argue for the theoretical novelty in the rebuttal.\n- Bard, N., Johanson, M., Burch, N., & Bowling, M. (2013, May). Online implicit agent modelling. In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems (pp. 255-262).",
    "review_points_list": [
      "The problem of opponent modeling in games has been extensively studied before.",
      "The idea to use the Dirichlet process (DP) to solve it may be new and is well motivated in the paper.",
      "The paper is reasonably clear and understandable.",
      "However, it would be further improved by use of examples to demonstrate the notions and algorithms on a specific tiny game.",
      "The notation and basic claims in the paper seem sound.",
      "My main concern is with the positioning with respect to the related work and the quality of the empirical evaluation.",
      "Opponent modelling in games is a classical problem with many solutions.",
      "This paper reviews a part of this literature, but it does not provide theoretical or empirical arguments on superiority of the proposed approach to the existing ones.",
      "A natural candidate for the context of the paper would be, for example, the implicit agent modelling framework [A], but at least a compelling argument of superiority to [10,15,49] either theoretically or empirically should be provided.",
      "There is no discussion of the computational complexity of the proposed method.",
      "It looks much more expensive than some of the alternatives mentioned above and the evaluation is only on a tiny game.",
      "How well scalable would the approach be to larger games?",
      "The experiments are performed on one small game against one opponent on 5 runs of the system, without any statistical analysis of the confidence intervals.",
      "It is not sufficient to make any conclusions.",
      "The proposed approach might be a good idea.",
      "However, without a clear comparison to competing methods, I do not think someone would choose to implement and use it on some practical problem.",
      "From the theoretical perspective, I understand the paper as an application of an existing method of DP to the problem of opponent modelling.",
      "Hence, there is no theoretical advancement.",
      "If it is not so, I assume the authors will argue for the theoretical novelty in the rebuttal.",
      "Bard, N., Johanson, M., Burch, N., & Bowling, M. (2013, May). Online implicit agent modelling. In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems (pp. 255-262)."
    ]
  },
  {
    "paper_id": "2102.09381v1",
    "submission_id": "zoQJBVrhnn3",
    "submission_title": "Learning to Safely Exploit a Non-Stationary Opponent",
    "review_id": "6qekFg96lf6",
    "input": {
      "title": "Learning to Safely Exploit a Non-Stationary Opponent",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper addresses an old problem that has, from my perspective, been re-hashed repeatedly over the last several decades.\n- Despite being foundational and somewhat simple, the answers remain extremely difficult and hard to come by (I argue that the problem itself is ill-posed).\n- The paper is, for the most part, well-written and polished. The technical description of the algorithm is detailed and seems solid (albeit I found the algorithm description difficult to follow).\n- I really like the two measures the paper analyzes: ability to exploit and exposure to exploitation. This trade-off is central to the problem at hand.\n- For the empirical results that are presented, the algorithm appears to perform well. The results are limited, and in some sense can be viewed as simply anecdotal.\n- However, I also do not want to diminish the strength of this contribution, as getting an algorithm to do this even in this limited context is no small accomplishment.\n- I read the results differently than how they are discussed in the text of the paper (which seems to be bent on advocating, perhaps understandably so, for the algorithm presented in the paper).\n- To me, the key insight of the paper is the trade-off between exploitability and exploitation.\n- I\u2019m not sure what the ideal trade-off between these measures is, but we could (for example) weight them equally.\n- If we do that, Figure 2 clearly shows that exploitability has a higher magnitude across the algorithms than does exploitation.\n- If we simply subtract exploitability from exploitation, it appears to me that only PSRO has a positive value, suggesting it may be the best algorithm in this case among the 5 presented (though, again, the trade-off assessment of equal value between the metrics is subjective).\n- In the second set of results shown in Table 1, a similar trend continues, except that CEPSOM also has a positive weight.\n- But the results don\u2019t seem to be telling me that CEPSOM necessarily obtains a better trade-off than PSRO (which is essentially minimax in this game, as I understand it).\n- In all honesty, it is hard to make any general assessment about EPSOM and CEPSOM from these limited results.\n- Is there some opponent that could really exploit CEPSOM. If so, when and how? If not, when and how?\n- The authors state at the outset of the paper that they are not interested in self-play. But self-play is a foundational comparison in this domain, as any good algorithm would be adopted by other agents over time necessitating a successful algorithm to be good (in some sense) in self-play.",
    "review_points_list": [
      "The paper addresses an old problem that has, from my perspective, been re-hashed repeatedly over the last several decades.",
      "Despite being foundational and somewhat simple, the answers remain extremely difficult and hard to come by (I argue that the problem itself is ill-posed).",
      "The paper is, for the most part, well-written and polished. The technical description of the algorithm is detailed and seems solid (albeit I found the algorithm description difficult to follow).",
      "I really like the two measures the paper analyzes: ability to exploit and exposure to exploitation. This trade-off is central to the problem at hand.",
      "For the empirical results that are presented, the algorithm appears to perform well. The results are limited, and in some sense can be viewed as simply anecdotal.",
      "However, I also do not want to diminish the strength of this contribution, as getting an algorithm to do this even in this limited context is no small accomplishment.",
      "I read the results differently than how they are discussed in the text of the paper (which seems to be bent on advocating, perhaps understandably so, for the algorithm presented in the paper).",
      "To me, the key insight of the paper is the trade-off between exploitability and exploitation.",
      "I\u2019m not sure what the ideal trade-off between these measures is, but we could (for example) weight them equally.",
      "If we do that, Figure 2 clearly shows that exploitability has a higher magnitude across the algorithms than does exploitation.",
      "If we simply subtract exploitability from exploitation, it appears to me that only PSRO has a positive value, suggesting it may be the best algorithm in this case among the 5 presented (though, again, the trade-off assessment of equal value between the metrics is subjective).",
      "In the second set of results shown in Table 1, a similar trend continues, except that CEPSOM also has a positive weight.",
      "But the results don\u2019t seem to be telling me that CEPSOM necessarily obtains a better trade-off than PSRO (which is essentially minimax in this game, as I understand it).",
      "In all honesty, it is hard to make any general assessment about EPSOM and CEPSOM from these limited results.",
      "Is there some opponent that could really exploit CEPSOM. If so, when and how? If not, when and how?",
      "The authors state at the outset of the paper that they are not interested in self-play. But self-play is a foundational comparison in this domain, as any good algorithm would be adopted by other agents over time necessitating a successful algorithm to be good (in some sense) in self-play."
    ]
  },
  {
    "paper_id": "2106.07631v3",
    "submission_id": "zmbiQmdtg9",
    "submission_title": "Improved Transformer for High-Resolution GANs",
    "review_id": "_aDLS3-PSF6",
    "input": {
      "title": "Improved Transformer for High-Resolution GANs",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper presents a new Transformer-based architecture for the generator in GANs.\n- The authors introduce several new techniques to make it work for the Transformer-based generative model.\n- This submission is technically sound and most claims are well supported.\n- This paper is clearly written and well organized.\n- This paper is very important for exploring Transformer-based architecture for generative models.\n- I am curious about what is a good noise injection method in the Transformer-based generative models.\n- Previous well-known methods StyleGAN and StyleGAN2 apply the AdaIN and modulated convolution for noise injection.\n- HiT proposes the cross-attention for noise injection, the authors should conduct experiments to compare cross-attention with these methods, maybe AdaIN and modulated convolution also work for the Transformer-based generative model.\n- Since the Transformer-based architectures are more data-hungry, maybe the authors can conduct experiments on large-scale datasets: LSUN-cats and LSUN-cars and compare the performance with StyleGAN2 without using bCR.\n- Transformer-based architectures have shown effectiveness in classification models, one of the most important reasons is the capability of capturing long-range dependencies.\n- But in HiT, the self-attention module is only applied in low-resolution stages, make the generator fails to capture long-range dependencies in high-resolution stages.\n- The authors may give some explanations for self-attention modules in low-resolution stages are enough in generative high-quality images.",
    "review_points_list": [
      "This paper presents a new Transformer-based architecture for the generator in GANs.",
      "The authors introduce several new techniques to make it work for the Transformer-based generative model.",
      "This submission is technically sound and most claims are well supported.",
      "This paper is clearly written and well organized.",
      "This paper is very important for exploring Transformer-based architecture for generative models.",
      "I am curious about what is a good noise injection method in the Transformer-based generative models.",
      "Previous well-known methods StyleGAN and StyleGAN2 apply the AdaIN and modulated convolution for noise injection.",
      "HiT proposes the cross-attention for noise injection, the authors should conduct experiments to compare cross-attention with these methods, maybe AdaIN and modulated convolution also work for the Transformer-based generative model.",
      "Since the Transformer-based architectures are more data-hungry, maybe the authors can conduct experiments on large-scale datasets: LSUN-cats and LSUN-cars and compare the performance with StyleGAN2 without using bCR.",
      "Transformer-based architectures have shown effectiveness in classification models, one of the most important reasons is the capability of capturing long-range dependencies.",
      "But in HiT, the self-attention module is only applied in low-resolution stages, make the generator fails to capture long-range dependencies in high-resolution stages.",
      "The authors may give some explanations for self-attention modules in low-resolution stages are enough in generative high-quality images."
    ]
  },
  {
    "paper_id": "2106.07631v3",
    "submission_id": "zmbiQmdtg9",
    "submission_title": "Improved Transformer for High-Resolution GANs",
    "review_id": "mzWzQ01gA5P",
    "input": {
      "title": "Improved Transformer for High-Resolution GANs",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This is a well-written paper with carefully designed experiments.\n- The motivation for utilizing the multi-axis blocked self-attention is reasonable.\n- The proposed cross-attention for input and intermediate features is novel and interesting.\n- The authors conduct various experiments to show the effectiveness of the proposed results, both quantitatively and qualitatively.\n- Hit has a linear computational complexity is a bit overclaim.\n- Since the full HiT model uses balancing attention between axes and its computational complexity is N*sqrt(N).\n- Why not apply bCR at 1024x1024 resolution?\n- Is not using bCR the reason why HIT is worse than StyleGAN2 at the 1024x1024 resolution?",
    "review_points_list": [
      "This is a well-written paper with carefully designed experiments.",
      "The motivation for utilizing the multi-axis blocked self-attention is reasonable.",
      "The proposed cross-attention for input and intermediate features is novel and interesting.",
      "The authors conduct various experiments to show the effectiveness of the proposed results, both quantitatively and qualitatively.",
      "Hit has a linear computational complexity is a bit overclaim.",
      "Since the full HiT model uses balancing attention between axes and its computational complexity is N*sqrt(N).",
      "Why not apply bCR at 1024x1024 resolution?",
      "Is not using bCR the reason why HIT is worse than StyleGAN2 at the 1024x1024 resolution?"
    ]
  },
  {
    "paper_id": "2106.07631v3",
    "submission_id": "zmbiQmdtg9",
    "submission_title": "Improved Transformer for High-Resolution GANs",
    "review_id": "nuhWYIg2Hhp",
    "input": {
      "title": "Improved Transformer for High-Resolution GANs",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Some motivations should be highlighted. What are the issues of directly using ViT into GAN? In this paper, the authors do not directly apply ViT in GAN. However, there are some papers that propose ViT-based GAN methods. In the low-resolution stage, why replace standard global self-attention. In the high-resolution stage, why drop self-attention in the proposed method?\n- Some related work should be discussed in the paper. There are some papers [1, 2] that propose ViT-based GAN methods. Could you please discuss the difference between them? In addition, one recent paper [3] proposes new attention to generate high-resolution images. Could you please discuss it in the related work?\n- The novelty of the proposed method should be highlighted. The proposed method seems to combine the Nested Transformer and the Axial attention. The authors should highlight that such a combination is non-trivial.\n- Some technical details are not clear. In Figure 1, what is block $B_i$? Are the two $B_0$ the same in the figure? In Line 154, why $b \\times b \\sim H/b \\times W/b$? In Lines 157 and 174, could you please give a more detailed analysis of complexity?\n- The experiments can be improved. In Table 3, it would be better to compare with BigGANs [4].  In the ablation study, the authors should compare with the Nested Transformer.",
    "review_points_list": [
      "Some motivations should be highlighted. What are the issues of directly using ViT into GAN? In this paper, the authors do not directly apply ViT in GAN. However, there are some papers that propose ViT-based GAN methods. In the low-resolution stage, why replace standard global self-attention. In the high-resolution stage, why drop self-attention in the proposed method?",
      "Some related work should be discussed in the paper. There are some papers [1, 2] that propose ViT-based GAN methods. Could you please discuss the difference between them? In addition, one recent paper [3] proposes new attention to generate high-resolution images. Could you please discuss it in the related work?",
      "The novelty of the proposed method should be highlighted. The proposed method seems to combine the Nested Transformer and the Axial attention. The authors should highlight that such a combination is non-trivial.",
      "Some technical details are not clear. In Figure 1, what is block $B_i$? Are the two $B_0$ the same in the figure? In Line 154, why $b \\times b \\sim H/b \\times W/b$? In Lines 157 and 174, could you please give a more detailed analysis of complexity?",
      "The experiments can be improved. In Table 3, it would be better to compare with BigGANs [4].  In the ablation study, the authors should compare with the Nested Transformer."
    ]
  },
  {
    "paper_id": "2109.14688v1",
    "submission_id": "xB4lGVLvXDz",
    "submission_title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
    "review_id": "-KjG5aJ_kB",
    "input": {
      "title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The constrained RHKS function for density ratio estimation appears to be a novel idea.\n- It provides a different formulation of deep kernels for supervised and unsupervised settings.\n- The analysis method uses standard theoretical methods, such as concentration inequalities and covering numbers.\n- Although some analyses have more in-depth derivations/proofs, and the experiments are somewhat underdeveloped, I think the method and theoretical contributions are clear.\n- The written quality can be improved.\n- For example, eqn (5) defines $\tilde{\rho}$ later, and $\tilde{\tau}$ is not defined.\n- I struggle to follow the proof logic in Lemma 1, and there seem to be some minor inconsistencies.\n- What would be valuable is the contrapositive of Eqn (17), i.e. $\tilde{\rho}_z(f_j) \rangle \theta \rightarrow ... \theta 2\theta$.\n- Is $\tilde{\tau}$ set to $\frac{\theta}{2\theta S_R}$ in the proof or $\frac{\theta}{4\theta S_R}$ in the main text?\n- Line 219: How does the norm of $\tilde{g}$ relate to $\tilde{R}$ in Theorem 2?\n- There are questions about the experimental results.\n- 1. In Figure 1, the top row shows a small bias for the blue clusters. Does this always persist? The bias in the bottom row also doesn't disappear.\n- 2. In Figure 3, the neural network decoding appears poor towards the end of training, but there seems to be a relatively stable region before the loss increases.\n- How does the model perform in terms of decoding at that stage?\n- The y-axis scale is not equal, making it slightly difficult to compare results.\n- Although the authors highlight stability, is the proposed method reliable and effective in terms of standard metrics like log-likelihood or sample quality (FID, KID)?",
    "review_points_list": [
      "The constrained RHKS function for density ratio estimation appears to be a novel idea.",
      "It provides a different formulation of deep kernels for supervised and unsupervised settings.",
      "The analysis method uses standard theoretical methods, such as concentration inequalities and covering numbers.",
      "Although some analyses have more in-depth derivations/proofs, and the experiments are somewhat underdeveloped, I think the method and theoretical contributions are clear.",
      "The written quality can be improved.",
      "For example, eqn (5) defines $\tilde{\rho}$ later, and $\tilde{\tau}$ is not defined.",
      "I struggle to follow the proof logic in Lemma 1, and there seem to be some minor inconsistencies.",
      "What would be valuable is the contrapositive of Eqn (17), i.e. $\tilde{\rho}_z(f_j) \rangle \theta \rightarrow ... \theta 2\theta$.",
      "Is $\tilde{\tau}$ set to $\frac{\theta}{2\theta S_R}$ in the proof or $\frac{\theta}{4\theta S_R}$ in the main text?",
      "Line 219: How does the norm of $\tilde{g}$ relate to $\tilde{R}$ in Theorem 2?",
      "There are questions about the experimental results.",
      "1. In Figure 1, the top row shows a small bias for the blue clusters. Does this always persist? The bias in the bottom row also doesn't disappear.",
      "2. In Figure 3, the neural network decoding appears poor towards the end of training, but there seems to be a relatively stable region before the loss increases.",
      "How does the model perform in terms of decoding at that stage?",
      "The y-axis scale is not equal, making it slightly difficult to compare results.",
      "Although the authors highlight stability, is the proposed method reliable and effective in terms of standard metrics like log-likelihood or sample quality (FID, KID)?"
    ]
  },
  {
    "paper_id": "2109.14688v1",
    "submission_id": "xB4lGVLvXDz",
    "submission_title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
    "review_id": "OsShS1lsmgH",
    "input": {
      "title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is well-organized and presents the results clearly.\n- The authors provide a comprehensive literature review to support their work.\n- The methodology section lacks sufficient detail and clarity.\n- Figures 3 and 4 are hard to interpret, and the captions need more explanation.\n- The conclusion does not effectively summarize the key findings of the study.\n- Overall, the paper has potential but requires significant revisions to be publication-ready.",
    "review_points_list": [
      "The paper is well-organized and presents the results clearly.",
      "The authors provide a comprehensive literature review to support their work.",
      "The methodology section lacks sufficient detail and clarity.",
      "Figures 3 and 4 are hard to interpret, and the captions need more explanation.",
      "The conclusion does not effectively summarize the key findings of the study.",
      "Overall, the paper has potential but requires significant revisions to be publication-ready."
    ]
  },
  {
    "paper_id": "2109.14688v1",
    "submission_id": "xB4lGVLvXDz",
    "submission_title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
    "review_id": "V8CGbQ_zAdd",
    "input": {
      "title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Although their idea of constructing a function in the RKHS is not new, the usage of this construction in GAN-type objective for KL estimation seems to be novel.\n- The submission is technically sound and most of the claims are well supported.\n- Theoretical results are solid and informative.\n- Experimental results are reasonable and show the improvements of the proposed method over previous ones.\n- In Abstract and line 40, they claim that methods with a neural-net based discriminator suffer from unreliability or instability.\n- However, they do not include the two methods, VDM and MINE, cited in the paper in the experiments. I would like to see results that can support the claim.\n- In Theorem 2, the quantities $C_p$, $S_p$, $\\mathscr{L}_p$ are not discussed.\n- How are they related to the architecture of $\\phi_\\theta$ and $\\gamma? Are they always finite?\n- The meaning of $\\text{sup}_{K_\\theta}$ is unclear (e.g., in Theorem 2).\n- Does that include all possible architectures of $\\phi_\\theta$ and all values of $\\gamma? If you maximize over $\\gamma$, then the finiteness in the above comment may be an issue.\n- Since what you are advertising is the regularized version of the estimator, it would be better to include theoretical analysis for the consistency of this regularized estimator.\n- Spectral normalization is used in the proposed discriminator but not in the baseline.\n- This seems to be unfair. Do you have an explanation about it?\n- The paper is clearly written and easy to follow.\n- The results are likely interesting and useful to the community.\n- The idea of constructing a function in RKHS and controlling its complexity could be used in GANs in general.\n- Though the authors provide a theoretical guarantee, the analysis is mainly limited to finite-dimensional setting.\n- The authors could benefit from more discussion on the practical implications of their results and how they can be applied to real-world problems.\n- The paper is mostly well-written, but it can be improved by providing more concrete examples of how their method can be used in practice.\n- The theoretical results are mostly well-supported, but the authors could provide more intuition behind their proofs.\n- The paper lacks a clear explanation of what the authors mean by 'KL estimation in GANs' and 'GAN-type objective for KL estimation'.\n- The authors could benefit from more discussion on the limitations of their approach and potential avenues for future research.\n- The paper is mostly easy to follow, but some of the notation and terminology could be clarified or rephrased for better comprehension.\n- The results are mostly well-supported, but the authors could provide more details on the experiments, such as the choice of architectures, hyperparameters, and training procedures.\n- The authors could benefit from more discussion on the potential applications of their results beyond GANs.",
    "review_points_list": [
      "Although their idea of constructing a function in the RKHS is not new, the usage of this construction in GAN-type objective for KL estimation seems to be novel.",
      "The submission is technically sound and most of the claims are well supported.",
      "Theoretical results are solid and informative.",
      "Experimental results are reasonable and show the improvements of the proposed method over previous ones.",
      "In Abstract and line 40, they claim that methods with a neural-net based discriminator suffer from unreliability or instability.",
      "However, they do not include the two methods, VDM and MINE, cited in the paper in the experiments. I would like to see results that can support the claim.",
      "In Theorem 2, the quantities $C_p$, $S_p$, $\\mathscr{L}_p$ are not discussed.",
      "How are they related to the architecture of $\\phi_\\theta$ and $\\gamma? Are they always finite?",
      "The meaning of $\\text{sup}_{K_\\theta}$ is unclear (e.g., in Theorem 2).",
      "Does that include all possible architectures of $\\phi_\\theta$ and all values of $\\gamma? If you maximize over $\\gamma$, then the finiteness in the above comment may be an issue.",
      "Since what you are advertising is the regularized version of the estimator, it would be better to include theoretical analysis for the consistency of this regularized estimator.",
      "Spectral normalization is used in the proposed discriminator but not in the baseline.",
      "This seems to be unfair. Do you have an explanation about it?",
      "The paper is clearly written and easy to follow.",
      "The results are likely interesting and useful to the community.",
      "The idea of constructing a function in RKHS and controlling its complexity could be used in GANs in general.",
      "Though the authors provide a theoretical guarantee, the analysis is mainly limited to finite-dimensional setting.",
      "The authors could benefit from more discussion on the practical implications of their results and how they can be applied to real-world problems.",
      "The paper is mostly well-written, but it can be improved by providing more concrete examples of how their method can be used in practice.",
      "The theoretical results are mostly well-supported, but the authors could provide more intuition behind their proofs.",
      "The paper lacks a clear explanation of what the authors mean by 'KL estimation in GANs' and 'GAN-type objective for KL estimation'.",
      "The authors could benefit from more discussion on the limitations of their approach and potential avenues for future research.",
      "The paper is mostly easy to follow, but some of the notation and terminology could be clarified or rephrased for better comprehension.",
      "The results are mostly well-supported, but the authors could provide more details on the experiments, such as the choice of architectures, hyperparameters, and training procedures.",
      "The authors could benefit from more discussion on the potential applications of their results beyond GANs."
    ]
  },
  {
    "paper_id": "1909.13035v3",
    "submission_id": "wHxnK7Ucogy",
    "submission_title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
    "review_id": "G9wFNO5ryAh",
    "input": {
      "title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The idea of using a combination of implicit and explicit models for joint training has been explored before in several papers recently (as the authors discuss in the detailed literature review section) and is an interesting direction of research.\n- Furthermore, the proposal to use mutual regularization of the two models to train such models is intriguing and novel.\n- Additionally, the experiments presented in the paper provide promising results and improvements.\n- The motivation behind the joint training of implicit and explicit model itself is unclear\n- The joint training strategy introduces an expensive overhead of training two rather costly models to overcome the drawbacks of the individual models\n- Training the two models does not directly address the drawbacks present in the individual models, but makes things better for the models indirectly\n- The incorporation of the loss term between the implicit and the explicit model does not provide any additional information during training apart from the data and the other two terms\n- The results presented may not hold for other metrics apart from Wasserstein or Jenson-Shanon\n- The authors should provide more information on which of the models is finally used after training\n- The methodology presented may be too costly for training an energy-based model, and a comparison with light-weight methods would be beneficial\n- The paper should include experiments on more complicated datasets to evaluate the scalability of the proposed model\n- There is a worry about the costly procedure of training two models jointly and how well this will scale to higher dimensions and more complicated datasets\n- The proposed method is interesting as it provides a new direction for addressing the problem of data imbalance.\n- The experimental results are also impressive, with the proposed method showing significant improvements over other methods.\n- However, there are some concerns about the methodology used.\n- The proposed method addresses a long-standing problem in the field and provides a new direction for research.\n- The authors provide clear explanations of the methodology, making it easy to understand.\n- The experimental results show significant improvements over other methods.\n- The authors also provide a clear comparison between their method and other existing methods, demonstrating the effectiveness of their approach.\n- The evaluation metrics used are quite traditional and may not fully capture the complexity of the problem.\n- There is a lack of discussion on the limitations of the proposed method and how it can be improved in the future.\n- The proposed method may not be generalizable to other domains or datasets, as the authors have not tested their approach on a diverse set of data.\n- It is unclear how the algorithm handles noisy or missing data, which is a common occurrence in real-world data.\n- The authors should provide more information on the computation resources required to run the proposed method, as this could be an important consideration for users.\n- The method of using a combination of explicit and implicit models is a creative approach to solve the problem.\n- The authors show that their method can improve over other methods in several cases.\n- However, the evaluation metric used in the paper may not be the best for this problem\n- There are some concerns about the robustness of the proposed method to the quality of the data\n- The authors could provide more information on how to handle noisy or missing data in their approach\n- Another potential concern is that the authors assume that the quality of the data is known, but this may not always be the case in real-world applications\n- The authors could also provide more details on how to handle data imbalance in their approach\n- The results are promising, but the authors should provide more information on the computational resources required to run their method\n- The proposed method is interesting, but it may not be generalizable to other domains or datasets\n- The authors should provide more information on how the algorithm handles noisy or missing data in their approach\n- More comparisons with other existing methods would be helpful to understand the full potential of the proposed approach\n- The authors propose an innovative and efficient approach for solving the problem of data imbalance.\n- However, there are some concerns about the methodology used.\n- The proposed method is innovative and efficiently solves the problem of data imbalance\n- The authors provide clear explanations of the methodology, making it easy to understand.\n- The experimental results show significant improvements over other methods.\n- The evaluation metric used in the paper may not be the best for this problem\n- There is a lack of discussion on the limitations of the proposed method and how it can be improved in the future.\n- The proposed method may not be generalizable to other domains or datasets, as the authors have not tested their approach on a diverse set of data.\n- It is unclear how the algorithm handles noisy or missing data, which is a common occurrence in real-world data.",
    "review_points_list": [
      "The idea of using a combination of implicit and explicit models for joint training has been explored before in several papers recently (as the authors discuss in the detailed literature review section) and is an interesting direction of research.",
      "Furthermore, the proposal to use mutual regularization of the two models to train such models is intriguing and novel.",
      "Additionally, the experiments presented in the paper provide promising results and improvements.",
      "The motivation behind the joint training of implicit and explicit model itself is unclear",
      "The joint training strategy introduces an expensive overhead of training two rather costly models to overcome the drawbacks of the individual models",
      "Training the two models does not directly address the drawbacks present in the individual models, but makes things better for the models indirectly",
      "The incorporation of the loss term between the implicit and the explicit model does not provide any additional information during training apart from the data and the other two terms",
      "The results presented may not hold for other metrics apart from Wasserstein or Jenson-Shanon",
      "The authors should provide more information on which of the models is finally used after training",
      "The methodology presented may be too costly for training an energy-based model, and a comparison with light-weight methods would be beneficial",
      "The paper should include experiments on more complicated datasets to evaluate the scalability of the proposed model",
      "There is a worry about the costly procedure of training two models jointly and how well this will scale to higher dimensions and more complicated datasets",
      "The proposed method is interesting as it provides a new direction for addressing the problem of data imbalance.",
      "The experimental results are also impressive, with the proposed method showing significant improvements over other methods.",
      "However, there are some concerns about the methodology used.",
      "The proposed method addresses a long-standing problem in the field and provides a new direction for research.",
      "The authors provide clear explanations of the methodology, making it easy to understand.",
      "The experimental results show significant improvements over other methods.",
      "The authors also provide a clear comparison between their method and other existing methods, demonstrating the effectiveness of their approach.",
      "The evaluation metrics used are quite traditional and may not fully capture the complexity of the problem.",
      "There is a lack of discussion on the limitations of the proposed method and how it can be improved in the future.",
      "The proposed method may not be generalizable to other domains or datasets, as the authors have not tested their approach on a diverse set of data.",
      "It is unclear how the algorithm handles noisy or missing data, which is a common occurrence in real-world data.",
      "The authors should provide more information on the computation resources required to run the proposed method, as this could be an important consideration for users.",
      "The method of using a combination of explicit and implicit models is a creative approach to solve the problem.",
      "The authors show that their method can improve over other methods in several cases.",
      "However, the evaluation metric used in the paper may not be the best for this problem",
      "There are some concerns about the robustness of the proposed method to the quality of the data",
      "The authors could provide more information on how to handle noisy or missing data in their approach",
      "Another potential concern is that the authors assume that the quality of the data is known, but this may not always be the case in real-world applications",
      "The authors could also provide more details on how to handle data imbalance in their approach",
      "The results are promising, but the authors should provide more information on the computational resources required to run their method",
      "The proposed method is interesting, but it may not be generalizable to other domains or datasets",
      "The authors should provide more information on how the algorithm handles noisy or missing data in their approach",
      "More comparisons with other existing methods would be helpful to understand the full potential of the proposed approach",
      "The authors propose an innovative and efficient approach for solving the problem of data imbalance.",
      "However, there are some concerns about the methodology used.",
      "The proposed method is innovative and efficiently solves the problem of data imbalance",
      "The authors provide clear explanations of the methodology, making it easy to understand.",
      "The experimental results show significant improvements over other methods.",
      "The evaluation metric used in the paper may not be the best for this problem",
      "There is a lack of discussion on the limitations of the proposed method and how it can be improved in the future.",
      "The proposed method may not be generalizable to other domains or datasets, as the authors have not tested their approach on a diverse set of data.",
      "It is unclear how the algorithm handles noisy or missing data, which is a common occurrence in real-world data."
    ]
  },
  {
    "paper_id": "1909.13035v3",
    "submission_id": "wHxnK7Ucogy",
    "submission_title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
    "review_id": "IUQTiRdMIA4",
    "input": {
      "title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This work seeks to combine energy modeling and implicit generative modeling to improve the learning of probabilistic models.\n- This direction has been extensively explored by various researchers in the past two years, with varying degrees of success.\n- The insight that the proposed formulation indeed implements Moreau-Yosida regularization, my major criticism is that there are too many moving parts in this model and thus making it overly complicated.\n- It feels like gluing the components together, and such bridging is not particularly meaningful.\n- More discussion is needed to explain why this work is significant compared to prior arts.\n- Also, while the author(s) have claimed that the integrated solution overcomes the individual limits of each component, it can totally go to the other extreme, with the interacting components amplifying each other's deficiencies.\n- No experiments are designed to verify this would not be the case (i.e., intentionally testing for the robustness).\n- And there is an established consensus that working with Stein discrepancy is quite tricky.\n- It is computationally intensive and scales poorly to data dimensionality and complexity (due to its kernel formulation and dependency on gradient computation).\n- Since the author(s) are quite vague on the actual implementation of Stein discrepancy in this paper (for general Stein discrepancy, what is $\\\\Gamma$, and for kernel Stein discrepancy, what kernel is used?), I am unable to properly evaluate the effectiveness of the proposed Stein Bridging.\n- The techniques used in this work are very well known.\n- The main novelty is to promote the consensus of model distribution and data distribution via minimizing the Stein discrepancy relative to the same energy model.\n- I am not sure this is the right thing to do, as the consensus can be a bad one.\n- Writing needs to be improved, many statements are not proper English.\n- For clarity, details on Stein discrepancy, its implementation, and the reasoning why the proposed methods should function as expected are missing in the current presentation.\n- I am not sure about the technical correctness of the theories.\n- This work is unlikely to impact the community in a significant way.\n- The solution is overly complicated and the theories are neither appealing to theorists nor practitioners.",
    "review_points_list": [
      "This work seeks to combine energy modeling and implicit generative modeling to improve the learning of probabilistic models.",
      "This direction has been extensively explored by various researchers in the past two years, with varying degrees of success.",
      "The insight that the proposed formulation indeed implements Moreau-Yosida regularization, my major criticism is that there are too many moving parts in this model and thus making it overly complicated.",
      "It feels like gluing the components together, and such bridging is not particularly meaningful.",
      "More discussion is needed to explain why this work is significant compared to prior arts.",
      "Also, while the author(s) have claimed that the integrated solution overcomes the individual limits of each component, it can totally go to the other extreme, with the interacting components amplifying each other's deficiencies.",
      "No experiments are designed to verify this would not be the case (i.e., intentionally testing for the robustness).",
      "And there is an established consensus that working with Stein discrepancy is quite tricky.",
      "It is computationally intensive and scales poorly to data dimensionality and complexity (due to its kernel formulation and dependency on gradient computation).",
      "Since the author(s) are quite vague on the actual implementation of Stein discrepancy in this paper (for general Stein discrepancy, what is $\\\\Gamma$, and for kernel Stein discrepancy, what kernel is used?), I am unable to properly evaluate the effectiveness of the proposed Stein Bridging.",
      "The techniques used in this work are very well known.",
      "The main novelty is to promote the consensus of model distribution and data distribution via minimizing the Stein discrepancy relative to the same energy model.",
      "I am not sure this is the right thing to do, as the consensus can be a bad one.",
      "Writing needs to be improved, many statements are not proper English.",
      "For clarity, details on Stein discrepancy, its implementation, and the reasoning why the proposed methods should function as expected are missing in the current presentation.",
      "I am not sure about the technical correctness of the theories.",
      "This work is unlikely to impact the community in a significant way.",
      "The solution is overly complicated and the theories are neither appealing to theorists nor practitioners."
    ]
  },
  {
    "paper_id": "1909.13035v3",
    "submission_id": "wHxnK7Ucogy",
    "submission_title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
    "review_id": "QB9IlEESCNZ",
    "input": {
      "title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- It seems to be an interesting and novel work.\n- The authors address the main differences with the previous works in the appendix.\n- I would like to know the specific applications of this work in real-world scenarios.\n- The authors did not mention some of the related works in their literature review.\n- I think the authors should clarify the assumptions used in the theoretical analysis.\n- Is there any experimental result in the paper that shows the superiority of the proposed approach?\n- I did not understand the proof of the main theorem.\n- The paper lacks some intuitive explanations of the algorithms used.\n- Is there any potential future work in this area?\n- I would like to see more comparisons with existing works.",
    "review_points_list": [
      "It seems to be an interesting and novel work.",
      "The authors address the main differences with the previous works in the appendix.",
      "I would like to know the specific applications of this work in real-world scenarios.",
      "The authors did not mention some of the related works in their literature review.",
      "I think the authors should clarify the assumptions used in the theoretical analysis.",
      "Is there any experimental result in the paper that shows the superiority of the proposed approach?",
      "I did not understand the proof of the main theorem.",
      "The paper lacks some intuitive explanations of the algorithms used.",
      "Is there any potential future work in this area?",
      "I would like to see more comparisons with existing works."
    ]
  },
  {
    "paper_id": "1909.13035v3",
    "submission_id": "wHxnK7Ucogy",
    "submission_title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
    "review_id": "295HlGJu5s",
    "input": {
      "title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I found this work enjoyable.\n- The idea of learning a deep energy model via the Stein discrepancy has been proposed before, but the addition of an implicit generative model really seems to help distribution fit.\n- This is possibly due to the increased size of the generative model function space.\n- [1, 2] consider the space of L2 functions, which need to vanish at the boundaries -- necessitating potentially harmful regularization on the generator function.\n- This method avoids this issue by training an implicit model, that is known to perform well for sample generation.\n- Figure 4 and figure 5 were quite convincing to me.\n- They show that both the implicit model and the energy model are able to capture the data distribution well, while their components trained independently more or less fail.\n- On a practical note, I believe it's clear from these toy examples that the learned distribution is more faithful with this joint training algorithm.\n- But what is less clear is the advantage in high(er) dimensional inference tasks.\n- The MNIST and CIFAR-10 results do not look much better than WGAN-GP.\n- I see that the inception score is better for Joint-W, but the 2X training time cost is high given this margin.\n- As an aside, I found the fact that minimizing the Stein discrepancy between the two models acts as a gradient penalty on the implicit model interesting.\n- This might make sense since the energy model is 'slower' than WGAN.\n- After reading the other reviews/discussions, I agree that higher-dimensional inference tasks are important to evaluate on for this method.\n- I do not intend to change my score, but I hope that the authors can show the applicability of their approach to tasks other than MNIST and CIFAR-10.",
    "review_points_list": [
      "I found this work enjoyable.",
      "The idea of learning a deep energy model via the Stein discrepancy has been proposed before, but the addition of an implicit generative model really seems to help distribution fit.",
      "This is possibly due to the increased size of the generative model function space.",
      "[1, 2] consider the space of L2 functions, which need to vanish at the boundaries -- necessitating potentially harmful regularization on the generator function.",
      "This method avoids this issue by training an implicit model, that is known to perform well for sample generation.",
      "Figure 4 and figure 5 were quite convincing to me.",
      "They show that both the implicit model and the energy model are able to capture the data distribution well, while their components trained independently more or less fail.",
      "On a practical note, I believe it's clear from these toy examples that the learned distribution is more faithful with this joint training algorithm.",
      "But what is less clear is the advantage in high(er) dimensional inference tasks.",
      "The MNIST and CIFAR-10 results do not look much better than WGAN-GP.",
      "I see that the inception score is better for Joint-W, but the 2X training time cost is high given this margin.",
      "As an aside, I found the fact that minimizing the Stein discrepancy between the two models acts as a gradient penalty on the implicit model interesting.",
      "This might make sense since the energy model is 'slower' than WGAN.",
      "After reading the other reviews/discussions, I agree that higher-dimensional inference tasks are important to evaluate on for this method.",
      "I do not intend to change my score, but I hope that the authors can show the applicability of their approach to tasks other than MNIST and CIFAR-10."
    ]
  },
  {
    "paper_id": "2106.05445v2",
    "submission_id": "zJynVlnoObx",
    "submission_title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
    "review_id": "nCMbRif0Cv",
    "input": {
      "title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is really clear.\n- I really liked the idea of exploiting this local superlinear convergence of the QN methods successively, by having $S_1 \rightarrow S_2 \rightarrow ...$.\n- The paper explains carefully its concepts, its algorithm implementation compares its cost against AdaNewton and then provides an easy to follow convergence analysis.\n- My first main concern (that I will develop below) is that the condition on $m_0$ that is $m_0 = \text{O}(\text{k}^2 s\text{log} d)$ is to my opinion completely undermined by the authors.\n- My second main concern is the experimental part wish I find unclear and unfair.\n- Style/typo comments: - 1) lines 37 and 74/75: not consistent writing of Quasi-Newton / quasi-Newton - 2) line 48, not nice looking line because of \\emph - 3) line 77: missing word before reference [20] - 4) line 106: ERM already defined earlier - 5) line 111: $w^*$ is an optimal solution of the expected risk minimization.\n- Minor comments: - 7) line 77: What did you mean when saying \"To exploit quasi-newton methods for the described adaptive sample scheme, however ...\" ? To be corrected I think - 8) line 7 of Algo. 1: a break-line for $y$ would make it way easier to read - 9) $\\mu$ is not really standard for a regularization parameter, I would prefer $\\lambda$ then say $\\mu \\approx \\lambda$ - 9 bis) line 373: cite maybe the most recent version of Bottou and Bousquet 2011\n- Major comments: - 10) A long comparison is made against AdaNewton, yet very few comparison is made against other (stochastic) QN methods like L-BFGS, which is known to perform very well in a lost of case, or the more recent K-BFGS (\"Practical quasi-newton methods for training deep neural networks\", Goldfarb, Ren, Bahamou, 2020) or Stochastic Block BFGS (Stochastic Block BFGS: Squeezing More Curvature out of Data, Gower, Goldfarb, Richtarik, 2016)\n- My main concern (that I will develop below) is that the condition on $m_0$ that is $m_0 = \text{O}(\text{k}^2 s\text{log} d)$ is to my opinion completely undermined by the authors.\n- I would have liked a clear explanation about what happens when we are facing a very ill-conditioned problem such that $\\kappa = N$ or even worse  $\\kappa >> N$.\n- What does AdaQN become in such a case ? Does it become QN with a constant sample size $m\\equiv N$ ? In this case isn't it way more costly than 1st order methods ? What about numerical experiments in this case ?\n- In Prop. 1 and elsewhere in the paper, you clearly explain that having $S_1 \\rightarrow S_2 \\rightarrow ...$ is necessary for AdaQN to work.\n- I have been through the proof and I was expecting that you would have to deal with a problem : at each outer loop iteration the points are \\textbf{not} sampled independently.\n- How is that it does not appear in your proof ?\n- Remark 4: Don't you omit the matrix inversions and the matrix vector product required by AdaQN when comparing it to Katyusha ?\n- In other words, is this comparison of gradient complexity really fair between a first and a second order method ?\n- How can you get a condition number which equals 1 in your experiments ? Normalizing can improve the condition number, but I have never seen such a bound related to just simple normalization.\n- It is not clear what is monitored by \"time\" in your experiments.\n- So I have been into the code and I noticed that the initialization which requires inverting the first hessian is not taken into account... In addition, SAGA is not initialized with a full gradient step, plus SAGA algorithm was launched with a decreasing step size which is unnecessary.\n- This is one of the main advantage of VR methods.\n- This is wat I really doubt about the fairness of the experiments.\n- In all your experiments $N$ is at most of medium size, you should have tried on RCV1.binary for instance $N = 600K$ to see if first order methods are still performing poorly compared to AdaQN.\n- Regularization parameters in table 2. look way to large... It is not representing the range of lambda used in practice.\n- I would recommend $10^{-4}$ or $10^{-6}$. Moreover we could maybe see this way an experiment with large $\\kappa$\n- Why is AdaQN adaptive since you basically double the sample size at each iteration, so it's not dependent on anything else than the current number of samples m ?\n- Why didn't you include L-BFGS in your experiments ?\n- With is your initial point set to 1 in your experiments ?\n- Main concerns I had about experimental settings, I think have been addressed.\n- I think the authors must detail the experimental settings in the appendix such that the community can understand their numerical experiments and really be convinced that the settings are fair.\n- Despite the discussion on the practical \"leveraging of superlinearity\", I finally decide to **increase my score** but only up **to 6**.\n- In his updated review, reiewer RQBF wrote : [...]\n- I agree with him on this point (and I think the authors too). Yet on my side, this discussion does not diminish the ideas nor the quality of the paper.\n- Moreover, as I said above, my concerns about numerical experiments will be addressed in the final version of the paper (with fairer comparisons).\n- My final thoughts about the paper are that despite this concern about superlinarity, it can be very profitable for the QN community and optimizers interested in adaptive mini-batch/sample size.\n- Hence, I increase my score to 7 and argue in favor of accepting this paper with one condition : a clear and important remark about this discussion on superlinearity must be added and highlighted.",
    "review_points_list": [
      "The paper is really clear.",
      "I really liked the idea of exploiting this local superlinear convergence of the QN methods successively, by having $S_1 \rightarrow S_2 \rightarrow ...$.",
      "The paper explains carefully its concepts, its algorithm implementation compares its cost against AdaNewton and then provides an easy to follow convergence analysis.",
      "My first main concern (that I will develop below) is that the condition on $m_0$ that is $m_0 = \text{O}(\text{k}^2 s\text{log} d)$ is to my opinion completely undermined by the authors.",
      "My second main concern is the experimental part wish I find unclear and unfair.",
      "Style/typo comments: - 1) lines 37 and 74/75: not consistent writing of Quasi-Newton / quasi-Newton - 2) line 48, not nice looking line because of \\emph - 3) line 77: missing word before reference [20] - 4) line 106: ERM already defined earlier - 5) line 111: $w^*$ is an optimal solution of the expected risk minimization.",
      "Minor comments: - 7) line 77: What did you mean when saying \"To exploit quasi-newton methods for the described adaptive sample scheme, however ...\" ? To be corrected I think - 8) line 7 of Algo. 1: a break-line for $y$ would make it way easier to read - 9) $\\mu$ is not really standard for a regularization parameter, I would prefer $\\lambda$ then say $\\mu \\approx \\lambda$ - 9 bis) line 373: cite maybe the most recent version of Bottou and Bousquet 2011",
      "Major comments: - 10) A long comparison is made against AdaNewton, yet very few comparison is made against other (stochastic) QN methods like L-BFGS, which is known to perform very well in a lost of case, or the more recent K-BFGS (\"Practical quasi-newton methods for training deep neural networks\", Goldfarb, Ren, Bahamou, 2020) or Stochastic Block BFGS (Stochastic Block BFGS: Squeezing More Curvature out of Data, Gower, Goldfarb, Richtarik, 2016)",
      "My main concern (that I will develop below) is that the condition on $m_0$ that is $m_0 = \text{O}(\text{k}^2 s\text{log} d)$ is to my opinion completely undermined by the authors.",
      "I would have liked a clear explanation about what happens when we are facing a very ill-conditioned problem such that $\\kappa = N$ or even worse  $\\kappa >> N$.",
      "What does AdaQN become in such a case ? Does it become QN with a constant sample size $m\\equiv N$ ? In this case isn't it way more costly than 1st order methods ? What about numerical experiments in this case ?",
      "In Prop. 1 and elsewhere in the paper, you clearly explain that having $S_1 \\rightarrow S_2 \\rightarrow ...$ is necessary for AdaQN to work.",
      "I have been through the proof and I was expecting that you would have to deal with a problem : at each outer loop iteration the points are \\textbf{not} sampled independently.",
      "How is that it does not appear in your proof ?",
      "Remark 4: Don't you omit the matrix inversions and the matrix vector product required by AdaQN when comparing it to Katyusha ?",
      "In other words, is this comparison of gradient complexity really fair between a first and a second order method ?",
      "How can you get a condition number which equals 1 in your experiments ? Normalizing can improve the condition number, but I have never seen such a bound related to just simple normalization.",
      "It is not clear what is monitored by \"time\" in your experiments.",
      "So I have been into the code and I noticed that the initialization which requires inverting the first hessian is not taken into account... In addition, SAGA is not initialized with a full gradient step, plus SAGA algorithm was launched with a decreasing step size which is unnecessary.",
      "This is one of the main advantage of VR methods.",
      "This is wat I really doubt about the fairness of the experiments.",
      "In all your experiments $N$ is at most of medium size, you should have tried on RCV1.binary for instance $N = 600K$ to see if first order methods are still performing poorly compared to AdaQN.",
      "Regularization parameters in table 2. look way to large... It is not representing the range of lambda used in practice.",
      "I would recommend $10^{-4}$ or $10^{-6}$. Moreover we could maybe see this way an experiment with large $\\kappa$",
      "Why is AdaQN adaptive since you basically double the sample size at each iteration, so it's not dependent on anything else than the current number of samples m ?",
      "Why didn't you include L-BFGS in your experiments ?",
      "With is your initial point set to 1 in your experiments ?",
      "Main concerns I had about experimental settings, I think have been addressed.",
      "I think the authors must detail the experimental settings in the appendix such that the community can understand their numerical experiments and really be convinced that the settings are fair.",
      "Despite the discussion on the practical \"leveraging of superlinearity\", I finally decide to **increase my score** but only up **to 6**.",
      "In his updated review, reiewer RQBF wrote : [...]",
      "I agree with him on this point (and I think the authors too). Yet on my side, this discussion does not diminish the ideas nor the quality of the paper.",
      "Moreover, as I said above, my concerns about numerical experiments will be addressed in the final version of the paper (with fairer comparisons).",
      "My final thoughts about the paper are that despite this concern about superlinarity, it can be very profitable for the QN community and optimizers interested in adaptive mini-batch/sample size.",
      "Hence, I increase my score to 7 and argue in favor of accepting this paper with one condition : a clear and important remark about this discussion on superlinearity must be added and highlighted."
    ]
  },
  {
    "paper_id": "2106.05445v2",
    "submission_id": "zJynVlnoObx",
    "submission_title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
    "review_id": "RX6ABfoopZK",
    "input": {
      "title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I consider this work gives an effective QN method through interesting intuition using adaptive sample size.\n- Avoiding adopting adaptive sample size arises in previous works [1], but not for Quasi-Newton methods, and it indeed provides convincing results.\n- Therefore, I still approve of the originality of this work.\n- Moreover, the work is well-written and has high quality and clarity.\n- The exploitation of the superlinear convergence of quasi-Newton methods globally is an important problem in my view.\n- As far as I know, previous results build on a small local neighborhood of the optimal solution as the authors mentioned.\n- Therefore, the significance of this work is undoubted.",
    "review_points_list": [
      "I consider this work gives an effective QN method through interesting intuition using adaptive sample size.",
      "Avoiding adopting adaptive sample size arises in previous works [1], but not for Quasi-Newton methods, and it indeed provides convincing results.",
      "Therefore, I still approve of the originality of this work.",
      "Moreover, the work is well-written and has high quality and clarity.",
      "The exploitation of the superlinear convergence of quasi-Newton methods globally is an important problem in my view.",
      "As far as I know, previous results build on a small local neighborhood of the optimal solution as the authors mentioned.",
      "Therefore, the significance of this work is undoubted."
    ]
  },
  {
    "paper_id": "2106.05445v2",
    "submission_id": "zJynVlnoObx",
    "submission_title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
    "review_id": "-KtAxVzPy0b",
    "input": {
      "title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Thank you to the authors for the response and the following discussion.\n- The authors have clarified that superlinearity of BFGS is not needed to establish their results, and they promised to clarify this in the paper, which is good.\n- They also promised to include in the future version of the paper a discussion and empirical results comparing to the prior work of [DMKVW18], which is appreciated.\n- In my view, a significant part of the technical novelty of the approach rested on the apparent reliance on the superlinear nature of the BFGS updates, but after a discussion with the authors I concluded that the BFGS Hessian updates have basically no effect on the complexity analysis.\n- This weakens the paper in my view.\n- Thus, all in all, I maintain my score.\n- First, there is one important related work that is missing from the literature review, namely [DMKVW18].\n- In fact, that paper is to my knowledge the first to propose using adaptive sample sizes with a quasi-Newton method (in their case, L-BFGS).\n- However, it does not provide a convergence analysis that is specific to any particular quasi-Newton method, which is the main contribution of the present submission.\n- Instead, they show that if for each sample size an optimizer exhibits a convergence rate of at least, say, (1/2)^t, then one should double the sample size at each stage after a constant number of iterations (same as in the present submission).\n- Thus, in principle, one could combine that result with the recent work on local convergence rates of BFGS [21-23] to establish a guarantee analogous to the one obtained by the authors.\n- Of course, this is by no means straightforward, but it should at least be addressed in the submission.\n- I would also be interested to see an empirical comparison of AdaQN with the BET method proposed by [DMKVW18] (ran with L-BFGS), since that can be considered as the closest baseline from the literature.\n- Second, I take issue with the claim that the proposed approach \u2018exploits the superlinear convergence\u2019 of quasi-Newton methods.\n- As the authors discuss, the superlinear convergence of BFGS takes the form of roughly (1/t)^t, where t is the number of iterations performed in one stage of Algorithm 1.\n- However, since the algorithm performs only 3 iterations per stage, then it has no opportunity to exploit superlinear convergence, and all you actually obtain is linear convergence that is independent of the condition number.\n- In fact, the proposed analysis demonstrates that the superlinear convergence doesn\u2019t really matter here, because we reach statistical accuracy before it kicks in.\n- So, I think the authors should not say that the algorithm is \u2018exploiting the superlinear convergence\u2019, but it is ok to say that the algorithm is \u2018exploiting local convergence\u2019, because that is still very much accurate.\n- The above discussion about the lack of true superlinear convergence highlights another issue with the algorithm, namely, with the way that the Hessian estimate is being updated.",
    "review_points_list": [
      "Thank you to the authors for the response and the following discussion.",
      "The authors have clarified that superlinearity of BFGS is not needed to establish their results, and they promised to clarify this in the paper, which is good.",
      "They also promised to include in the future version of the paper a discussion and empirical results comparing to the prior work of [DMKVW18], which is appreciated.",
      "In my view, a significant part of the technical novelty of the approach rested on the apparent reliance on the superlinear nature of the BFGS updates, but after a discussion with the authors I concluded that the BFGS Hessian updates have basically no effect on the complexity analysis.",
      "This weakens the paper in my view.",
      "Thus, all in all, I maintain my score.",
      "First, there is one important related work that is missing from the literature review, namely [DMKVW18].",
      "In fact, that paper is to my knowledge the first to propose using adaptive sample sizes with a quasi-Newton method (in their case, L-BFGS).",
      "However, it does not provide a convergence analysis that is specific to any particular quasi-Newton method, which is the main contribution of the present submission.",
      "Instead, they show that if for each sample size an optimizer exhibits a convergence rate of at least, say, (1/2)^t, then one should double the sample size at each stage after a constant number of iterations (same as in the present submission).",
      "Thus, in principle, one could combine that result with the recent work on local convergence rates of BFGS [21-23] to establish a guarantee analogous to the one obtained by the authors.",
      "Of course, this is by no means straightforward, but it should at least be addressed in the submission.",
      "I would also be interested to see an empirical comparison of AdaQN with the BET method proposed by [DMKVW18] (ran with L-BFGS), since that can be considered as the closest baseline from the literature.",
      "Second, I take issue with the claim that the proposed approach \u2018exploits the superlinear convergence\u2019 of quasi-Newton methods.",
      "As the authors discuss, the superlinear convergence of BFGS takes the form of roughly (1/t)^t, where t is the number of iterations performed in one stage of Algorithm 1.",
      "However, since the algorithm performs only 3 iterations per stage, then it has no opportunity to exploit superlinear convergence, and all you actually obtain is linear convergence that is independent of the condition number.",
      "In fact, the proposed analysis demonstrates that the superlinear convergence doesn\u2019t really matter here, because we reach statistical accuracy before it kicks in.",
      "So, I think the authors should not say that the algorithm is \u2018exploiting the superlinear convergence\u2019, but it is ok to say that the algorithm is \u2018exploiting local convergence\u2019, because that is still very much accurate.",
      "The above discussion about the lack of true superlinear convergence highlights another issue with the algorithm, namely, with the way that the Hessian estimate is being updated."
    ]
  },
  {
    "paper_id": "2106.09019v2",
    "submission_id": "wdIDt--oLmV",
    "submission_title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate",
    "review_id": "8zQ669zPfG",
    "input": {
      "title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Checklist should start on a separate page after the references.\n- I think the paper outlines a novel approach to a significant problem, provides thorough details on the method and relevant experiments, and has strong clarity in the writing and presentation.\n- The paper tackles a new and relevant problem with a novel deep learning based method that involves end-to-end training of a differential surrogate model and a design generator.\n- The approach is novel approach to synthesis problems the authors describe, which are a new set of challenges for deep learning methods.\n- The authors provide mostly thorough details on their method, formulation and experimental test cases.\n- In order to improve the score, I would to get more clarity on the following questions:\n- How does the 'sliding window' work to generate an extruder path? Since the is sequential and time dependent, does the 'encoder' produce the full directly or does it produce it sequentially? If is sequentially generated, how does the input change over time?\n- What are the size of the design spaces for both cases? This would help get a sense of the scale of problems the paper tackled.\n- It would also be useful to have a discussion on how the current approach could scale to larger and more complex challenges.\n- Did you do an ablation of different loss functions? You discuss the importance of the Chamfer distance for the extrusion case, which is helpful, and I would be interested to learn more about how potentially naive loss function affect performance.\n- I assume they would make things worse, but it would be helpful to get more detail on that.\n- A discussion on future work/directions of the work would be nice.\n- The paper is well written and presents relevant equations, formulations and diagrams to understand most of the relevant elements of the study.\n- As mentioned above, to further improve on the clarity it would be helpful to get more detail on the input & output setups and relevant dimensions of the design cases, especially the outputs.\n- There are more details on the inputs in the appendix, but the outputs are still somewhat unclear.\n- The paper addresses the application of a novel deep learning method to a non-traditional challenges that is significant.",
    "review_points_list": [
      "Checklist should start on a separate page after the references.",
      "I think the paper outlines a novel approach to a significant problem, provides thorough details on the method and relevant experiments, and has strong clarity in the writing and presentation.",
      "The paper tackles a new and relevant problem with a novel deep learning based method that involves end-to-end training of a differential surrogate model and a design generator.",
      "The approach is novel approach to synthesis problems the authors describe, which are a new set of challenges for deep learning methods.",
      "The authors provide mostly thorough details on their method, formulation and experimental test cases.",
      "In order to improve the score, I would to get more clarity on the following questions:",
      "How does the 'sliding window' work to generate an extruder path? Since the is sequential and time dependent, does the 'encoder' produce the full directly or does it produce it sequentially? If is sequentially generated, how does the input change over time?",
      "What are the size of the design spaces for both cases? This would help get a sense of the scale of problems the paper tackled.",
      "It would also be useful to have a discussion on how the current approach could scale to larger and more complex challenges.",
      "Did you do an ablation of different loss functions? You discuss the importance of the Chamfer distance for the extrusion case, which is helpful, and I would be interested to learn more about how potentially naive loss function affect performance.",
      "I assume they would make things worse, but it would be helpful to get more detail on that.",
      "A discussion on future work/directions of the work would be nice.",
      "The paper is well written and presents relevant equations, formulations and diagrams to understand most of the relevant elements of the study.",
      "As mentioned above, to further improve on the clarity it would be helpful to get more detail on the input & output setups and relevant dimensions of the design cases, especially the outputs.",
      "There are more details on the inputs in the appendix, but the outputs are still somewhat unclear.",
      "The paper addresses the application of a novel deep learning method to a non-traditional challenges that is significant."
    ]
  },
  {
    "paper_id": "2106.09019v2",
    "submission_id": "wdIDt--oLmV",
    "submission_title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate",
    "review_id": "tPEXfaKVsVG",
    "input": {
      "title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper addresses a very important challenge of automatically finding configurations that meet design constraints and are optimum with respect to a given set of objectives.\n- The approach is demonstrated on a path planning (for a class of 3D printers) problem and a snake-like soft robotics inverse kinematics problem.",
    "review_points_list": [
      "The paper addresses a very important challenge of automatically finding configurations that meet design constraints and are optimum with respect to a given set of objectives.",
      "The approach is demonstrated on a path planning (for a class of 3D printers) problem and a snake-like soft robotics inverse kinematics problem."
    ]
  },
  {
    "paper_id": "2106.09019v2",
    "submission_id": "wdIDt--oLmV",
    "submission_title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate",
    "review_id": "qLCcIKIiquj",
    "input": {
      "title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This is a clearly written paper on a timely topic.\n- The core contribution is the formulation factoring the evaluation of the design from the mapping of goals to designs, which breaks down the problem of many-valued mapping.\n- Both parts are defined in terms of a differentiable module, so that the training can be achieved jointly.\n- The evaluation methodology addresses what I would have expected to see - setting up baselines to show the extent to which each piece is relevant.\n- One baseline asks how much is achieved by just mapping from goals to designs from historical data, while another asks how much is achieved by optimizing designs given the trained surrogate or decoder.\n- I have only a few points of critique that I would have liked to have seen expanded in this paper.\n- This is based on the acknowledgement that the technically arguments seem essentially sound to me, so my comments are mainly on formulation.\n- Building on the notion of multiple ways for a design to be good, many engineering designs have path-dependent constraints.\n- For instance, while moving fast there may be a different cost to lateral deviations than when moving slowly.\n- This is a combination of dynamics (something not considered explicitly in the examples) and other forms of heteroscedasticity or path dependence.\n- To the extent that the surrogate and the decoder are fairly simple MLPs, it is unclear if such phenomena can be handled.\n- I think this is an important consideration, especially if the use cases include robotics and cyber-physical domains involving control.\n- While the introduction talks about considerations beyond the basic path, e.g., 'ease of manufacturing' mentioned in sec 3, they do not feature later on.\n- So, for instance, the soft body reaching problem could have been solved to some extent by multi-rigid body assumptions without seriously needing to acknowledge the infinite-dimensional space.\n- On the other hand, there are design problems involving fundamentally different designs with discrete and continuous choices, e.g., multiple body morphologies involving soft components each (just for argument's sake).\n- It is not clear if the proposed method would apply as described in these settings - calling for a clarification of generality and scope.\n- In the same vein, the design objectives here do admit a clear quantitative description always, e.g., distance from a desired path and reach error.\n- There are many design problems where the objective is qualitative (e.g., any design from a topologically defined class will suffice).\n- I expect that this might be easier to encode in this architecture but an example of this kind would have been helpful.\n- In summary, this is a clearly written paper whose methodology is sound.\n- It would be stronger if the formulation's assumptions and limits are more clearly delineated.",
    "review_points_list": [
      "This is a clearly written paper on a timely topic.",
      "The core contribution is the formulation factoring the evaluation of the design from the mapping of goals to designs, which breaks down the problem of many-valued mapping.",
      "Both parts are defined in terms of a differentiable module, so that the training can be achieved jointly.",
      "The evaluation methodology addresses what I would have expected to see - setting up baselines to show the extent to which each piece is relevant.",
      "One baseline asks how much is achieved by just mapping from goals to designs from historical data, while another asks how much is achieved by optimizing designs given the trained surrogate or decoder.",
      "I have only a few points of critique that I would have liked to have seen expanded in this paper.",
      "This is based on the acknowledgement that the technically arguments seem essentially sound to me, so my comments are mainly on formulation.",
      "Building on the notion of multiple ways for a design to be good, many engineering designs have path-dependent constraints.",
      "For instance, while moving fast there may be a different cost to lateral deviations than when moving slowly.",
      "This is a combination of dynamics (something not considered explicitly in the examples) and other forms of heteroscedasticity or path dependence.",
      "To the extent that the surrogate and the decoder are fairly simple MLPs, it is unclear if such phenomena can be handled.",
      "I think this is an important consideration, especially if the use cases include robotics and cyber-physical domains involving control.",
      "While the introduction talks about considerations beyond the basic path, e.g., 'ease of manufacturing' mentioned in sec 3, they do not feature later on.",
      "So, for instance, the soft body reaching problem could have been solved to some extent by multi-rigid body assumptions without seriously needing to acknowledge the infinite-dimensional space.",
      "On the other hand, there are design problems involving fundamentally different designs with discrete and continuous choices, e.g., multiple body morphologies involving soft components each (just for argument's sake).",
      "It is not clear if the proposed method would apply as described in these settings - calling for a clarification of generality and scope.",
      "In the same vein, the design objectives here do admit a clear quantitative description always, e.g., distance from a desired path and reach error.",
      "There are many design problems where the objective is qualitative (e.g., any design from a topologically defined class will suffice).",
      "I expect that this might be easier to encode in this architecture but an example of this kind would have been helpful.",
      "In summary, this is a clearly written paper whose methodology is sound.",
      "It would be stronger if the formulation's assumptions and limits are more clearly delineated."
    ]
  },
  {
    "paper_id": "2004.09623v1",
    "submission_id": "uNiEcPcn0J",
    "submission_title": "Fast Pure Exploration via Frank-Wolfe",
    "review_id": "LR9oqaHHL0",
    "input": {
      "title": "Fast Pure Exploration via Frank-Wolfe",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Thank you for your feedback, my concern is clearly addressed by author's response.\n- The authors propose a new algorithm called Cluster-Margin for multi-class active learning, which uses margin-based sampling.\n- The algorithm is compared to several state-of-the-art methods on several datasets and outperforms them in most cases.\n- The paper is well-written and easy to understand, but it lacks a thorough comparison of the algorithm's performance on different types of data.\n- This study could be improved by including a more detailed analysis of the algorithm's performance on different types of data, such as imbalanced datasets, high-dimensional data, or data with varying levels of noise.\n- The authors should also provide more details on the hyperparameters used in the experiments.\n- The algorithm's performance on imbalanced datasets is particularly important because many real-world datasets are imbalanced.\n- The authors should also compare the algorithm's performance to other state-of-the-art methods on these datasets.\n- The algorithm seems to be promising, but its performance on different types of data needs to be more thoroughly explored.\n- The paper is well-written and easy to understand.\n- The authors also provide some intuition behind the algorithm, which is very helpful for understanding its working mechanism.\n- The authors should also discuss the potential applications of the algorithm in real-world scenarios.\n- The authors also discuss the potential limitations of the algorithm and how future work could address these limitations.\n- The authors should also discuss the potential future work that could improve the algorithm.",
    "review_points_list": [
      "Thank you for your feedback, my concern is clearly addressed by author's response.",
      "The authors propose a new algorithm called Cluster-Margin for multi-class active learning, which uses margin-based sampling.",
      "The algorithm is compared to several state-of-the-art methods on several datasets and outperforms them in most cases.",
      "The paper is well-written and easy to understand, but it lacks a thorough comparison of the algorithm's performance on different types of data.",
      "This study could be improved by including a more detailed analysis of the algorithm's performance on different types of data, such as imbalanced datasets, high-dimensional data, or data with varying levels of noise.",
      "The authors should also provide more details on the hyperparameters used in the experiments.",
      "The algorithm's performance on imbalanced datasets is particularly important because many real-world datasets are imbalanced.",
      "The authors should also compare the algorithm's performance to other state-of-the-art methods on these datasets.",
      "The algorithm seems to be promising, but its performance on different types of data needs to be more thoroughly explored.",
      "The paper is well-written and easy to understand.",
      "The authors also provide some intuition behind the algorithm, which is very helpful for understanding its working mechanism.",
      "The authors should also discuss the potential applications of the algorithm in real-world scenarios.",
      "The authors also discuss the potential limitations of the algorithm and how future work could address these limitations.",
      "The authors should also discuss the potential future work that could improve the algorithm."
    ]
  },
  {
    "paper_id": "2004.09623v1",
    "submission_id": "uNiEcPcn0J",
    "submission_title": "Fast Pure Exploration via Frank-Wolfe",
    "review_id": "ok_761rmzm3",
    "input": {
      "title": "Fast Pure Exploration via Frank-Wolfe",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is well written and the delivered message is clear.\n- I did not check the all the proofs in detail (54 pages is quite long honestly), but they seem to be sound in general.\n- Proof-related questions are listed in the 'Limitations And Societal Impact' section.\n- This paper proposed a variant of BC with a modified FW (adapted to non-smooth functions) that achieves the asymptotic optimality in a wide range of pure exploration scenarios.\n- For me it is an interesting result and valuable contribution to the BAI community (at least for fixed-confidence BAI), since it has a wide range of practical usage thanks to FW and in the mean while avoids the $\beta$-tuning issue for top-two algorithms.\n- The technical novelty is also generally sound (please see below for some technical comments).\n- What if $N_k(t) = 0$?\n- How do we know that $\rm{\textbf{E}}$ = \rm{\textbf{E}_{\rm{\textbf{I}}}}^{t-1}\rm{\textbf{E}}$?\n- $\rm{\textbf{Y}}$ needs to be in $\rm{\textbf{M}}$ right?\n- F is Lipschitz with respect to which norm?\n- Are you sure about the last inequality $||z-x||_{\rm{\textbf{\textup{\fontsize{3}{3}{\begin{array}{c} \\\textup{\fontsize{3}{3}{\text{\text/.}}}} \\\textup{\text{\text/.}}}}}_{\rm{\textbf{\textup{\fontsize{3}{3}{\begin{array}{c} \\-\rm{\text{\textbf{\textup{\fontsize{3}{3}{\text{\fontsize{3}{3}{\text/.}}}} \\\textup{\text/.}}}}}}}}$?\n- $z_k(t) \rightarrow z_k(s)$.\n- It seems that $\rm{\textbf{\textup{\fontsize{3}{3}{\text{\texttd{\textvisualangles}}}}\"$}}$ is exponential in the number of arms $\rm{\textbf{V}}?$ Could it be a problem when dealing with many arms?\n- Overall, I vote for accept for this paper. Please see the next section for major questions and minor comments.",
    "review_points_list": [
      "The paper is well written and the delivered message is clear.",
      "I did not check the all the proofs in detail (54 pages is quite long honestly), but they seem to be sound in general.",
      "Proof-related questions are listed in the 'Limitations And Societal Impact' section.",
      "This paper proposed a variant of BC with a modified FW (adapted to non-smooth functions) that achieves the asymptotic optimality in a wide range of pure exploration scenarios.",
      "For me it is an interesting result and valuable contribution to the BAI community (at least for fixed-confidence BAI), since it has a wide range of practical usage thanks to FW and in the mean while avoids the $\beta$-tuning issue for top-two algorithms.",
      "The technical novelty is also generally sound (please see below for some technical comments).",
      "What if $N_k(t) = 0$?",
      "How do we know that $\rm{\textbf{E}}$ = \rm{\textbf{E}_{\rm{\textbf{I}}}}^{t-1}\rm{\textbf{E}}$?",
      "$\rm{\textbf{Y}}$ needs to be in $\rm{\textbf{M}}$ right?",
      "F is Lipschitz with respect to which norm?",
      "Are you sure about the last inequality $||z-x||_{\rm{\textbf{\textup{\fontsize{3}{3}{\begin{array}{c} \\\textup{\fontsize{3}{3}{\text{\text/.}}}} \\\textup{\text{\text/.}}}}}_{\rm{\textbf{\textup{\fontsize{3}{3}{\begin{array}{c} \\-\rm{\text{\textbf{\textup{\fontsize{3}{3}{\text{\fontsize{3}{3}{\text/.}}}} \\\textup{\text/.}}}}}}}}$?",
      "$z_k(t) \rightarrow z_k(s)$.",
      "It seems that $\rm{\textbf{\textup{\fontsize{3}{3}{\text{\texttd{\textvisualangles}}}}\"$}}$ is exponential in the number of arms $\rm{\textbf{V}}?$ Could it be a problem when dealing with many arms?",
      "Overall, I vote for accept for this paper. Please see the next section for major questions and minor comments."
    ]
  },
  {
    "paper_id": "2004.09623v1",
    "submission_id": "uNiEcPcn0J",
    "submission_title": "Fast Pure Exploration via Frank-Wolfe",
    "review_id": "Hs3G6LGBqtt",
    "input": {
      "title": "Fast Pure Exploration via Frank-Wolfe",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- High originality\n- Mediocre quality (see my comments on experiments below)\n- Clarity is good\n- Significance is above bar\n- The main contribution is very clear: they provide a new framework and recipe for solving generic pure exploration problems.\n- I think the authors did a nice job of interpreting best challenger from Garivier & Kauffman as a frank-wolfe step and find a way to do it right.\n- I feel the experiments are quite biased in that the baseline methods are missing a few popular algorithms including RAGE, LinGapE, and ALBA.\n- For the unstructured problem, lil ucb and LUCB are missing.\n- Thus, I am not convinced about its performance.\n- I was excited to read the abstract that we take only one step in each iteration since that would mean the computational complexity is much improved.\n- However, it seems to require solving the linear program.\n- Could the authors discuss the time complexity of the algorithm?\n- For pure exploration problems, It seems to be worse than TaS.\n- Please add a discussion on the significance of Theorem 2 in the main content of the paper.\n- I guess the first term looks fine, but how should readers take \u03a8() and T\u03b5,L?\n- Does the authors think the exponential dependence on K should be improvable?\n- L136: 'close the boundary' => close to the boundeary?\n- L151-152: 'the optimism-in-front-of-uncertainty ': I've seen it called 'optimism in the face of uncertainty' more frequently.\n- L680: did you mean [21] rather than [29] here?\n- L698: there are other places like L680.\n- Okay, I am struggling with finding the precise definition of \u039b.\n- So, \u039b is the set of possible instances for which there is only a unique best arm?\n- Then, it is not a convex set\n- I ask this to understand the if statement in Algorithm 1 that has hat \u03bc(t \u2212 1) \u2209 \u039b.\n- I could guess but I would appreciate if the definition of \u039b is more explicit.\n- Assumption 3: is \u2200 symbol missing in front of \u03c0 \u2208 S i\u204e(\u03bc)?\n- L290: con() was defined as con{}.\n- I am mostly satisfied with the rebuttal.\n- I now remember that lil'ucb was evaluated in 'Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting' and was empirically shown to be worse than LUCB in practical regimes anyways.\n- But it would be great to have that remark in the final version.\n- For Assumption 3, I understand it now.\n- I rephrase and expand the sentence to make it easier to read.\n- Significance is above bar",
    "review_points_list": [
      "High originality",
      "Mediocre quality (see my comments on experiments below)",
      "Clarity is good",
      "Significance is above bar",
      "The main contribution is very clear: they provide a new framework and recipe for solving generic pure exploration problems.",
      "I think the authors did a nice job of interpreting best challenger from Garivier & Kauffman as a frank-wolfe step and find a way to do it right.",
      "I feel the experiments are quite biased in that the baseline methods are missing a few popular algorithms including RAGE, LinGapE, and ALBA.",
      "For the unstructured problem, lil ucb and LUCB are missing.",
      "Thus, I am not convinced about its performance.",
      "I was excited to read the abstract that we take only one step in each iteration since that would mean the computational complexity is much improved.",
      "However, it seems to require solving the linear program.",
      "Could the authors discuss the time complexity of the algorithm?",
      "For pure exploration problems, It seems to be worse than TaS.",
      "Please add a discussion on the significance of Theorem 2 in the main content of the paper.",
      "I guess the first term looks fine, but how should readers take \u03a8() and T\u03b5,L?",
      "Does the authors think the exponential dependence on K should be improvable?",
      "L136: 'close the boundary' => close to the boundeary?",
      "L151-152: 'the optimism-in-front-of-uncertainty ': I've seen it called 'optimism in the face of uncertainty' more frequently.",
      "L680: did you mean [21] rather than [29] here?",
      "L698: there are other places like L680.",
      "Okay, I am struggling with finding the precise definition of \u039b.",
      "So, \u039b is the set of possible instances for which there is only a unique best arm?",
      "Then, it is not a convex set",
      "I ask this to understand the if statement in Algorithm 1 that has hat \u03bc(t \u2212 1) \u2209 \u039b.",
      "I could guess but I would appreciate if the definition of \u039b is more explicit.",
      "Assumption 3: is \u2200 symbol missing in front of \u03c0 \u2208 S i\u204e(\u03bc)?",
      "L290: con() was defined as con{}.",
      "I am mostly satisfied with the rebuttal.",
      "I now remember that lil'ucb was evaluated in 'Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting' and was empirically shown to be worse than LUCB in practical regimes anyways.",
      "But it would be great to have that remark in the final version.",
      "For Assumption 3, I understand it now.",
      "I rephrase and expand the sentence to make it easier to read.",
      "Significance is above bar"
    ]
  },
  {
    "paper_id": "2107.09609v2",
    "submission_id": "tfBBt_q4nHT",
    "submission_title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
    "review_id": "aoT6ot4EuTk",
    "input": {
      "title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- 1. The dataset unifies two tasks based on text queries -- moment retrieval and highlight detection, and show how joint training can affect the performance of the baseline model on these two tasks respectively.\n- 2. The collected dataset overcomes some of the limitations in other moment retrieval datasets by having less temporal bias and allowing a text query to match multiple disjoint moments.\n- 3. The dataset accounts for the subjectiveness in highlight detection and provide dense annotations.\n- 4. Good quality control through the qualification test and the verification between multiple workers.\n- 1. Line 259-260 'we define a prediction to be positive if it has a high IoU with one of the ground truth moments'. It is not very clear here whether 'one of the ground truth moments' means one from 3 annotators or one from disjoint moments.\n- 2. For highlight detection: Line 262-263, 'we have ground-truth saliency scores from 3 users, we evaluate performance against each then take the average'. Does it mean the model's performance is dependent on the inter-user agreement?\n- 3. Missing baseline experiments/ablations: - The results from using only CLIP and Slow-Fast networks as visual encoder, as in how important the temporal information is in the two tasks.\n-  - Performance on moment retrieval after the model is pre-trained via ASR, to see how much improvement has supervised training brought.\n-  - Evaluation of SoTA models on moment retrieval which have official implementations online: 2D-TAN[1], DRN[2], CBP[3]\n- 4. Performance of the baseline model: - Since one advantage of the dataset is having queries matching disjoint moments, how well can the baseline model spotting all the disjoint moments given a query, is there a metric to evaluate it?\n-  - Table 1 shows the dataset has a wide range of granularity in the verbs (e.g, cut and report) and nouns(dog and weather), does the performance of the baseline model varies by the granularity?\n- 5. While the highlights are labelled on a five-point scale, is that only the 'very good' ones are used in training and evaluation?\n- 6. Related work: [4] uses a DETR-like architecture for temporal detection, and treat the task as a set-prediction task.\n- The QVHIGHLIGHTS dataset addresses some limitations in current moment retrieval/highlight detection datasets. It is presented in a clear way and will be beneficial to future research in these areas. However, some detailed analysis on the dataset is missing, and more baseline experiments are needed to verify the design choice in the baseline architecture.",
    "review_points_list": [
      "1. The dataset unifies two tasks based on text queries -- moment retrieval and highlight detection, and show how joint training can affect the performance of the baseline model on these two tasks respectively.",
      "2. The collected dataset overcomes some of the limitations in other moment retrieval datasets by having less temporal bias and allowing a text query to match multiple disjoint moments.",
      "3. The dataset accounts for the subjectiveness in highlight detection and provide dense annotations.",
      "4. Good quality control through the qualification test and the verification between multiple workers.",
      "1. Line 259-260 'we define a prediction to be positive if it has a high IoU with one of the ground truth moments'. It is not very clear here whether 'one of the ground truth moments' means one from 3 annotators or one from disjoint moments.",
      "2. For highlight detection: Line 262-263, 'we have ground-truth saliency scores from 3 users, we evaluate performance against each then take the average'. Does it mean the model's performance is dependent on the inter-user agreement?",
      "3. Missing baseline experiments/ablations: - The results from using only CLIP and Slow-Fast networks as visual encoder, as in how important the temporal information is in the two tasks.",
      " - Performance on moment retrieval after the model is pre-trained via ASR, to see how much improvement has supervised training brought.",
      " - Evaluation of SoTA models on moment retrieval which have official implementations online: 2D-TAN[1], DRN[2], CBP[3]",
      "4. Performance of the baseline model: - Since one advantage of the dataset is having queries matching disjoint moments, how well can the baseline model spotting all the disjoint moments given a query, is there a metric to evaluate it?",
      " - Table 1 shows the dataset has a wide range of granularity in the verbs (e.g, cut and report) and nouns(dog and weather), does the performance of the baseline model varies by the granularity?",
      "5. While the highlights are labelled on a five-point scale, is that only the 'very good' ones are used in training and evaluation?",
      "6. Related work: [4] uses a DETR-like architecture for temporal detection, and treat the task as a set-prediction task.",
      "The QVHIGHLIGHTS dataset addresses some limitations in current moment retrieval/highlight detection datasets. It is presented in a clear way and will be beneficial to future research in these areas. However, some detailed analysis on the dataset is missing, and more baseline experiments are needed to verify the design choice in the baseline architecture."
    ]
  },
  {
    "paper_id": "2107.09609v2",
    "submission_id": "tfBBt_q4nHT",
    "submission_title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
    "review_id": "CPYUigiH6v",
    "input": {
      "title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is clearly written\n- The dataset is going to be useful to the community as it seems it is quite high quality (based on the description and the rater agreement).\n- I believe that the introduced method MomentDETR is a good extension to the DETR approach for moment retrieval in videos.\n- The pre-training strategy on weakly supervised data with ASR is interesting and leads to a substantial improvement.\n- The weakly supervised pretraining seems to bring a substantial gain in performance (Table 3). However from what I understood, only a small dataset is used for this pretraining (only 5.4K videos).\n- Have you thought about using larger datasets for this pretraining? How would that impact the performance of the method?\n- One could for example use the HowTo100M dataset for that [22].\n- I believe that a scale study would be very valuable there even if not scaling to the full HowTo100M dataset\n- e.g. providing the performance with only 2.5K videos and 10K videos to see the impact of increasing the size of the pretraining.\n- The VLOGs dataset [a] could also be used for that purpose.\n- MomentDETR is an interesting method in itself.\n- For that reason, I think it would be worthwhile to run MomentDETR on other moment retrieval benchmarks to show it is competitive on known benchmarks.\n- I think one of the main issue of the paper is that it introduces a new dataset and a new method so it is hard to convince the reader that the method is indeed better than pas work such as [30,19,12,4,16] as we cannot compare direclty to some numbers provided in these papers.\n- I do trust the authors for having run these baselines properly but I think that running the method on other datasets would increase the strength of the paper.\n- How are $t_{high}$ and $t_{low}$ defined based on the scores from 1 to 5?\n- How important is the term using low/high versus the term using out/in.\n- Could you provide an ablation about this in Table 4 (keeping all other elements i.e. L1/gIoU and CLS)?\n- If the term low/high is indeed important that would justify more the need for annotating individual frames which is one of the specificity of the QVHighLights dataset.\n- Some frames within positive annotated segments are annotated as `Very bad` (and `Bad`) for the given query.\n- This seems to contradict that these frames are indeed positive for that query.\n- In that case why not removing these frames all together from the annotation and create more moments.\n- It would also be useful to provide the score distribution (% of frames that receive a score of 1,2,3,4,5...).\n- L79: lsmiting -> limiting\n- L100: sailiency -> saliency\n- L326: detetcion -> detection\n- From lifestyle vlogs to everyday interactions. Fouhey et al. CVPR18\n- The paper makes good contributions.\n- However I believe that these contributions could be strengthen following the advices provided in the weaknesses section.\n- For now I lean towards acceptance and I am ready to increase my score if some/all concerns are addressed during the rebuttal.\n- I am happy to support acceptance of this paper.\n- I really encourage the authors to include all the new ablations and set of experiments requested by the reviewers as they will make for a stronger paper, which in turn will be more influential in the community.",
    "review_points_list": [
      "The paper is clearly written",
      "The dataset is going to be useful to the community as it seems it is quite high quality (based on the description and the rater agreement).",
      "I believe that the introduced method MomentDETR is a good extension to the DETR approach for moment retrieval in videos.",
      "The pre-training strategy on weakly supervised data with ASR is interesting and leads to a substantial improvement.",
      "The weakly supervised pretraining seems to bring a substantial gain in performance (Table 3). However from what I understood, only a small dataset is used for this pretraining (only 5.4K videos).",
      "Have you thought about using larger datasets for this pretraining? How would that impact the performance of the method?",
      "One could for example use the HowTo100M dataset for that [22].",
      "I believe that a scale study would be very valuable there even if not scaling to the full HowTo100M dataset",
      "e.g. providing the performance with only 2.5K videos and 10K videos to see the impact of increasing the size of the pretraining.",
      "The VLOGs dataset [a] could also be used for that purpose.",
      "MomentDETR is an interesting method in itself.",
      "For that reason, I think it would be worthwhile to run MomentDETR on other moment retrieval benchmarks to show it is competitive on known benchmarks.",
      "I think one of the main issue of the paper is that it introduces a new dataset and a new method so it is hard to convince the reader that the method is indeed better than pas work such as [30,19,12,4,16] as we cannot compare direclty to some numbers provided in these papers.",
      "I do trust the authors for having run these baselines properly but I think that running the method on other datasets would increase the strength of the paper.",
      "How are $t_{high}$ and $t_{low}$ defined based on the scores from 1 to 5?",
      "How important is the term using low/high versus the term using out/in.",
      "Could you provide an ablation about this in Table 4 (keeping all other elements i.e. L1/gIoU and CLS)?",
      "If the term low/high is indeed important that would justify more the need for annotating individual frames which is one of the specificity of the QVHighLights dataset.",
      "Some frames within positive annotated segments are annotated as `Very bad` (and `Bad`) for the given query.",
      "This seems to contradict that these frames are indeed positive for that query.",
      "In that case why not removing these frames all together from the annotation and create more moments.",
      "It would also be useful to provide the score distribution (% of frames that receive a score of 1,2,3,4,5...).",
      "L79: lsmiting -> limiting",
      "L100: sailiency -> saliency",
      "L326: detetcion -> detection",
      "From lifestyle vlogs to everyday interactions. Fouhey et al. CVPR18",
      "The paper makes good contributions.",
      "However I believe that these contributions could be strengthen following the advices provided in the weaknesses section.",
      "For now I lean towards acceptance and I am ready to increase my score if some/all concerns are addressed during the rebuttal.",
      "I am happy to support acceptance of this paper.",
      "I really encourage the authors to include all the new ablations and set of experiments requested by the reviewers as they will make for a stronger paper, which in turn will be more influential in the community."
    ]
  },
  {
    "paper_id": "2107.09609v2",
    "submission_id": "tfBBt_q4nHT",
    "submission_title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
    "review_id": "SlMc6R9f6L",
    "input": {
      "title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The interface design could be improved for better user experience.\n- The product features are very innovative and add value to the users.\n- However, there are some performance issues that need to be addressed.\n- The customer support team is responsive and helpful.\n- Overall, it's a good product with room for improvement.",
    "review_points_list": [
      "The interface design could be improved for better user experience.",
      "The product features are very innovative and add value to the users.",
      "However, there are some performance issues that need to be addressed.",
      "The customer support team is responsive and helpful.",
      "Overall, it's a good product with room for improvement."
    ]
  },
  {
    "paper_id": "1810.02358v2",
    "submission_id": "vsCCDVdTAx",
    "submission_title": "Human-Adversarial Visual Question Answering",
    "review_id": "_6HMhTMn7ce",
    "input": {
      "title": "Human-Adversarial Visual Question Answering",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- It is interesting and novel to collect adversarial examples by making human annotators interact with and fool state-of-the-art models. The questions generated in this way are semantically meaningful to humans, compared with adversarial examples created by adding imperceptible noise.\n- The collected benchmark AdVQA is valuable to the community. It provides a quantitive measure of how well current state-of-the-art models perform on adversarial examples and can further spur the progress along this direction. The authors promise to make a public evaluation server to evaluate on AdVQA.\n- The authors conduct extensive experiments on AdVQA and benchmark tens of baseline methods. The analysis points out several reasons for the performance gap between humans and models, such as inability to read text and count, heavy bias, and external knowledge.\n- The paper is overall well-written and easy to follow.\n- My major concern is that the collected examples are more of 'out-of-distribution' than 'adversarial'. As explained in Line 302~303, 15.4% of the questions in AdVQA require detecting texts in images, as shown by the first example in Table 1. The state-of-the-art models that are trained on VQA v2 definitely fail on these questions, since they are never trained on such examples. It is similar to the questions requiring external knowledge or involving unseen concepts.\n- It is unclear what is the most significant difference between AdVQA and other VQA datasets, such as OKVQA (requiring external knowledge), TextVQA (involving texts in images), and VQA-CP (changing prior). Although AdVQA is collected in a human-adversarial manner, the collected questions seems a combination of previous datasets. The authors might clarify if there are some unique points in AdVQA, or if not, why such a combination is meaningful and what benefits it brings compared with previous datasets.",
    "review_points_list": [
      "It is interesting and novel to collect adversarial examples by making human annotators interact with and fool state-of-the-art models. The questions generated in this way are semantically meaningful to humans, compared with adversarial examples created by adding imperceptible noise.",
      "The collected benchmark AdVQA is valuable to the community. It provides a quantitive measure of how well current state-of-the-art models perform on adversarial examples and can further spur the progress along this direction. The authors promise to make a public evaluation server to evaluate on AdVQA.",
      "The authors conduct extensive experiments on AdVQA and benchmark tens of baseline methods. The analysis points out several reasons for the performance gap between humans and models, such as inability to read text and count, heavy bias, and external knowledge.",
      "The paper is overall well-written and easy to follow.",
      "My major concern is that the collected examples are more of 'out-of-distribution' than 'adversarial'. As explained in Line 302~303, 15.4% of the questions in AdVQA require detecting texts in images, as shown by the first example in Table 1. The state-of-the-art models that are trained on VQA v2 definitely fail on these questions, since they are never trained on such examples. It is similar to the questions requiring external knowledge or involving unseen concepts.",
      "It is unclear what is the most significant difference between AdVQA and other VQA datasets, such as OKVQA (requiring external knowledge), TextVQA (involving texts in images), and VQA-CP (changing prior). Although AdVQA is collected in a human-adversarial manner, the collected questions seems a combination of previous datasets. The authors might clarify if there are some unique points in AdVQA, or if not, why such a combination is meaningful and what benefits it brings compared with previous datasets."
    ]
  },
  {
    "paper_id": "1810.02358v2",
    "submission_id": "vsCCDVdTAx",
    "submission_title": "Human-Adversarial Visual Question Answering",
    "review_id": "y3WCVkYg4R-",
    "input": {
      "title": "Human-Adversarial Visual Question Answering",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- To this reviewer, this paper seems strong.\n- It is perhaps not surprising that performance is lower on AdVQA, however, it is impressive that it has higher human agreement than the non-adversarial VQA.\n- The magnitude of how much models drop in performance is significant (at least this reviewer): from the 70% range to 33% at most.\n- It seems like this could be a great addition in the community (assuming it is released, etc.) for modeling work on VQA-V2 to report on AdVQA in addition.\n- The qualitative analysis is insightful and much appreciated (to this reviewer).\n- The additional experiments are helpful.\n- Interestingly, the discussion at L222 (suggesting that performance goes up just a little bit if finetuned on val) seems qualitatively different to trends with other datasets.\n- For instance, in a lot of past work (e.g. Adversarial Filtering (from Le Bras et al ICML 2020 and earlier works)) one round of 'fooling the model' is shown to be not enough. However, it's great that it works here (possibly though because val is small).\n- The limitations and broader impact sections could be somewhat improved (see the next section).",
    "review_points_list": [
      "To this reviewer, this paper seems strong.",
      "It is perhaps not surprising that performance is lower on AdVQA, however, it is impressive that it has higher human agreement than the non-adversarial VQA.",
      "The magnitude of how much models drop in performance is significant (at least this reviewer): from the 70% range to 33% at most.",
      "It seems like this could be a great addition in the community (assuming it is released, etc.) for modeling work on VQA-V2 to report on AdVQA in addition.",
      "The qualitative analysis is insightful and much appreciated (to this reviewer).",
      "The additional experiments are helpful.",
      "Interestingly, the discussion at L222 (suggesting that performance goes up just a little bit if finetuned on val) seems qualitatively different to trends with other datasets.",
      "For instance, in a lot of past work (e.g. Adversarial Filtering (from Le Bras et al ICML 2020 and earlier works)) one round of 'fooling the model' is shown to be not enough. However, it's great that it works here (possibly though because val is small).",
      "The limitations and broader impact sections could be somewhat improved (see the next section)."
    ]
  },
  {
    "paper_id": "1810.02358v2",
    "submission_id": "vsCCDVdTAx",
    "submission_title": "Human-Adversarial Visual Question Answering",
    "review_id": "ixTtSeT3AX",
    "input": {
      "title": "Human-Adversarial Visual Question Answering",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- In my view, this is mainly a resource paper as the major contribution would be the collected new dataset.\n- The data collection procedure is standard, but collecting high quality dataset is definitely not trivial especially for human-in-the-loop dataset collection.\n- This is the first work to explore multimodal human-adversarial benchmark.\n- Extensive analysis about the new dataset is provided in the paper, and experiments with existing VQA methods show the usefulness of the dataset.\n- The technical contribution of this work is limited as it evaluated existing methods off-the-shelf on this new dataset.\n- The paper is well-written and quite easy to follow.\n- This benchmark has the potential to be an important 'stress test' testbed for new VQA methods, if there is no obvious bias in the collected dataset (which is not thoroughly tested thus unknown from this paper).",
    "review_points_list": [
      "In my view, this is mainly a resource paper as the major contribution would be the collected new dataset.",
      "The data collection procedure is standard, but collecting high quality dataset is definitely not trivial especially for human-in-the-loop dataset collection.",
      "This is the first work to explore multimodal human-adversarial benchmark.",
      "Extensive analysis about the new dataset is provided in the paper, and experiments with existing VQA methods show the usefulness of the dataset.",
      "The technical contribution of this work is limited as it evaluated existing methods off-the-shelf on this new dataset.",
      "The paper is well-written and quite easy to follow.",
      "This benchmark has the potential to be an important 'stress test' testbed for new VQA methods, if there is no obvious bias in the collected dataset (which is not thoroughly tested thus unknown from this paper)."
    ]
  },
  {
    "paper_id": "1810.02358v2",
    "submission_id": "vsCCDVdTAx",
    "submission_title": "Human-Adversarial Visual Question Answering",
    "review_id": "TY2zCfx2Lt",
    "input": {
      "title": "Human-Adversarial Visual Question Answering",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Missing discussions on related VQA stress test datasets\n- AdVQA probably is not the first work to have human adversarial approach in Multimodal space\n- Detailed question type analysis\n- Answers in AdVQA\n- BERT performance on AdVQA\n- Differences between AdVQA and ANLI\n- Best Practice with AdVQA",
    "review_points_list": [
      "Missing discussions on related VQA stress test datasets",
      "AdVQA probably is not the first work to have human adversarial approach in Multimodal space",
      "Detailed question type analysis",
      "Answers in AdVQA",
      "BERT performance on AdVQA",
      "Differences between AdVQA and ANLI",
      "Best Practice with AdVQA"
    ]
  },
  {
    "paper_id": "1611.08194v1",
    "submission_id": "zDtFO9vohmF",
    "submission_title": "Kernel Functional Optimisation",
    "review_id": "8VZJTOdQvTs",
    "input": {
      "title": "Kernel Functional Optimisation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I have found the article to be clearly written, with impressive numerical benchmarks, and a nice balance between exposition of the theory, of the algorithms, of the theoretical and numerical results.\n- Though I did not go through the proofs in the appendix, the latter nicely complements the main body of the article.\n- Clipping: Since for indefinite kernels the strong topology is given by $k_++k_-$, have the authors tried flipping the coefficients ($\rightarrow |\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\textbf{ w_pp i} \rightarrow i)|$) instead of clipping them?\n- In the synthetic/numerical experiments, can the authors show the number/magnitude of the coefficients that where clipped?\n- I expect that, since kernels are understood in GPs as generating the covariance matrices, few negative coefficients should appear when fitting the synthetic curves.\n- Could you remind the reader why negative definite covariance matrices cannot be considered in GPs as to explain why a clipping has to be introduced?\n- It seems to me that in ``Learning the Kernel with Hyperkernels`` the discussion after Lemma 7 rules indefinite kernels out only because the authors have decided to focus on RKHSs rather than RKKSs.\n- I thus imagine that the reasons for clipping differ between the kernel and GP communities.\n- Synthetic experiments: In Figure 2 and in the appendix, all the functions appear to have a non-zero mean.\n- In kernel ridge regression (KRR), this would make the task much more difficult for the RBF/SE/Gaussian kernel and I am wondering if this may explain the failure of the SE kernel with this characteristic downward form sticking to zero in the area without observations.\n- Can the authors repeat the synthetic experiments when normalizing the curves to have zero mean?\n- While the choice of a linear kernel is classical, for these types of signals, it would be more adapted if there were some linear trends.\n- Here it just seems to skew the confidence intervals.\n- I believe that if the x-axis was centered to zero, then the weight assigned to the linear kernel would be close to null.\n- Overall, the Matern 3/2 kernel seems to produce confidence intervals very much alike the ones of the KFO.\n- This begs the question of plotting the kernel of the KFO (for instance by showing $y\rightarrow k(x,y)$ for several $x$).\n- Does it recover some translation-invariant features of the data?\n- All the experiments seem to focus exclusively on regression, would the KFO procedure be able to recover the covariance/kernel of a dataset that would be generated with zero mean and covariance some given kernel $k(x,y)$?\n- Can an experiment be done in such a direction?\n- More generally, since $\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{hell}$ was managed to be chosen in &[0,1]&$, would there be techniques to sample the kernels exclusively in the cone of positive definite kernels?\n- As said above, I liked the article and am willing to upgrade my mark depending on the authors' answers.\n- Minor comments:\n- 7 I would suggest citing the Ong et al. 2005 paper directly in the abstract since it serves as the main source for the procedure\n- 110 Hilbert with uppercase\n- 119  ``Learning the Kernel with Hyperkernels`` (Lemma 7) could be quoted with profit to justify for the finite decomposition.\n- Or do the authors only intend to give examples of the type of indefinite kernels contained in $H_\textit{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{Were will be examples of the type of indefinite kernels contained in Amp}$?\n- I very much appreciated the discussion of paragraph 109-120 which denotes a good knowledge of the existing literature.\n- 207 this should be a $succcurlyeq$\n- 209-214 It is not clear to me why EVD was presented on 196-208 if not used (this contradicts 194 by the way).\n- If the techniques of 196-208 are just quoted as other possibilities that are dismissed, then it should be stated more clearly.\n- 225 Euclidean with uppercase\n- 240 Formulation of Definition 3 is convoluted.\n- I would suggest \u201cif there exists\u201d rather than \u201csuch that there exists\u201d and to write $f:H_\textit{\textit{With$ \rightarrow H_\textit{Kappa$ to make explicit input and output spaces (same for Theorem 2).\n- 241 Why underline the kernels?\n- They have the same interpretation as the ones of Eq. (5).\n- 244 The kernel $k$ is a kernel over the hyper-RKHS, using $k$ here can be confusing.\n- Maybe change it to $\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textit{\boldsymbol{\boldsymbol{or any preferred notation?\n- 319 In Table 1, what is $\textit{\textit{\textit{\boldsymbol{\textit{\boldsymbol{\textit{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{referer\" referring to?",
    "review_points_list": [
      "I have found the article to be clearly written, with impressive numerical benchmarks, and a nice balance between exposition of the theory, of the algorithms, of the theoretical and numerical results.",
      "Though I did not go through the proofs in the appendix, the latter nicely complements the main body of the article.",
      "Clipping: Since for indefinite kernels the strong topology is given by $k_++k_-$, have the authors tried flipping the coefficients ($\rightarrow |\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\textbf{ w_pp i} \rightarrow i)|$) instead of clipping them?",
      "In the synthetic/numerical experiments, can the authors show the number/magnitude of the coefficients that where clipped?",
      "I expect that, since kernels are understood in GPs as generating the covariance matrices, few negative coefficients should appear when fitting the synthetic curves.",
      "Could you remind the reader why negative definite covariance matrices cannot be considered in GPs as to explain why a clipping has to be introduced?",
      "It seems to me that in ``Learning the Kernel with Hyperkernels`` the discussion after Lemma 7 rules indefinite kernels out only because the authors have decided to focus on RKHSs rather than RKKSs.",
      "I thus imagine that the reasons for clipping differ between the kernel and GP communities.",
      "Synthetic experiments: In Figure 2 and in the appendix, all the functions appear to have a non-zero mean.",
      "In kernel ridge regression (KRR), this would make the task much more difficult for the RBF/SE/Gaussian kernel and I am wondering if this may explain the failure of the SE kernel with this characteristic downward form sticking to zero in the area without observations.",
      "Can the authors repeat the synthetic experiments when normalizing the curves to have zero mean?",
      "While the choice of a linear kernel is classical, for these types of signals, it would be more adapted if there were some linear trends.",
      "Here it just seems to skew the confidence intervals.",
      "I believe that if the x-axis was centered to zero, then the weight assigned to the linear kernel would be close to null.",
      "Overall, the Matern 3/2 kernel seems to produce confidence intervals very much alike the ones of the KFO.",
      "This begs the question of plotting the kernel of the KFO (for instance by showing $y\rightarrow k(x,y)$ for several $x$).",
      "Does it recover some translation-invariant features of the data?",
      "All the experiments seem to focus exclusively on regression, would the KFO procedure be able to recover the covariance/kernel of a dataset that would be generated with zero mean and covariance some given kernel $k(x,y)$?",
      "Can an experiment be done in such a direction?",
      "More generally, since $\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\textbf{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{\boldsymbol{\boldsymbol{\textbf{\textbf{hell}$ was managed to be chosen in &[0,1]&$, would there be techniques to sample the kernels exclusively in the cone of positive definite kernels?",
      "As said above, I liked the article and am willing to upgrade my mark depending on the authors' answers.",
      "Minor comments:",
      "7 I would suggest citing the Ong et al. 2005 paper directly in the abstract since it serves as the main source for the procedure",
      "110 Hilbert with uppercase",
      "119  ``Learning the Kernel with Hyperkernels`` (Lemma 7) could be quoted with profit to justify for the finite decomposition.",
      "Or do the authors only intend to give examples of the type of indefinite kernels contained in $H_\textit{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textbf{\boldsymbol{\textbf{Were will be examples of the type of indefinite kernels contained in Amp}$?",
      "I very much appreciated the discussion of paragraph 109-120 which denotes a good knowledge of the existing literature.",
      "207 this should be a $succcurlyeq$",
      "209-214 It is not clear to me why EVD was presented on 196-208 if not used (this contradicts 194 by the way).",
      "If the techniques of 196-208 are just quoted as other possibilities that are dismissed, then it should be stated more clearly.",
      "225 Euclidean with uppercase",
      "240 Formulation of Definition 3 is convoluted.",
      "I would suggest \u201cif there exists\u201d rather than \u201csuch that there exists\u201d and to write $f:H_\textit{\textit{With$ \rightarrow H_\textit{Kappa$ to make explicit input and output spaces (same for Theorem 2).",
      "241 Why underline the kernels?",
      "They have the same interpretation as the ones of Eq. (5).",
      "244 The kernel $k$ is a kernel over the hyper-RKHS, using $k$ here can be confusing.",
      "Maybe change it to $\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\textit{\boldsymbol{\boldsymbol{or any preferred notation?",
      "319 In Table 1, what is $\textit{\textit{\textit{\boldsymbol{\textit{\boldsymbol{\textit{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{referer\" referring to?"
    ]
  },
  {
    "paper_id": "1611.08194v1",
    "submission_id": "zDtFO9vohmF",
    "submission_title": "Kernel Functional Optimisation",
    "review_id": "lpzP9SzcWs3",
    "input": {
      "title": "Kernel Functional Optimisation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- A key contribution of the paper is of the ability to address a broader set of kernels that are non-stationary by using indefinite kernels later approximating positive-definite projection.\n- This allows the kernel regression approximation to deal with sharp changes in function value as shown in synthetic data.\n- Paper also provides sound theoretical justification in terms of regret convergence of the proposed algorithm as parameters of the effective dimension.\n- Projection of data to S subspaces provides a scalable approach to deal with larger datasets but parameter selection is not clear.\n- (n, T, S) seem to best selected for UCI datasets but selection strategy is not clear.\n- How would parameter selection work for datasets like ImageNet and so on.\n- Evaluation is shown on UCI dataset which are real-word but small and not clear if the proposed technique also works for large scale 100s of categories' classification.\n- Table 1 and Table 2 show that KFO does well for almost all but a few datasets.\n- For classification Credit, Biodeg and Phoneme other classifiers do well and for regression Fertility dataset ARD Matern does better.\n- Is there an explanation for such a difference?\n- Are there any dataset specific peculiarities when KFO does not do well?",
    "review_points_list": [
      "A key contribution of the paper is of the ability to address a broader set of kernels that are non-stationary by using indefinite kernels later approximating positive-definite projection.",
      "This allows the kernel regression approximation to deal with sharp changes in function value as shown in synthetic data.",
      "Paper also provides sound theoretical justification in terms of regret convergence of the proposed algorithm as parameters of the effective dimension.",
      "Projection of data to S subspaces provides a scalable approach to deal with larger datasets but parameter selection is not clear.",
      "(n, T, S) seem to best selected for UCI datasets but selection strategy is not clear.",
      "How would parameter selection work for datasets like ImageNet and so on.",
      "Evaluation is shown on UCI dataset which are real-word but small and not clear if the proposed technique also works for large scale 100s of categories' classification.",
      "Table 1 and Table 2 show that KFO does well for almost all but a few datasets.",
      "For classification Credit, Biodeg and Phoneme other classifiers do well and for regression Fertility dataset ARD Matern does better.",
      "Is there an explanation for such a difference?",
      "Are there any dataset specific peculiarities when KFO does not do well?"
    ]
  },
  {
    "paper_id": "1611.08194v1",
    "submission_id": "zDtFO9vohmF",
    "submission_title": "Kernel Functional Optimisation",
    "review_id": "Rvw8_MSK8g",
    "input": {
      "title": "Kernel Functional Optimisation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper introduces a novel approach which constructs the best fitting kernel function with adaptive tuned parameters.\n- This kernel function is obtained by the linear combination of multiple kernels sampled from a prior Gaussian Process.\n- The experimental results of two different kernel-based algorithms show the superiority of the proposed method.",
    "review_points_list": [
      "This paper introduces a novel approach which constructs the best fitting kernel function with adaptive tuned parameters.",
      "This kernel function is obtained by the linear combination of multiple kernels sampled from a prior Gaussian Process.",
      "The experimental results of two different kernel-based algorithms show the superiority of the proposed method."
    ]
  },
  {
    "paper_id": "1611.08194v1",
    "submission_id": "zDtFO9vohmF",
    "submission_title": "Kernel Functional Optimisation",
    "review_id": "Ueas9mffuJB",
    "input": {
      "title": "Kernel Functional Optimisation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The authors use Bayesian optimization for kernel selection, which has previously been proposed in Malkomes et al.\n- There are similarities between the proposed method and the mentioned paper.\n- Both papers use the GP to model the expensive objective function and optimize the objective via Bayesian optimization.\n- There are also differences. Most notably, in the mentioned paper, the kernel space is defined in terms of base kernels and a grammar to combine them, instead of using the Hyper-RKHS.\n- The mentioned paper does not reference the paper under review, unfortunately.\n- The paper is technically sound.\n- The authors provide a convergence analysis of the proposed method.\n- The paper is clearly written.\n- One unclear point that needs more discussion is the way used to select $N^2$ points for representing the hyper-kernel.\n- The points are constructed such that they represent the kernel sufficiently well (lines 142-143), however, it is not clear how.\n- The empirical evaluation shows promising results.\n- There are points that need to be clarified. The paper does not provide a clear discussion of the computational/memory demands of the method.\n- The paper does not provide a clear discussion of the efficacy w.r.t. the size of the data.\n- The authors should provide at least rough information about the computational time needed to perform the experiments on the data used.",
    "review_points_list": [
      "The authors use Bayesian optimization for kernel selection, which has previously been proposed in Malkomes et al.",
      "There are similarities between the proposed method and the mentioned paper.",
      "Both papers use the GP to model the expensive objective function and optimize the objective via Bayesian optimization.",
      "There are also differences. Most notably, in the mentioned paper, the kernel space is defined in terms of base kernels and a grammar to combine them, instead of using the Hyper-RKHS.",
      "The mentioned paper does not reference the paper under review, unfortunately.",
      "The paper is technically sound.",
      "The authors provide a convergence analysis of the proposed method.",
      "The paper is clearly written.",
      "One unclear point that needs more discussion is the way used to select $N^2$ points for representing the hyper-kernel.",
      "The points are constructed such that they represent the kernel sufficiently well (lines 142-143), however, it is not clear how.",
      "The empirical evaluation shows promising results.",
      "There are points that need to be clarified. The paper does not provide a clear discussion of the computational/memory demands of the method.",
      "The paper does not provide a clear discussion of the efficacy w.r.t. the size of the data.",
      "The authors should provide at least rough information about the computational time needed to perform the experiments on the data used."
    ]
  },
  {
    "paper_id": "1611.08194v1",
    "submission_id": "zDtFO9vohmF",
    "submission_title": "Kernel Functional Optimisation",
    "review_id": "j52iL35p5jI",
    "input": {
      "title": "Kernel Functional Optimisation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The problem of kernel selection with non-parametric class of kernels is a subject of interest which has already been investigated.\n- The limitations of the previous works indeed motivate new methods.\n- It would have been great to have a more in-depth comparison with those works after the presentation of KFO to show that the latter does not suffer the same limitations.\n- Section 2 and 3 are clear enough.\n- The weak point of this work is the theoretical analyses of KFO.\n- Indeed, the theorems may be stated in a more precise way : recall the space of each object, highlight the main claim with a centering when possible, recall the definition of each quantity or give a precise reference with equation numbers.\n- The proofs are not easy to check mainly because they are too wordy where a sequence of equations would be easier to follow.\n- The use of previous results could also be more precise.\n- The paragraph l.206 to l.216 is one example of those two problems.\n- It would be great to discuss more the results, comparing it to previous works on non-parametric kernel selection.\n- Assumption 2 of Kirschner et al. 2019 should be recalled.\n- The experimental part seems more interesting.\n- Overall, this work has a good potential but the theoretical part should be polished.\n- Typos:\n- l.17 parameterise -> parameterize\n- l.79 symbol of average regret\n- l.241 space of f ? if f(K) is real no need for ||.||_{H_k}.",
    "review_points_list": [
      "The problem of kernel selection with non-parametric class of kernels is a subject of interest which has already been investigated.",
      "The limitations of the previous works indeed motivate new methods.",
      "It would have been great to have a more in-depth comparison with those works after the presentation of KFO to show that the latter does not suffer the same limitations.",
      "Section 2 and 3 are clear enough.",
      "The weak point of this work is the theoretical analyses of KFO.",
      "Indeed, the theorems may be stated in a more precise way : recall the space of each object, highlight the main claim with a centering when possible, recall the definition of each quantity or give a precise reference with equation numbers.",
      "The proofs are not easy to check mainly because they are too wordy where a sequence of equations would be easier to follow.",
      "The use of previous results could also be more precise.",
      "The paragraph l.206 to l.216 is one example of those two problems.",
      "It would be great to discuss more the results, comparing it to previous works on non-parametric kernel selection.",
      "Assumption 2 of Kirschner et al. 2019 should be recalled.",
      "The experimental part seems more interesting.",
      "Overall, this work has a good potential but the theoretical part should be polished.",
      "Typos:",
      "l.17 parameterise -> parameterize",
      "l.79 symbol of average regret",
      "l.241 space of f ? if f(K) is real no need for ||.||_{H_k}."
    ]
  },
  {
    "paper_id": "2011.01218v1",
    "submission_id": "vPVTsuJtGky",
    "submission_title": "Early-stopped neural networks are consistent",
    "review_id": "9MCRXqgPwyW",
    "input": {
      "title": "Early-stopped neural networks are consistent",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The proof of Theorem 1.1, as stated in line 107, lets the training iterations increase.\n- The authors refer to this as the early stopped model, but the model is more the best model obtained on the training data up to time $t.\n- However, in practice an early stopped model does not have the lowest training loss: a low training loss is avoided by early stopping, which should not interpolate all the training samples and therefore the noise that could be present in the training set.\n- Could you make the connection between the model $W_\\leq t$ and early stopping?\n- Minor nit: In the equation between the lines 41 and 42: $l(r$ should be $l(z$).",
    "review_points_list": [
      "The proof of Theorem 1.1, as stated in line 107, lets the training iterations increase.",
      "The authors refer to this as the early stopped model, but the model is more the best model obtained on the training data up to time $t.",
      "However, in practice an early stopped model does not have the lowest training loss: a low training loss is avoided by early stopping, which should not interpolate all the training samples and therefore the noise that could be present in the training set.",
      "Could you make the connection between the model $W_\\leq t$ and early stopping?",
      "Minor nit: In the equation between the lines 41 and 42: $l(r$ should be $l(z$)."
    ]
  },
  {
    "paper_id": "2011.01218v1",
    "submission_id": "vPVTsuJtGky",
    "submission_title": "Early-stopped neural networks are consistent",
    "review_id": "mv-Rm2gUTR7",
    "input": {
      "title": "Early-stopped neural networks are consistent",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper gives a very strong theoretical result regarding the impact of early-stopping on training a shallow neural network.\n- Specifically, it associates a multiplicative factor on the term R(overline U_infinity), suppose this term is smaller than eps_gd then tau_0 can be ignored and the KL divergence term is also smaller than eps_gd.\n- This recovers a result for data separable setting [Ji and Telgarsky, ICLR\u20192020] but generalizes it into the non-separable setting.\n- The technical lemma regarding comparing a neural network to a local linear approximation has been improved, rather than an additive error term as in prior works, it yields a multiplicative error term.\n- This is the key to ensure this paper to get a similar result as in data separable setting.\n- To prove the linearization approximation, it bounds an expression <nabla f(x;W)-nabla f(x;W_0), W> for all |x| <= 1.\n- This is in contrast to the standard practice where the quantity f(x;V)-f(x;W)- <nabla f(x;W), V-W> for a fixed data point x and use a union bound.\n- The technique employed in this paper is novel and from a different perspective compared to the convergence analysis of gradient descent on over-parametrized neural networks [Allen-Zhu, Li and Song, ICML\u20192019].\n- This co-VC dimension inspired technique has potentially to be used in other analysis, e.g., the convergence analysis of NTK and other variants.\n- The paper layout is a bit unusual and hard to follow.\n- It spends a very small amount of paragraphs to set up the problem tentatively, then discuss the major contributions.\n- These contributions are discussed in more details later with some of the notations left undefined until a later section.\n- Section 2 and 3 are dedicated to present the essentials of the proof.\n- It might be better to first introduce the problem background, its formal setup and some of the related literature on this problem in more detail.\n- Then present a preliminary version of the main result.\n- More details of the result can be laid out after introducing the notations.\n- It might be better to summarize and compare the results obtained in this paper with various previous works under different settings.\n- For example, this paper obtains a similar popular risk loss result for data non-separable setting as some early works addressing data separable setting.\n- Also, what\u2019s the difference between the results obtained using the co-VC dimension technique compared with the standard analysis which makes use of the fact that weights are not far away from the initialization?\n- The early-stopping discussion in this paper is very preliminary.\n- It is suggested on line 132 that this is due to technical reasons in the proof of Theorem 1.1, however, I don\u2019t see an explicit mention or discussion related to early-stopping for this Theorem.\n- A negative 1d result is also discussed as in Proposition 1.2.\n- Due to the limited paragraphs involving this topic, it is hard to understand the importance and impact of early-stopping.\n- If authors can address the above 2 and 3, and provide further clarification on the subject, I can raise the score.\n- On line 144, the definition of {cal F}_n should be f: R\u2192R since the proposition considers 1d data.\n- Some missing NTK citations:\n- Quadratic suffices for over-parametrization via matrix Chernoff bound.\n- On the Convergence Rate of Training Recurrent Neural Networks.\n- Training (over-parameterized) neural network in near linear time.",
    "review_points_list": [
      "This paper gives a very strong theoretical result regarding the impact of early-stopping on training a shallow neural network.",
      "Specifically, it associates a multiplicative factor on the term R(overline U_infinity), suppose this term is smaller than eps_gd then tau_0 can be ignored and the KL divergence term is also smaller than eps_gd.",
      "This recovers a result for data separable setting [Ji and Telgarsky, ICLR\u20192020] but generalizes it into the non-separable setting.",
      "The technical lemma regarding comparing a neural network to a local linear approximation has been improved, rather than an additive error term as in prior works, it yields a multiplicative error term.",
      "This is the key to ensure this paper to get a similar result as in data separable setting.",
      "To prove the linearization approximation, it bounds an expression <nabla f(x;W)-nabla f(x;W_0), W> for all |x| <= 1.",
      "This is in contrast to the standard practice where the quantity f(x;V)-f(x;W)- <nabla f(x;W), V-W> for a fixed data point x and use a union bound.",
      "The technique employed in this paper is novel and from a different perspective compared to the convergence analysis of gradient descent on over-parametrized neural networks [Allen-Zhu, Li and Song, ICML\u20192019].",
      "This co-VC dimension inspired technique has potentially to be used in other analysis, e.g., the convergence analysis of NTK and other variants.",
      "The paper layout is a bit unusual and hard to follow.",
      "It spends a very small amount of paragraphs to set up the problem tentatively, then discuss the major contributions.",
      "These contributions are discussed in more details later with some of the notations left undefined until a later section.",
      "Section 2 and 3 are dedicated to present the essentials of the proof.",
      "It might be better to first introduce the problem background, its formal setup and some of the related literature on this problem in more detail.",
      "Then present a preliminary version of the main result.",
      "More details of the result can be laid out after introducing the notations.",
      "It might be better to summarize and compare the results obtained in this paper with various previous works under different settings.",
      "For example, this paper obtains a similar popular risk loss result for data non-separable setting as some early works addressing data separable setting.",
      "Also, what\u2019s the difference between the results obtained using the co-VC dimension technique compared with the standard analysis which makes use of the fact that weights are not far away from the initialization?",
      "The early-stopping discussion in this paper is very preliminary.",
      "It is suggested on line 132 that this is due to technical reasons in the proof of Theorem 1.1, however, I don\u2019t see an explicit mention or discussion related to early-stopping for this Theorem.",
      "A negative 1d result is also discussed as in Proposition 1.2.",
      "Due to the limited paragraphs involving this topic, it is hard to understand the importance and impact of early-stopping.",
      "If authors can address the above 2 and 3, and provide further clarification on the subject, I can raise the score.",
      "On line 144, the definition of {cal F}_n should be f: R\u2192R since the proposition considers 1d data.",
      "Some missing NTK citations:",
      "Quadratic suffices for over-parametrization via matrix Chernoff bound.",
      "On the Convergence Rate of Training Recurrent Neural Networks.",
      "Training (over-parameterized) neural network in near linear time."
    ]
  },
  {
    "paper_id": "2011.01218v1",
    "submission_id": "vPVTsuJtGky",
    "submission_title": "Early-stopped neural networks are consistent",
    "review_id": "f_VfkVYAFXG",
    "input": {
      "title": "Early-stopped neural networks are consistent",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is very well-written, clear, and the discussion on both implications and limitations of the analysis is impeccable.\n- The main theorem is a strong contribution, showing optimality of shallow ReLU networks w.r.t. the population loss under very mild assumptions.\n- The need for early-stopping for the main result is thought-provoking, and the provided discussion on its relation to the interpolation regime and NTK draws exciting connections and open new questions for future research.\n- The complexity measure presented here is also exciting, as it seems to interact in a very desirable way with the relevant model & training factors and is also seems intuitive and is strongly connected to the NTK.\n- A key question is how much the behavior changes once actual learning is taking place, but I can see this analysis requiring considerable additional technical effort and it being appropriate to leave it completely to future work.\n- Although I haven't checked the proofs in detail, it seems that there is considerable technical contribution as well.\n- I believe this is a strong submission, with a solid contribution which is presented extremely well and raises interesting questions, therefore I recommend acceptance.\n- However, I would like to note that I am not very familiar with the topic and might have missed possible limitations / concerns.",
    "review_points_list": [
      "The paper is very well-written, clear, and the discussion on both implications and limitations of the analysis is impeccable.",
      "The main theorem is a strong contribution, showing optimality of shallow ReLU networks w.r.t. the population loss under very mild assumptions.",
      "The need for early-stopping for the main result is thought-provoking, and the provided discussion on its relation to the interpolation regime and NTK draws exciting connections and open new questions for future research.",
      "The complexity measure presented here is also exciting, as it seems to interact in a very desirable way with the relevant model & training factors and is also seems intuitive and is strongly connected to the NTK.",
      "A key question is how much the behavior changes once actual learning is taking place, but I can see this analysis requiring considerable additional technical effort and it being appropriate to leave it completely to future work.",
      "Although I haven't checked the proofs in detail, it seems that there is considerable technical contribution as well.",
      "I believe this is a strong submission, with a solid contribution which is presented extremely well and raises interesting questions, therefore I recommend acceptance.",
      "However, I would like to note that I am not very familiar with the topic and might have missed possible limitations / concerns."
    ]
  },
  {
    "paper_id": "2110.00351v2",
    "submission_id": "yxsak5ND2pA",
    "submission_title": "Smooth Normalizing Flows",
    "review_id": "jlQheYILtpo",
    "input": {
      "title": "Smooth Normalizing Flows",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The authors are motivated by tackling the challenging problem of fitting the Boltzmann distribution of complex molecules.\n- They made two significant contributions, which overcame problems previous articles had.\n- They proposed a flow architecture based on mixtures of smooth bump functions, which are C-infinity-smooth, act on a compact interval, and can easily model multimodal distributions.\n- They showed that these kinds of flows can be trained although their inverse is non-analytic by evaluating the inverse through black-box root finding and computing the gradients via the push-forward density.\n- They trained their models via force matching which makes the gradients of the transformation closer to those of the energy function of the target.\n- This stabilizes training when using the reverse Kullback-Leiber divergence as objective an allows the model to be used as potential in a molecular dynamics simulation.\n- The first contribution is novel and although force matching is a known training method, it has not been used in this context yet so the second contribution is novel as well.\n- The experimental results are impressive.\n- There should be more comparisons to competing methods.\n- For instance in section 6.3, there is no clear comparison to training without force matching.\n- They mention a competing method, i.e. stochastic normalizing flows, but they only refer to the article and do not show the results when stochastic normalizing flows are applied to the same problem.\n- Overall, the paper is well written and has clear illustrations.",
    "review_points_list": [
      "The authors are motivated by tackling the challenging problem of fitting the Boltzmann distribution of complex molecules.",
      "They made two significant contributions, which overcame problems previous articles had.",
      "They proposed a flow architecture based on mixtures of smooth bump functions, which are C-infinity-smooth, act on a compact interval, and can easily model multimodal distributions.",
      "They showed that these kinds of flows can be trained although their inverse is non-analytic by evaluating the inverse through black-box root finding and computing the gradients via the push-forward density.",
      "They trained their models via force matching which makes the gradients of the transformation closer to those of the energy function of the target.",
      "This stabilizes training when using the reverse Kullback-Leiber divergence as objective an allows the model to be used as potential in a molecular dynamics simulation.",
      "The first contribution is novel and although force matching is a known training method, it has not been used in this context yet so the second contribution is novel as well.",
      "The experimental results are impressive.",
      "There should be more comparisons to competing methods.",
      "For instance in section 6.3, there is no clear comparison to training without force matching.",
      "They mention a competing method, i.e. stochastic normalizing flows, but they only refer to the article and do not show the results when stochastic normalizing flows are applied to the same problem.",
      "Overall, the paper is well written and has clear illustrations."
    ]
  },
  {
    "paper_id": "2110.00351v2",
    "submission_id": "yxsak5ND2pA",
    "submission_title": "Smooth Normalizing Flows",
    "review_id": "MG9AwPll_5s",
    "input": {
      "title": "Smooth Normalizing Flows",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The proposed transformations are novel and the application of the inverse function theorem is neat.\n- This work clearly fills the gap of $C^k$-smooth ($k>1$) flows on intervals and hypertori and provides necessary discussion on related works in the end of section 1.\n- The paper is a complete piece and the techniques proposed (both transformations and training method) are technically sound.\n- The intuition behind each transformation is given when it is introduced, and the experiments clearly shows that the proposed method can be trained successfully while having well defined higher-order derivatives.\n- The authors mostly focus on the advantages of smooth flows in compare to spline flows but limitations are not adequately discussed (only discussion on computation cost is given).\n- The paper is well-written: the motivation, how the transformations and training are explained and how the method is evaluated are all clear and the structure makes sense.\n- While it might be due to page limit, it could be helpful to visualise each transformations (and their alternatives) when they are introduced and contrasted in section 4.\n- In section 5, it was mentioned in L245 that 'In contrast to previous work that used affine coupling layers [43], the modes are cleanly separated.' but not visualisation was shown. Again it would be helpful to see it in the paper.\n- The results clearly show on $C^k$-smoothness can help model training (section 6.4) and model usage (section 6.5).\n- As said, it is not achievable by previous methods.\n- As motivated and demonstrated by the authors, smooth flows can be widely used in modelling physics systems in which topological constraints are needed, so I can see smooth flows would be beneficial to the community especially who works on scientific modelling.\n- How would other types of generative models perform on the toy example in section 6.1, or in other experiments? For example I wonder if [1] would a strong competitor as it can learn the support and the distribution on it quite well. Extra experiments would be great to have but no necessary I believe, but it's worth some discussion on why flows are the right choice of generative models in the beginning.\n- Extra experiments would be great to have but no necessary I believe, but it's worth some discussion on why flows are the right choice of generative models in the beginning.",
    "review_points_list": [
      "The proposed transformations are novel and the application of the inverse function theorem is neat.",
      "This work clearly fills the gap of $C^k$-smooth ($k>1$) flows on intervals and hypertori and provides necessary discussion on related works in the end of section 1.",
      "The paper is a complete piece and the techniques proposed (both transformations and training method) are technically sound.",
      "The intuition behind each transformation is given when it is introduced, and the experiments clearly shows that the proposed method can be trained successfully while having well defined higher-order derivatives.",
      "The authors mostly focus on the advantages of smooth flows in compare to spline flows but limitations are not adequately discussed (only discussion on computation cost is given).",
      "The paper is well-written: the motivation, how the transformations and training are explained and how the method is evaluated are all clear and the structure makes sense.",
      "While it might be due to page limit, it could be helpful to visualise each transformations (and their alternatives) when they are introduced and contrasted in section 4.",
      "In section 5, it was mentioned in L245 that 'In contrast to previous work that used affine coupling layers [43], the modes are cleanly separated.' but not visualisation was shown. Again it would be helpful to see it in the paper.",
      "The results clearly show on $C^k$-smoothness can help model training (section 6.4) and model usage (section 6.5).",
      "As said, it is not achievable by previous methods.",
      "As motivated and demonstrated by the authors, smooth flows can be widely used in modelling physics systems in which topological constraints are needed, so I can see smooth flows would be beneficial to the community especially who works on scientific modelling.",
      "How would other types of generative models perform on the toy example in section 6.1, or in other experiments? For example I wonder if [1] would a strong competitor as it can learn the support and the distribution on it quite well. Extra experiments would be great to have but no necessary I believe, but it's worth some discussion on why flows are the right choice of generative models in the beginning.",
      "Extra experiments would be great to have but no necessary I believe, but it's worth some discussion on why flows are the right choice of generative models in the beginning."
    ]
  },
  {
    "paper_id": "2110.00351v2",
    "submission_id": "yxsak5ND2pA",
    "submission_title": "Smooth Normalizing Flows",
    "review_id": "dig_lEoLgex",
    "input": {
      "title": "Smooth Normalizing Flows",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I should begin by saying that I am not an expert in the domain-specific aspects of the paper, so a few things went over my head.\n- a few things went over my head.\n- This also means I can't be the judge of the importance of the outlined problems in the context of physical sciences.\n- Having said that, I've been able to follow the motivation behind the work --- authors have done a good job laying out the story behind why the different proposed pieces are necessary.\n- I've enjoyed reading the paper, although it's not the easiest one to wrap your head around.\n- On the first look the univariate diffeomorphism proposed in section 4 very much resembles the deep sigmoid flow by Huang et al., being a mixture of sigmoid-like functions.\n- Am I correct in understanding that the primary difference is in the proposed transformation being restricted to some interval [a, b] (e.g. [0, 1]), while also being well-behaved/smooth at the boundaries of that interval?\n- I did not fully understand the reason authors dismiss references [6] and [41] due to their 'computational costs'.\n- I am not *too* familiar with that literature and the trade-offs of associated methods, but I've seen them applied to large, high-dimensional datasets, so their cost must be at least somewhat manageable?\n- Given that the method authors propose is also 'non-analytic', at least in terms of its inverse, I wonder if these references would be worth investigating, especially given their desirable smoothness properties?\n- The convex combination of the forward and the inverse KL divergences in eq. (5) reminds me of the \u03b1-divergence: could this be a more principled way to express the desired 'distance'?\n- Section 5 is very cool, especially because the results in eqs. (8)-(11) are immediately applicable to all normalizing flows without an analytic inverse, in cases where it is necessary to evaluate the gradient of the inverse of the flow during training, which I have seen some examples of before.\n- The experiments are nice: a strong baseline (the rational-quadratic spline) is used, and the differences between it and the proposed method are clear in both the carefully-constructed toy data and real data.\n- Overall, a well-written paper with a strong motivation, plenty of technical novelty, and robust evaluation. I strongly recommend accepting it to the conference.\n- I believe the write-up, while already excellent, will benefit from the proposed clarifications.\n- Furthermore, I agree with reviewer *q1W2* that visualisations of individual 'ramp' functions, in addition to Figure 1, would be great to aid understanding.\n- After reading the author's response and the other reviews/responses, I maintain my recommendation for acceptance, although lower my score somewhat to be better calibrated with other reviews, and in recognition of the fact that even though there are multiple useful ideas in this paper, few of them are ground-breaking *by themselves*.",
    "review_points_list": [
      "I should begin by saying that I am not an expert in the domain-specific aspects of the paper, so a few things went over my head.",
      "a few things went over my head.",
      "This also means I can't be the judge of the importance of the outlined problems in the context of physical sciences.",
      "Having said that, I've been able to follow the motivation behind the work --- authors have done a good job laying out the story behind why the different proposed pieces are necessary.",
      "I've enjoyed reading the paper, although it's not the easiest one to wrap your head around.",
      "On the first look the univariate diffeomorphism proposed in section 4 very much resembles the deep sigmoid flow by Huang et al., being a mixture of sigmoid-like functions.",
      "Am I correct in understanding that the primary difference is in the proposed transformation being restricted to some interval [a, b] (e.g. [0, 1]), while also being well-behaved/smooth at the boundaries of that interval?",
      "I did not fully understand the reason authors dismiss references [6] and [41] due to their 'computational costs'.",
      "I am not *too* familiar with that literature and the trade-offs of associated methods, but I've seen them applied to large, high-dimensional datasets, so their cost must be at least somewhat manageable?",
      "Given that the method authors propose is also 'non-analytic', at least in terms of its inverse, I wonder if these references would be worth investigating, especially given their desirable smoothness properties?",
      "The convex combination of the forward and the inverse KL divergences in eq. (5) reminds me of the \u03b1-divergence: could this be a more principled way to express the desired 'distance'?",
      "Section 5 is very cool, especially because the results in eqs. (8)-(11) are immediately applicable to all normalizing flows without an analytic inverse, in cases where it is necessary to evaluate the gradient of the inverse of the flow during training, which I have seen some examples of before.",
      "The experiments are nice: a strong baseline (the rational-quadratic spline) is used, and the differences between it and the proposed method are clear in both the carefully-constructed toy data and real data.",
      "Overall, a well-written paper with a strong motivation, plenty of technical novelty, and robust evaluation. I strongly recommend accepting it to the conference.",
      "I believe the write-up, while already excellent, will benefit from the proposed clarifications.",
      "Furthermore, I agree with reviewer *q1W2* that visualisations of individual 'ramp' functions, in addition to Figure 1, would be great to aid understanding.",
      "After reading the author's response and the other reviews/responses, I maintain my recommendation for acceptance, although lower my score somewhat to be better calibrated with other reviews, and in recognition of the fact that even though there are multiple useful ideas in this paper, few of them are ground-breaking *by themselves*."
    ]
  },
  {
    "paper_id": "2110.13985v1",
    "submission_id": "yWd42CWN3c",
    "submission_title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
    "review_id": "UgcI99gqb6D",
    "input": {
      "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The experimental framework is properly designed and shows the benefits of the proposed SLLSSL approach.\n- The theoretical results are insightful; proofs are provided in Appendix and the code is provided in full. Very much appreciated!\n- Incorporating convolution operators in LSSL is novel.\n- I believe that the approach proposed here has limited novelty. In particular, the bilinear transformation which is core to the method proposed was historically introduced as dynamic causal models back in 2003, and more recently as Liquid time-constant networks (LTCs).\n- I believe authors should clearly differentiate their framework from DCMs and LTCs while including them in their experimentation. 1) I suggest authors formally state what is new in LSSL compared to LTCs? 2) The time-gating discovery of this paper is very similar to that of the liquid time-constant proposed in LTCs. Please comment. 3) The fact that authors find 5x fewer parameters than the previous SoTA was observed for networks comprised of LTCs. 4) The expressivity of the models is thoroughly discussed in LTCs whereas LSSL does not provide formal evidence. What is new here in this regard? 5) How would LTC networks equipped with HIPPO similar to the SLLSSL models perform in the experimental setup provided in the paper.\n- All ODE-based models provably suffer from gradient issues ergo learning long-term dependencies (similar to LSSL) and naturally require modules to tackle this shortcoming. The authors use HIPPO in this case. But there are many fundamental approaches proposed for solving the long-term dependency learning capabilities of RNNs and CT-RNNs that are missing in this paper. In particular, orthogonal models Lipschitz RNNs, and mixed memory architectures such as ODE-LSTMs are encouraged to be used as baselines in all experiments concerning recurrent networks.\n- [less critical] In the Intro, the authors draw a solid line between RNN models and continuous-time models which I find inaccurate. RNNs are a discretized version of CT networks. In line 65, the Authors mention the limitations of RNNs in remembering long dependencies, but in fact, all ODE-based RNNs also suffer from this limitation, because CT models are where RNNs are originally coming from.",
    "review_points_list": [
      "The experimental framework is properly designed and shows the benefits of the proposed SLLSSL approach.",
      "The theoretical results are insightful; proofs are provided in Appendix and the code is provided in full. Very much appreciated!",
      "Incorporating convolution operators in LSSL is novel.",
      "I believe that the approach proposed here has limited novelty. In particular, the bilinear transformation which is core to the method proposed was historically introduced as dynamic causal models back in 2003, and more recently as Liquid time-constant networks (LTCs).",
      "I believe authors should clearly differentiate their framework from DCMs and LTCs while including them in their experimentation. 1) I suggest authors formally state what is new in LSSL compared to LTCs? 2) The time-gating discovery of this paper is very similar to that of the liquid time-constant proposed in LTCs. Please comment. 3) The fact that authors find 5x fewer parameters than the previous SoTA was observed for networks comprised of LTCs. 4) The expressivity of the models is thoroughly discussed in LTCs whereas LSSL does not provide formal evidence. What is new here in this regard? 5) How would LTC networks equipped with HIPPO similar to the SLLSSL models perform in the experimental setup provided in the paper.",
      "All ODE-based models provably suffer from gradient issues ergo learning long-term dependencies (similar to LSSL) and naturally require modules to tackle this shortcoming. The authors use HIPPO in this case. But there are many fundamental approaches proposed for solving the long-term dependency learning capabilities of RNNs and CT-RNNs that are missing in this paper. In particular, orthogonal models Lipschitz RNNs, and mixed memory architectures such as ODE-LSTMs are encouraged to be used as baselines in all experiments concerning recurrent networks.",
      "[less critical] In the Intro, the authors draw a solid line between RNN models and continuous-time models which I find inaccurate. RNNs are a discretized version of CT networks. In line 65, the Authors mention the limitations of RNNs in remembering long dependencies, but in fact, all ODE-based RNNs also suffer from this limitation, because CT models are where RNNs are originally coming from."
    ]
  },
  {
    "paper_id": "2110.13985v1",
    "submission_id": "yWd42CWN3c",
    "submission_title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
    "review_id": "Ec7SRc98W48",
    "input": {
      "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- the paper suggests a novel reparameterization for RNNs. The work clearly extends previous work such as HiPPO. Related work is discussed thoroughly, sometimes too briefly, specifically, relation to prior work in control theory requires a more comprehensive discussion.\n- The theoretical claims are well supported. The empirical evaluation shows remarkable results compared to the baselines considered.\n- Overall, the paper is written well. The ideas presented may very well be the basis for future work. My problem here is that the bottom line is not clearly stated, rather it\u2019s implicit and a reader must go back and forth to understand the big picture. I think an implementation subsection may help make things clearer.\n- The subject of improving recurrent networks for long-time dependency is a very important topic. This paper suggests a new scheme, improving on existing ones significantly. In addition, the theoretical analysis may also lead to other parameterizations for continuous linear dynamical systems.\n- Lines 58-59 - this connection is an important part of this work, it should be included at least in the appendix, also in order to implement an arbitrary convolution with a linear dynamical system, the dimensionality of the RNN must be larger than that of the 1d convolution.\n- Lines 113-116 - something seems wrong here (perhaps one of the \u2206t is a typo), if you set \u03b1=0 you get x(t+\u2206t)=(I+\u2206t A)x(t)+\u2206tB u(t)\u2206t which can be written as x(t+\u2206t)=x(t)+\u2206t(Ax(t)+ \u2206t Bu(t)), but x\u2019(t)=Ax(t)+Bx(t).\n- Lines 119-120 - this is confusing, it is clear the paper continues with \u03b1=1/2, A and B are not a function of \u2206t, why denote them as such?\n- Lines 186-188 - this statement is misleading, the overall functions that can be expressed with deep linear RNNs are still linear in the inputs, the statement implies otherwise.\n- Theorem 1 - the citation of [8] is in italics.\n- Corollary 4.1 - quasiseparable should be defined prior to the corollary.\n- Section 5.1 - the margins are so narrow here, reporting std is a must.\n- Line 310 - shouldn\u2019t \u2206t be a scalar? why is it O(H) parameters?\n- The learning setup is not clear from the main paper, what is the eventual optimization problem that is being minimized, how is \u2206t parameterized in the learning setup. It is clear that the paper discusses special structures for A which are favorable in terms of computation but a reader is left a bit puzzled as to what is actually done in practice. Is it the GBT in equation (3) with \u03b1=1/2?",
    "review_points_list": [
      "the paper suggests a novel reparameterization for RNNs. The work clearly extends previous work such as HiPPO. Related work is discussed thoroughly, sometimes too briefly, specifically, relation to prior work in control theory requires a more comprehensive discussion.",
      "The theoretical claims are well supported. The empirical evaluation shows remarkable results compared to the baselines considered.",
      "Overall, the paper is written well. The ideas presented may very well be the basis for future work. My problem here is that the bottom line is not clearly stated, rather it\u2019s implicit and a reader must go back and forth to understand the big picture. I think an implementation subsection may help make things clearer.",
      "The subject of improving recurrent networks for long-time dependency is a very important topic. This paper suggests a new scheme, improving on existing ones significantly. In addition, the theoretical analysis may also lead to other parameterizations for continuous linear dynamical systems.",
      "Lines 58-59 - this connection is an important part of this work, it should be included at least in the appendix, also in order to implement an arbitrary convolution with a linear dynamical system, the dimensionality of the RNN must be larger than that of the 1d convolution.",
      "Lines 113-116 - something seems wrong here (perhaps one of the \u2206t is a typo), if you set \u03b1=0 you get x(t+\u2206t)=(I+\u2206t A)x(t)+\u2206tB u(t)\u2206t which can be written as x(t+\u2206t)=x(t)+\u2206t(Ax(t)+ \u2206t Bu(t)), but x\u2019(t)=Ax(t)+Bx(t).",
      "Lines 119-120 - this is confusing, it is clear the paper continues with \u03b1=1/2, A and B are not a function of \u2206t, why denote them as such?",
      "Lines 186-188 - this statement is misleading, the overall functions that can be expressed with deep linear RNNs are still linear in the inputs, the statement implies otherwise.",
      "Theorem 1 - the citation of [8] is in italics.",
      "Corollary 4.1 - quasiseparable should be defined prior to the corollary.",
      "Section 5.1 - the margins are so narrow here, reporting std is a must.",
      "Line 310 - shouldn\u2019t \u2206t be a scalar? why is it O(H) parameters?",
      "The learning setup is not clear from the main paper, what is the eventual optimization problem that is being minimized, how is \u2206t parameterized in the learning setup. It is clear that the paper discusses special structures for A which are favorable in terms of computation but a reader is left a bit puzzled as to what is actually done in practice. Is it the GBT in equation (3) with \u03b1=1/2?"
    ]
  },
  {
    "paper_id": "2110.13985v1",
    "submission_id": "yWd42CWN3c",
    "submission_title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
    "review_id": "wxVZbmDIaQc",
    "input": {
      "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The proposed model seems novel, computationally and numerically efficient, and well motivated by theoretical considerations.\n- The extent of the contributions is significant and may be of broad interest for the NeurIPS community.\n- The versatile abilities of MLSSL and SLLSSL and their thorough analysis further strengthen the contribution.\n- The claims of the paper are validated by a strong set of experiments with a provided code that seems complete and runnable, showing the clear advantage of MLSSL and SLLSSL to tackle long sequences by establishing state-of-the-art results on challenging tasks with reduced number of parameters and computational cost.\n- This paper is too long and dense, with a long appendix, to adequately review given the short allocated reviewing time and high number of papers to review.\n- I appreciate the efforts of the authors to provide a strong and well motivated paper, but I am not sure whether the NeurIPS reviewing system can assimilate such papers.\n- Nonetheless, I would have some questions for the authors.\n- The main issue of the paper as I understand it is how the learned timesteps are handled with respect to the input time series temporality.\n- In Lemma 3.1, the learned $\\Delta t$ most likely does not correspond to the actual temporal discrepancy between $x_t$ and $x_{t+1}$ (i.e., $\\Delta t \\neq 1$).\n- This decorrelates the step size $\\Delta t$ and the temporality of $x$ from the temporality of the input $u$, at least when following the dynamical system interpretation of Equation (3).\n- Could the authors clarify this point, and in particular explain how this does not partly invalidate the dynamical viewpoint of LSSL?\n- To the best of my understanding, all considered tasks in the experiments of the paper deal with classification and regression to scalar values.\n- This is unfortunate, given that the proposed architecture is a sequence-to-sequence model.\n- Could any sequence-to-sequence task may be considered?\n- The claim of lines 57-58 that LSSL generalizes convolutions and CNNs should be corrected, as it is shown in Section 3.2 that it can only approximate them, which is substantially different.\n- This submission can be difficult to read for the uninitiated reader given the information density and high number of introduced notions.\n- Nonetheless, the claims and results presented by the authors are clear and well explained throughout the paper, especially as the submission is fully self-contained thanks to a thorough appendix.\n- While the submission externally seems solid, I am unable to commit to a strong acceptation recommendation due to the aforementioned inherent time constraints of the NeurIPS reviewing system.\n- Therefore, and partly because of the concerns expressed above, I am only suggesting a 'Marginally above the acceptance threshold', but I am strongly willing to change my score following the authors' response and discussion phase with the other reviewers.\n- I acknowledge the authors' response and thank them for their answers.\n- The responses to all reviewers are thorough and seem to tackle the raised concerns, especially regarding readability and positioning.\n- As far as I am concerned, my concerns are appropriately addressed.\n- In particular, I believe that the solutions proposed by the authors will improve the quality of the submission.\n- Within the already exposed limits of my appreciation, I think that this paper can be accepted, thus raising my score to 'Good paper, accept' but keeping my low confidence rating.\n- Following the authors' convincing responses and further investigation, I still think that this paper should be accepted but with a higher confidence.\n- I consequently choose to raise my confidence score.",
    "review_points_list": [
      "The proposed model seems novel, computationally and numerically efficient, and well motivated by theoretical considerations.",
      "The extent of the contributions is significant and may be of broad interest for the NeurIPS community.",
      "The versatile abilities of MLSSL and SLLSSL and their thorough analysis further strengthen the contribution.",
      "The claims of the paper are validated by a strong set of experiments with a provided code that seems complete and runnable, showing the clear advantage of MLSSL and SLLSSL to tackle long sequences by establishing state-of-the-art results on challenging tasks with reduced number of parameters and computational cost.",
      "This paper is too long and dense, with a long appendix, to adequately review given the short allocated reviewing time and high number of papers to review.",
      "I appreciate the efforts of the authors to provide a strong and well motivated paper, but I am not sure whether the NeurIPS reviewing system can assimilate such papers.",
      "Nonetheless, I would have some questions for the authors.",
      "The main issue of the paper as I understand it is how the learned timesteps are handled with respect to the input time series temporality.",
      "In Lemma 3.1, the learned $\\Delta t$ most likely does not correspond to the actual temporal discrepancy between $x_t$ and $x_{t+1}$ (i.e., $\\Delta t \\neq 1$).",
      "This decorrelates the step size $\\Delta t$ and the temporality of $x$ from the temporality of the input $u$, at least when following the dynamical system interpretation of Equation (3).",
      "Could the authors clarify this point, and in particular explain how this does not partly invalidate the dynamical viewpoint of LSSL?",
      "To the best of my understanding, all considered tasks in the experiments of the paper deal with classification and regression to scalar values.",
      "This is unfortunate, given that the proposed architecture is a sequence-to-sequence model.",
      "Could any sequence-to-sequence task may be considered?",
      "The claim of lines 57-58 that LSSL generalizes convolutions and CNNs should be corrected, as it is shown in Section 3.2 that it can only approximate them, which is substantially different.",
      "This submission can be difficult to read for the uninitiated reader given the information density and high number of introduced notions.",
      "Nonetheless, the claims and results presented by the authors are clear and well explained throughout the paper, especially as the submission is fully self-contained thanks to a thorough appendix.",
      "While the submission externally seems solid, I am unable to commit to a strong acceptation recommendation due to the aforementioned inherent time constraints of the NeurIPS reviewing system.",
      "Therefore, and partly because of the concerns expressed above, I am only suggesting a 'Marginally above the acceptance threshold', but I am strongly willing to change my score following the authors' response and discussion phase with the other reviewers.",
      "I acknowledge the authors' response and thank them for their answers.",
      "The responses to all reviewers are thorough and seem to tackle the raised concerns, especially regarding readability and positioning.",
      "As far as I am concerned, my concerns are appropriately addressed.",
      "In particular, I believe that the solutions proposed by the authors will improve the quality of the submission.",
      "Within the already exposed limits of my appreciation, I think that this paper can be accepted, thus raising my score to 'Good paper, accept' but keeping my low confidence rating.",
      "Following the authors' convincing responses and further investigation, I still think that this paper should be accepted but with a higher confidence.",
      "I consequently choose to raise my confidence score."
    ]
  },
  {
    "paper_id": "2110.13985v1",
    "submission_id": "yWd42CWN3c",
    "submission_title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
    "review_id": "UZg8z3S6aPt",
    "input": {
      "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I think the idea is very interesting and authors have a done descent job in presenting details.\n- However, my main concern with this paper is readability.\n- The 9 pages submitted as the main paper were not sufficient for me to understand the whole content and answer some of my basic questions like: 1. Dimension of y(t):\n- In the model definition, y is one dimensional, which is not necessarily that appealing for many deep neural architectures where intermediate dimensions might have different dimensions.\n- However, in appendix A.4, this is generalized to M > 1.\n- 2. What is an example of coefficients A, B, C and D?\n- The difficulty of training A has been mentioned multiple times in the paper but A is introduced for the first time in appendix A.2, line 521.\n- 3. Same argument with delta t.\n- While it plays a really important role in equations and discussions, there are not many details about it until reading appendices.\n- 4. Reading the first 9 pages and experiments, I did not have any clue how the stack of LSSL look like.\n- Section A.4 was needed.\n- And after that, I am guessing the class of RNNs discussed here are the ones with non-linearity on depth and not on time.\n- If that's right, I am not sure if the sentencing of the theorems necessarily reflect this point?\n- 5. Authors seems have used many existing heuristic techniques like residual connections, pre-norm, post-norm, ...\n- And I am wondering if using these will have any impact on the formulation?\n- For example, can we use the residual layer and still analyze our model with a linear left-to-right system in Eq 1 and 2?\n- 6. Experiment design is not reflecting the advantages of this model.\n- - I enjoyed reading the arguments about modeling long sequences.\n- However, all experiments are some empirical performance comparison and since the architectural details are not clear, it is not clear to me if the gains are because this model is more capable of modeling longer sequences or maybe benefited from some architectural detail choices?\n- I was hoping to see some advantage of using LSSL, one can always train an LSTM network and tune all parameters to get state-of-the-art, what LSSL provides that make it better than the simple tuning ?\n- - For CelebA task, I don't understand the reasoning",
    "review_points_list": [
      "I think the idea is very interesting and authors have a done descent job in presenting details.",
      "However, my main concern with this paper is readability.",
      "The 9 pages submitted as the main paper were not sufficient for me to understand the whole content and answer some of my basic questions like: 1. Dimension of y(t):",
      "In the model definition, y is one dimensional, which is not necessarily that appealing for many deep neural architectures where intermediate dimensions might have different dimensions.",
      "However, in appendix A.4, this is generalized to M > 1.",
      "2. What is an example of coefficients A, B, C and D?",
      "The difficulty of training A has been mentioned multiple times in the paper but A is introduced for the first time in appendix A.2, line 521.",
      "3. Same argument with delta t.",
      "While it plays a really important role in equations and discussions, there are not many details about it until reading appendices.",
      "4. Reading the first 9 pages and experiments, I did not have any clue how the stack of LSSL look like.",
      "Section A.4 was needed.",
      "And after that, I am guessing the class of RNNs discussed here are the ones with non-linearity on depth and not on time.",
      "If that's right, I am not sure if the sentencing of the theorems necessarily reflect this point?",
      "5. Authors seems have used many existing heuristic techniques like residual connections, pre-norm, post-norm, ...",
      "And I am wondering if using these will have any impact on the formulation?",
      "For example, can we use the residual layer and still analyze our model with a linear left-to-right system in Eq 1 and 2?",
      "6. Experiment design is not reflecting the advantages of this model.",
      "- I enjoyed reading the arguments about modeling long sequences.",
      "However, all experiments are some empirical performance comparison and since the architectural details are not clear, it is not clear to me if the gains are because this model is more capable of modeling longer sequences or maybe benefited from some architectural detail choices?",
      "I was hoping to see some advantage of using LSSL, one can always train an LSTM network and tune all parameters to get state-of-the-art, what LSSL provides that make it better than the simple tuning ?",
      "- For CelebA task, I don't understand the reasoning"
    ]
  },
  {
    "paper_id": "2106.03400v2",
    "submission_id": "yNzF41lHYV",
    "submission_title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
    "review_id": "3hW_Dpfxk9q",
    "input": {
      "title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Overall, the proposed method is well-motivated by theoretical results and appears to have strong experimental evidence, but there is a concern about experimental protocol and there are various claims that need more justification and clarification.\n- The proposed reformulation of offline RL using the viewpoint of convex optimization appears to be novel, given the accuracy of the summary of related work.\n- This motivates the proposed method, ICQ, to avoid the use of unseen state-action pairs, which would have contributed to error in previous methods.\n- The improvement due to avoiding the use of unseen pairs is shown in a didactic finite game, and further validated in higher dimensional environments.\n- Theorems 1 and 2 are useful for motivating the method.\n- I am slightly concerned about the author's claim in line 165 that the importance sampling weight does not involve \u03c0.\n- Where does this paper demonstrate it?",
    "review_points_list": [
      "Overall, the proposed method is well-motivated by theoretical results and appears to have strong experimental evidence, but there is a concern about experimental protocol and there are various claims that need more justification and clarification.",
      "The proposed reformulation of offline RL using the viewpoint of convex optimization appears to be novel, given the accuracy of the summary of related work.",
      "This motivates the proposed method, ICQ, to avoid the use of unseen state-action pairs, which would have contributed to error in previous methods.",
      "The improvement due to avoiding the use of unseen pairs is shown in a didactic finite game, and further validated in higher dimensional environments.",
      "Theorems 1 and 2 are useful for motivating the method.",
      "I am slightly concerned about the author's claim in line 165 that the importance sampling weight does not involve \u03c0.",
      "Where does this paper demonstrate it?"
    ]
  },
  {
    "paper_id": "2106.03400v2",
    "submission_id": "yNzF41lHYV",
    "submission_title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
    "review_id": "pMl3h_etUat",
    "input": {
      "title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is well-motivated - the problem of offline learning from a static dataset is well known and its challenges have been studied in terms of unseen actions during value-function training.\n- The authors also state in the appendix that the coordinated setting is a limitation, but this is a reasonable starting point for this work.\n- The results and experiments are extensive and very convincing.\n- I also appreciate the simplification of the algorithm over the BCQ baseline, which requires training an additional generative model and perturbation model.\n- My main questions are around the derivation of the forward KL objective.\n- Specifically, I am not clear on what the target of the normalization constant should be.\n- It is also not clear from the B.4 section of the appendix, and I would like to have seen a more detailed derivation from equations 31 to 32.\n- Other works that derive a policy optimization objective from the forward KL, the partition function is computed by integrating over actions, for example in the M-step of MPO algorithm (Abdolmaleki 2018).\n- In the practical algorithm, this is approximated by sampling several actions for a given state and taking the softmax.\n- In this paper, however, I am not sure what the normalizing constant should be capturing.\n- In the practical algorithm, it seems you are taking a softmax accross the batch dimension as you state, which I interpret as integrating over actions and states.\n- However, the motivation behind this is confusing to me without seeing more detailed steps.\n- Finally, this raises a question for me in terms of the practical consideration of batch-size.\n- I imagine that given the weights are computed as a softmax operation on the minibatch Q-values, this would mean that small batch sizes are preferable for stronger learning signal.\n- It would be interesting to see this explored and/or expanded on.\n- This seems novel as far as I know.\n- There are some works that utilize the forward KL in RL whereas most policy gradient objectives can be viewed as derived from the reverse KL.\n- However, this is a specific case of utilizing it in order to avoid sampling from the trained policy for the updates, which seems very clever.\n- Tuning the alpha parameter between 0 and \u221e interpolates between Q-learning and behavior cloning, but an intermediate value works best for high reward.\n- This is a dense paper, but there are several ablation studies and intuitions given for the implementation details.\n- The experiments on the multi-agent settings are especially extensive and impressive.\n- Offline RL is an important topic in RL, and that this is able to outperform existing baselines in the single agent case is already impressive to see.\n- The significance is greatly improved given the results in the multi-agent settings.\n- There is a minor typo - the perturbation model takes \u03c4' instead of \u03c4 in the Bellman target.\n- CQL (Conservative Q-Learning for Offline RL) is not cited but used as a baseline.\n- I would also discuss how this is a much simpler version of the existing methods, namely CQL and BCQ.\n- Effectively, by applying the constraint implicitly, the authors avoid an additional regularization term (as most offline RL papers use to enforce similarity to the behavior policy) or even training a separate network as BCQ does.",
    "review_points_list": [
      "The paper is well-motivated - the problem of offline learning from a static dataset is well known and its challenges have been studied in terms of unseen actions during value-function training.",
      "The authors also state in the appendix that the coordinated setting is a limitation, but this is a reasonable starting point for this work.",
      "The results and experiments are extensive and very convincing.",
      "I also appreciate the simplification of the algorithm over the BCQ baseline, which requires training an additional generative model and perturbation model.",
      "My main questions are around the derivation of the forward KL objective.",
      "Specifically, I am not clear on what the target of the normalization constant should be.",
      "It is also not clear from the B.4 section of the appendix, and I would like to have seen a more detailed derivation from equations 31 to 32.",
      "Other works that derive a policy optimization objective from the forward KL, the partition function is computed by integrating over actions, for example in the M-step of MPO algorithm (Abdolmaleki 2018).",
      "In the practical algorithm, this is approximated by sampling several actions for a given state and taking the softmax.",
      "In this paper, however, I am not sure what the normalizing constant should be capturing.",
      "In the practical algorithm, it seems you are taking a softmax accross the batch dimension as you state, which I interpret as integrating over actions and states.",
      "However, the motivation behind this is confusing to me without seeing more detailed steps.",
      "Finally, this raises a question for me in terms of the practical consideration of batch-size.",
      "I imagine that given the weights are computed as a softmax operation on the minibatch Q-values, this would mean that small batch sizes are preferable for stronger learning signal.",
      "It would be interesting to see this explored and/or expanded on.",
      "This seems novel as far as I know.",
      "There are some works that utilize the forward KL in RL whereas most policy gradient objectives can be viewed as derived from the reverse KL.",
      "However, this is a specific case of utilizing it in order to avoid sampling from the trained policy for the updates, which seems very clever.",
      "Tuning the alpha parameter between 0 and \u221e interpolates between Q-learning and behavior cloning, but an intermediate value works best for high reward.",
      "This is a dense paper, but there are several ablation studies and intuitions given for the implementation details.",
      "The experiments on the multi-agent settings are especially extensive and impressive.",
      "Offline RL is an important topic in RL, and that this is able to outperform existing baselines in the single agent case is already impressive to see.",
      "The significance is greatly improved given the results in the multi-agent settings.",
      "There is a minor typo - the perturbation model takes \u03c4' instead of \u03c4 in the Bellman target.",
      "CQL (Conservative Q-Learning for Offline RL) is not cited but used as a baseline.",
      "I would also discuss how this is a much simpler version of the existing methods, namely CQL and BCQ.",
      "Effectively, by applying the constraint implicitly, the authors avoid an additional regularization term (as most offline RL papers use to enforce similarity to the behavior policy) or even training a separate network as BCQ does."
    ]
  },
  {
    "paper_id": "2106.03400v2",
    "submission_id": "yNzF41lHYV",
    "submission_title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
    "review_id": "u7Sen8QrJ6b",
    "input": {
      "title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- 1. The algorithm seems to be novel and differs from previous comparable approaches like CoreSet or BADGE. Related work is adequately cited.\n- 2. The authors show empirically that their algorithm, Cluster-Margin, is both more efficient (O(nlog(n) vs O(n\u221a(n)) than CoreSet and BADGE in practice and more effective.\n- 3. The algorithm clearly outperforms CoreSet, BADGE, Margin and Random on the Open Images dataset. The algorithm requires 29% less labels than the second-best model in the 100k batch-size setting and 60% less labels in the 1M batch-size setting to achieve the same result (mAP).\n- 4. Cluster-Margin also outperforms all other methods on CIFAR10, CIFAR100 and obtains a similar performance on SVHN.\n- 5. The authors establish a theoretical guarantee for the Cluster-MarginV algorithm and show that those results hold for the Cluster-Margin algorithm in specific settings.\n- 6. The Cluster-MarginV algorithm has a label complexity bound which improves over the Margin algorithm by a factor beta.\n- 7. This improvement is possible, under specific hypotheses like an optimal volume-based sampler, when the dimensionality of the embedding space is small or when the batch size k is large.\n- 8. The optimal volume-based sampler is approximately equivalent to the Cluster-Margin algorithm.\n- 9. log(k) is an upper bound on the improvement of query complexity for any sampler.\n- 10. The authors are aware and mention that their theoretical results are initial and that equating volume based samplers and the Cluster-Margin algorithm is an open research question.\n- 11. The paper is very clear and well organized. The authors detail the hyper-parameters and compute details used for the experiments. The Cluster-Margin algorithm is also explained in detail.\n- 12. The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods. The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm.",
    "review_points_list": [
      "1. The algorithm seems to be novel and differs from previous comparable approaches like CoreSet or BADGE. Related work is adequately cited.",
      "2. The authors show empirically that their algorithm, Cluster-Margin, is both more efficient (O(nlog(n) vs O(n\u221a(n)) than CoreSet and BADGE in practice and more effective.",
      "3. The algorithm clearly outperforms CoreSet, BADGE, Margin and Random on the Open Images dataset. The algorithm requires 29% less labels than the second-best model in the 100k batch-size setting and 60% less labels in the 1M batch-size setting to achieve the same result (mAP).",
      "4. Cluster-Margin also outperforms all other methods on CIFAR10, CIFAR100 and obtains a similar performance on SVHN.",
      "5. The authors establish a theoretical guarantee for the Cluster-MarginV algorithm and show that those results hold for the Cluster-Margin algorithm in specific settings.",
      "6. The Cluster-MarginV algorithm has a label complexity bound which improves over the Margin algorithm by a factor beta.",
      "7. This improvement is possible, under specific hypotheses like an optimal volume-based sampler, when the dimensionality of the embedding space is small or when the batch size k is large.",
      "8. The optimal volume-based sampler is approximately equivalent to the Cluster-Margin algorithm.",
      "9. log(k) is an upper bound on the improvement of query complexity for any sampler.",
      "10. The authors are aware and mention that their theoretical results are initial and that equating volume based samplers and the Cluster-Margin algorithm is an open research question.",
      "11. The paper is very clear and well organized. The authors detail the hyper-parameters and compute details used for the experiments. The Cluster-Margin algorithm is also explained in detail.",
      "12. The results are important as the algorithm allows for more efficient and effective large-batch-size active learning compared to existing methods. The authors also provide initial theoretical guarantees to explain the improvements obtained with the Cluster-Margin algorithm."
    ]
  },
  {
    "paper_id": "2111.15077v2",
    "submission_id": "yH2VrkpiCK6",
    "submission_title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
    "review_id": "9lgfyuA_1rq",
    "input": {
      "title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- UDA is a highly explored area and new contributions should introduce novel ideas that can address existing challenges or boost state-of-the-art performances significantly.\n- More specifically, the idea of using prototypes for UDA has been explored extensively in the literature.\n- Transport-based optimization also has been used extensively for this purpose.\n- Source-free UDA algorithms based on using prototypes have been developed as well, e.g.,: Universal domain adaptation through self-supervision, NeurIPS 2020, and Unsupervised model adaptation for continual semantic segmentation, AAAI 2021.\n- It is true the proposed algorithm is different in the idea implementation details but the broad solution is not novel due to the precedent.\n- Performance results also do not demonstrate a significant boost over prior works.\n- Finally, there is no theoretical analysis to improve our understanding about using a similar approach to address UDA.\n- Hence, I think this work is not suitable for publication at NeurIPS.\n- Recent UDA algorithms perform more extensive experiments to demonstrate the generalizability of the method and offer benefits over prior works.\n- In this aspect, I think the experiments need significant improvement: results on some important benchmarks, including ImageCLEF, cross-domain digits, Office-Caltech, etc, are missing.\n- Multi-source and source-private experiments are even less extensive in terms of both used datasets and comparison with prior works.\n- Comparison has been done against mostly methods that have not been developed in the past three years. Please check the literature to include the existing state-of-the-art performance.\n- An ablative study on Eq (7) is helpful to determine the relative importance of the proposed terms.\n- In lines 49-55, it is claimed that the proposed approach is advantageous compared to adversarial learning because of robustness and faster convergence properties.\n- Experiments are based on using test error versus learning iteration to demonstrate this claim.\n- But I think this is not fully convincing.\n- Comparisons should be performed based on time because the algorithms are different in terms of computations at each iteration.\n- A second set of experiments to demonstrate robustness is going to be very helpful.\n- After more elaboration, I finalized my updated rating and decided to maintain it.\n- I think this work is best suited for future venues after incorporating all the review feedback.",
    "review_points_list": [
      "UDA is a highly explored area and new contributions should introduce novel ideas that can address existing challenges or boost state-of-the-art performances significantly.",
      "More specifically, the idea of using prototypes for UDA has been explored extensively in the literature.",
      "Transport-based optimization also has been used extensively for this purpose.",
      "Source-free UDA algorithms based on using prototypes have been developed as well, e.g.,: Universal domain adaptation through self-supervision, NeurIPS 2020, and Unsupervised model adaptation for continual semantic segmentation, AAAI 2021.",
      "It is true the proposed algorithm is different in the idea implementation details but the broad solution is not novel due to the precedent.",
      "Performance results also do not demonstrate a significant boost over prior works.",
      "Finally, there is no theoretical analysis to improve our understanding about using a similar approach to address UDA.",
      "Hence, I think this work is not suitable for publication at NeurIPS.",
      "Recent UDA algorithms perform more extensive experiments to demonstrate the generalizability of the method and offer benefits over prior works.",
      "In this aspect, I think the experiments need significant improvement: results on some important benchmarks, including ImageCLEF, cross-domain digits, Office-Caltech, etc, are missing.",
      "Multi-source and source-private experiments are even less extensive in terms of both used datasets and comparison with prior works.",
      "Comparison has been done against mostly methods that have not been developed in the past three years. Please check the literature to include the existing state-of-the-art performance.",
      "An ablative study on Eq (7) is helpful to determine the relative importance of the proposed terms.",
      "In lines 49-55, it is claimed that the proposed approach is advantageous compared to adversarial learning because of robustness and faster convergence properties.",
      "Experiments are based on using test error versus learning iteration to demonstrate this claim.",
      "But I think this is not fully convincing.",
      "Comparisons should be performed based on time because the algorithms are different in terms of computations at each iteration.",
      "A second set of experiments to demonstrate robustness is going to be very helpful.",
      "After more elaboration, I finalized my updated rating and decided to maintain it.",
      "I think this work is best suited for future venues after incorporating all the review feedback."
    ]
  },
  {
    "paper_id": "2111.15077v2",
    "submission_id": "yH2VrkpiCK6",
    "submission_title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
    "review_id": "-5OIfYgqeAy",
    "input": {
      "title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- As suggested in the main contribution, the author claims the originality of using learning parameters for class prototypes and bidirectional prototype-oriented transport loss.\n- I doubt the originality since the below paper also utilizes the learnable parameters to construct the class prototype for labeled data.\n- - Semi-supervised Domain Adaptation via Minimax Entropy, Saito et al.\n- It would be more convincing if the author can prove the originality and show the main difference from the above paper.\n- The motivation of using learnable parameters for class prototype and technical approach using prototype-oriented transport loss seems to be reasonable.\n- It is supported with various unsupervised domain adaptation settings (one source, multi-source, source-data-private).\n- Furthermore, explicit comparisons on the number of parameters and converging speed prove its effectiveness on practical domain adaptation.\n- Comparison with proposed baselines seems to be insufficient.\n- Specifically, the proposed method needs to be compared with other prototype-based alignment methods.\n- - Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al.\n- - Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation, Yue et al.\n- It is easy to follow and well-written.\n- The paper contains well-supported theoretical analysis and experiments.\n- Suggesting the framework that requires no additional model parameters and converges fast seems to be applied in real-world domain adaptation problems.",
    "review_points_list": [
      "As suggested in the main contribution, the author claims the originality of using learning parameters for class prototypes and bidirectional prototype-oriented transport loss.",
      "I doubt the originality since the below paper also utilizes the learnable parameters to construct the class prototype for labeled data.",
      "- Semi-supervised Domain Adaptation via Minimax Entropy, Saito et al.",
      "It would be more convincing if the author can prove the originality and show the main difference from the above paper.",
      "The motivation of using learnable parameters for class prototype and technical approach using prototype-oriented transport loss seems to be reasonable.",
      "It is supported with various unsupervised domain adaptation settings (one source, multi-source, source-data-private).",
      "Furthermore, explicit comparisons on the number of parameters and converging speed prove its effectiveness on practical domain adaptation.",
      "Comparison with proposed baselines seems to be insufficient.",
      "Specifically, the proposed method needs to be compared with other prototype-based alignment methods.",
      "- Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al.",
      "- Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation, Yue et al.",
      "It is easy to follow and well-written.",
      "The paper contains well-supported theoretical analysis and experiments.",
      "Suggesting the framework that requires no additional model parameters and converges fast seems to be applied in real-world domain adaptation problems."
    ]
  },
  {
    "paper_id": "2111.15077v2",
    "submission_id": "yH2VrkpiCK6",
    "submission_title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
    "review_id": "dNQLg2QOGM_",
    "input": {
      "title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is well organized and written. The approach is well motivated.\n- The source-data-private setting is practical.\n- I especially liked the discussion around the use of classifier weights as prototypes, as compared to taking feature mean.\n- The results look promising.\n- The analysis in Section 4 (and Appendix) is comprehensive.\n- The visualizations in Appendix Section E are interesting.\n- Once the model is trained on the source domain, the prototypes are kept fixed.\n- This would enforce the target domain to acquire the semantics learned by the source domain.\n- I agree that this would lead to a more stable training, however, this restriction may not be suitable in category-shift scenarios (e.g., see [P2]).\n- Could the authors comment on this aspect?\n- There are several source-data free approaches in the literature [P1, P2] that have been missed in the related work.\n- In particular, [P2] also uses a prototype based metadata to perform \u201csource-free\u201d adaptation, however recommends using separate latent spaces for source and target domain. IMO, these should be discussed in the related work since ideas such as source-data-private setting, and compact feature clusters are already described in these works.\n- It is unclear what is meant by the term \u201cmissing classes\u201d here.\n- Does it imply that the method is applicable under category-shift scenario as well (e.g. open-set domain adaptation)?\n- Classical DA methods work under the theoretical framework of [5] where the adapted model\u2019s generalization bound depends on three terms (source-domain error, divergence between the domains, a constant that depends on the chosen hypothesis space). Here however, in the source-private-data setting, the source domain error becomes immaterial since the feature extractor is updated solely based on the target domain. It would be good to see some theoretical discussion on why the proposed method works (since the adapted feature extractor would not be ideal for the source domain). An interesting analysis could be analyzing the performance of source domain data on the adapted model.\n- Please fix typos (e.g. Fig. 2 caption, \u201cimabalance\u201d). Avoid single worded lines (e.g. Table 5 caption has the word Resnet50 consuming an entire line).\n- Fig. 1b is not very informative/novel. This could be removed and the space obtained could be used to plot visualizations shown in Appendix (esp. Class proportion estimation).",
    "review_points_list": [
      "The paper is well organized and written. The approach is well motivated.",
      "The source-data-private setting is practical.",
      "I especially liked the discussion around the use of classifier weights as prototypes, as compared to taking feature mean.",
      "The results look promising.",
      "The analysis in Section 4 (and Appendix) is comprehensive.",
      "The visualizations in Appendix Section E are interesting.",
      "Once the model is trained on the source domain, the prototypes are kept fixed.",
      "This would enforce the target domain to acquire the semantics learned by the source domain.",
      "I agree that this would lead to a more stable training, however, this restriction may not be suitable in category-shift scenarios (e.g., see [P2]).",
      "Could the authors comment on this aspect?",
      "There are several source-data free approaches in the literature [P1, P2] that have been missed in the related work.",
      "In particular, [P2] also uses a prototype based metadata to perform \u201csource-free\u201d adaptation, however recommends using separate latent spaces for source and target domain. IMO, these should be discussed in the related work since ideas such as source-data-private setting, and compact feature clusters are already described in these works.",
      "It is unclear what is meant by the term \u201cmissing classes\u201d here.",
      "Does it imply that the method is applicable under category-shift scenario as well (e.g. open-set domain adaptation)?",
      "Classical DA methods work under the theoretical framework of [5] where the adapted model\u2019s generalization bound depends on three terms (source-domain error, divergence between the domains, a constant that depends on the chosen hypothesis space). Here however, in the source-private-data setting, the source domain error becomes immaterial since the feature extractor is updated solely based on the target domain. It would be good to see some theoretical discussion on why the proposed method works (since the adapted feature extractor would not be ideal for the source domain). An interesting analysis could be analyzing the performance of source domain data on the adapted model.",
      "Please fix typos (e.g. Fig. 2 caption, \u201cimabalance\u201d). Avoid single worded lines (e.g. Table 5 caption has the word Resnet50 consuming an entire line).",
      "Fig. 1b is not very informative/novel. This could be removed and the space obtained could be used to plot visualizations shown in Appendix (esp. Class proportion estimation)."
    ]
  },
  {
    "paper_id": "2111.15077v2",
    "submission_id": "yH2VrkpiCK6",
    "submission_title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
    "review_id": "zkDjXP3E4ck",
    "input": {
      "title": "A Prototype-Oriented Framework for Unsupervised Domain Adaptation",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Their way of using prototypes (using last layers' weights) and formulation of CE resembles supervised contrastive learning(SupCon)[1]. However, this is just one part of their model, and further formulating the alignment in a two way conditional distributional setting and estimating class priors together is novel as of my knowledge.",
    "review_points_list": [
      "Their way of using prototypes (using last layers' weights) and formulation of CE resembles supervised contrastive learning(SupCon)[1]. However, this is just one part of their model, and further formulating the alignment in a two way conditional distributional setting and estimating class priors together is novel as of my knowledge."
    ]
  },
  {
    "paper_id": "2106.12242v2",
    "submission_id": "vMWHOumNj5",
    "submission_title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
    "review_id": "cIfT-YRFeQ",
    "input": {
      "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The connection between Blackwell approachability and online learning has been made before.\n- However, this paper also considers various fairness objectives, more specifically group-wise calibration/regret, and fairness constraints such as statistical parity and equalized average payoffs.\n- It is novel to use the Blackwell approachability result to see the compatibility of different objectives and constraints.\n- Notations developed here are pretty precise (some comments on notations below though),\n- and the paper is mostly written clearly.\n- However, there are some parts where some of the details are not clear (e.g. comment below on estimating the unknown set in section 5).\n- Also, it would be helpful to have the contribution of this paper more expounded upon especially in relations to the other related work.\n- For instance, it has been known how to leverage blackwell approachability to derive online calibration, and Gupta et al shows using techniques similar to blackwell approachability how to achieve online multicalibration (groupwise calibration).\n- These kinds of discussions seem to be missing.\n- The framework (looking at online learning problem via Blackwell Approachability) is significant in that even for any new fairness objectives/constraints in the online learning setting, one can immediately see whether it's compatible and and immediately get an algorithm, if compatible.\n- In terms of the impossibility results, the original Blackwell approachability provides that the set is approachable if and only if the condition that resembles best response condition (i.e. no matter what the the other player plays, you can land in the set) is met.\n- Using this idea, the paper, for instance, shows that the set that corresponds to group-wise no-regret is not approachable.\n- But shouldn't the final statement be regarding online learnability \u2014 that is, doesn't one need to show that if a set is not Blackwell-approachable, then the corresponding learning problem isn't possible?\n- It seems like one would need to invoke the equivalence result of 'Blackwell Approachability and No-Regret Learning are Equivalent' by Abernethy et al?\n- In section 6, it's not clear to me how \\hat{C}_r is constructed to estimate the unknown set C.\n- section 2.1, objective 1: N is never defined; I'm assuming it should be N = |A| I think?\n- I think the dependence on |S| here will be linear.\n- I wonder whether it's be possible to achieve logarithmic dependence on |S| using the trick in Gupta (using the exponential surrogate loss).\n- In Theorem 1, d^2_T hasn't been defined.\n- I'm assuming it's min_{v \notin C} ||m_T - v|| the rate of convergence toward the set you are trying to approach to?\n- Thanks for answering my questions.\n- I recommend acceptance!",
    "review_points_list": [
      "The connection between Blackwell approachability and online learning has been made before.",
      "However, this paper also considers various fairness objectives, more specifically group-wise calibration/regret, and fairness constraints such as statistical parity and equalized average payoffs.",
      "It is novel to use the Blackwell approachability result to see the compatibility of different objectives and constraints.",
      "Notations developed here are pretty precise (some comments on notations below though),",
      "and the paper is mostly written clearly.",
      "However, there are some parts where some of the details are not clear (e.g. comment below on estimating the unknown set in section 5).",
      "Also, it would be helpful to have the contribution of this paper more expounded upon especially in relations to the other related work.",
      "For instance, it has been known how to leverage blackwell approachability to derive online calibration, and Gupta et al shows using techniques similar to blackwell approachability how to achieve online multicalibration (groupwise calibration).",
      "These kinds of discussions seem to be missing.",
      "The framework (looking at online learning problem via Blackwell Approachability) is significant in that even for any new fairness objectives/constraints in the online learning setting, one can immediately see whether it's compatible and and immediately get an algorithm, if compatible.",
      "In terms of the impossibility results, the original Blackwell approachability provides that the set is approachable if and only if the condition that resembles best response condition (i.e. no matter what the the other player plays, you can land in the set) is met.",
      "Using this idea, the paper, for instance, shows that the set that corresponds to group-wise no-regret is not approachable.",
      "But shouldn't the final statement be regarding online learnability \u2014 that is, doesn't one need to show that if a set is not Blackwell-approachable, then the corresponding learning problem isn't possible?",
      "It seems like one would need to invoke the equivalence result of 'Blackwell Approachability and No-Regret Learning are Equivalent' by Abernethy et al?",
      "In section 6, it's not clear to me how \\hat{C}_r is constructed to estimate the unknown set C.",
      "section 2.1, objective 1: N is never defined; I'm assuming it should be N = |A| I think?",
      "I think the dependence on |S| here will be linear.",
      "I wonder whether it's be possible to achieve logarithmic dependence on |S| using the trick in Gupta (using the exponential surrogate loss).",
      "In Theorem 1, d^2_T hasn't been defined.",
      "I'm assuming it's min_{v \notin C} ||m_T - v|| the rate of convergence toward the set you are trying to approach to?",
      "Thanks for answering my questions.",
      "I recommend acceptance!"
    ]
  },
  {
    "paper_id": "2106.12242v2",
    "submission_id": "vMWHOumNj5",
    "submission_title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
    "review_id": "LAvd_ooFola",
    "input": {
      "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- - The connection between Blackwell approachability and group notions of fairness (rather than just calibration) seems relatively novel.\n- Using this connection, the authors are able to study several group notions of fairness and to figure out whether online learning with said fairness constraints is possible, based on checking whether the approachability condition (2) holds.\n- Of particular interest is the fact that the authors are able to provide new (to the best of my knowledge) results on the trade-off that is achievable between group-wise calibration and demographic parity.\n- The authors provide rates of convergence based on the Blackwell approachability framework.\n- They extend their results to the case where the set to approach is unknown, and show how to use this extension to provide convergence results even when some of the priors on the probability of being in each group is unknown to the learner.\n- - It is a bit hard from reading the paper to figure out what are the authors\u2019 contributions versus what is new.\n- My understanding is that the connection between Blackwell approachability and no-regret learning was previously known, but that does not seem obvious from reading the paper.\n- Note that I am also not sure about the technical novelty of the paper, since I am not an expert on Blackwell approachability.",
    "review_points_list": [
      "- The connection between Blackwell approachability and group notions of fairness (rather than just calibration) seems relatively novel.",
      "Using this connection, the authors are able to study several group notions of fairness and to figure out whether online learning with said fairness constraints is possible, based on checking whether the approachability condition (2) holds.",
      "Of particular interest is the fact that the authors are able to provide new (to the best of my knowledge) results on the trade-off that is achievable between group-wise calibration and demographic parity.",
      "The authors provide rates of convergence based on the Blackwell approachability framework.",
      "They extend their results to the case where the set to approach is unknown, and show how to use this extension to provide convergence results even when some of the priors on the probability of being in each group is unknown to the learner.",
      "- It is a bit hard from reading the paper to figure out what are the authors\u2019 contributions versus what is new.",
      "My understanding is that the connection between Blackwell approachability and no-regret learning was previously known, but that does not seem obvious from reading the paper.",
      "Note that I am also not sure about the technical novelty of the paper, since I am not an expert on Blackwell approachability."
    ]
  },
  {
    "paper_id": "2106.12242v2",
    "submission_id": "vMWHOumNj5",
    "submission_title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
    "review_id": "L-GjSvkA91m",
    "input": {
      "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- I believe that this paper is an important and timely contribution to the literature of online fair learning.\n- The paper adapts Blackwell approachability to handle the case of both sensitive and non-sensitive stochastic contexts coming from a fixed but unknown distribution.\n- The framework provides 'if and only if' learnability results, and gives explicit rates in the learnable case.\n- The framework gives explicit rates in the learnable case, making it more efficient.\n- The main assumption (fast sequential estimation of the contextual distribution) may appear technical, but it is easily satisfied in most important online fairness settings studied in the literature.\n- The assumption poses virtually no problem in terms of the generality of the 'if and only if' results.\n- The identification of Pareto frontiers for group calibration under the parity constraint is a crucial contribution of this paper.\n- The authors do a good job with helping the reader keep track of the various assumptions made at various junctures in the proofs.\n- Some minor corrections are needed: The display below line 91, the second regret term should be r(a, b_t, x_t, s_t), not r(a'_s, b_t, x_t, s_t).\n- Line 144, I believe it should say 'perfect knowledge of the former', not 'the latter'.\n- Line 162, I suggest adding the word 'below' after 'Assumption 1', as the assumption has not been defined yet.",
    "review_points_list": [
      "I believe that this paper is an important and timely contribution to the literature of online fair learning.",
      "The paper adapts Blackwell approachability to handle the case of both sensitive and non-sensitive stochastic contexts coming from a fixed but unknown distribution.",
      "The framework provides 'if and only if' learnability results, and gives explicit rates in the learnable case.",
      "The framework gives explicit rates in the learnable case, making it more efficient.",
      "The main assumption (fast sequential estimation of the contextual distribution) may appear technical, but it is easily satisfied in most important online fairness settings studied in the literature.",
      "The assumption poses virtually no problem in terms of the generality of the 'if and only if' results.",
      "The identification of Pareto frontiers for group calibration under the parity constraint is a crucial contribution of this paper.",
      "The authors do a good job with helping the reader keep track of the various assumptions made at various junctures in the proofs.",
      "Some minor corrections are needed: The display below line 91, the second regret term should be r(a, b_t, x_t, s_t), not r(a'_s, b_t, x_t, s_t).",
      "Line 144, I believe it should say 'perfect knowledge of the former', not 'the latter'.",
      "Line 162, I suggest adding the word 'below' after 'Assumption 1', as the assumption has not been defined yet."
    ]
  },
  {
    "paper_id": "2506.02386v1",
    "submission_id": "zqo2sqixxbE",
    "submission_title": "Asymptotically Best Causal Effect Identification with Multi-Armed Bandits",
    "review_id": "nls2Ktw69Qo",
    "input": {
      "title": "Asymptotically Best Causal Effect Identification with Multi-Armed Bandits",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper attempts to provide a step towards identifying best available formula for the causal effect identification without deriving a solution from graphical criteria.\n- Causal effect identification is posed as a sequential decision problem where the investigator chooses which observations to make and updates the beliefs regarding the available formulas.\n- - The difference from the previous works is stated clearly.\n- Some of the limitations of the recent works, e.g., practical constraints for selection methods based on graphical criteria, are addressed via estimating the asymptotic variance directly.\n- - The idea of framing this problem as a best-arm-identification bandits problem is simple yet effective.\n- It may serve as a first step for solving more general problems as briefly touched in the discussion section.\n- The main technical result is a novel confidence sequence for the sample-splitting estimator.\n- It shows that having an uncentered influence function leads to a better convergence rate in terms of nuisance estimation.\n- Since most causal estimators require a nuisance estimate, this can be a nice feature.\n- - Adaptation of LUCB and SE bandit algorithms are sound.\n- However, only 9 formulas are considered.\n- It would have been insightful to comment on the effect of the number of formulas, e.g., does scaling behavior bear similarities to the sample complexity results presented in the section 4.\n- The general problem of trying to obtain the best estimate of the causal effect is briefly mentioned in the limitations section.\n- The reviewer understands that this paper is a first step in that direction.\n- Nevertheless, it would have been interesting to talk a bit more about the general problem.\n- For instance, a discussion on the applicability of the methods employed in this paper to infinite-armed bandits setting would shed light on the importance of this work for the future research.",
    "review_points_list": [
      "This paper attempts to provide a step towards identifying best available formula for the causal effect identification without deriving a solution from graphical criteria.",
      "Causal effect identification is posed as a sequential decision problem where the investigator chooses which observations to make and updates the beliefs regarding the available formulas.",
      "- The difference from the previous works is stated clearly.",
      "Some of the limitations of the recent works, e.g., practical constraints for selection methods based on graphical criteria, are addressed via estimating the asymptotic variance directly.",
      "- The idea of framing this problem as a best-arm-identification bandits problem is simple yet effective.",
      "It may serve as a first step for solving more general problems as briefly touched in the discussion section.",
      "The main technical result is a novel confidence sequence for the sample-splitting estimator.",
      "It shows that having an uncentered influence function leads to a better convergence rate in terms of nuisance estimation.",
      "Since most causal estimators require a nuisance estimate, this can be a nice feature.",
      "- Adaptation of LUCB and SE bandit algorithms are sound.",
      "However, only 9 formulas are considered.",
      "It would have been insightful to comment on the effect of the number of formulas, e.g., does scaling behavior bear similarities to the sample complexity results presented in the section 4.",
      "The general problem of trying to obtain the best estimate of the causal effect is briefly mentioned in the limitations section.",
      "The reviewer understands that this paper is a first step in that direction.",
      "Nevertheless, it would have been interesting to talk a bit more about the general problem.",
      "For instance, a discussion on the applicability of the methods employed in this paper to infinite-armed bandits setting would shed light on the importance of this work for the future research."
    ]
  },
  {
    "paper_id": "2506.02386v1",
    "submission_id": "zqo2sqixxbE",
    "submission_title": "Asymptotically Best Causal Effect Identification with Multi-Armed Bandits",
    "review_id": "CVK4M7nFuoe",
    "input": {
      "title": "Asymptotically Best Causal Effect Identification with Multi-Armed Bandits",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This study insists that 'Asymptotically Best Casual Effect Identification,' but there is not proof the asymptotic optimality.\n- The 'Asymptotically Best' may refer to asymptotically efficient estimator for the causal effect.\n- However it does not mean the asymptotic optimality of the best arm identification (BAI) algorithm.\n- For more details, see Gariver and Kaufmann 2016.\n- In the main algorithm, the authors do not use half of samples to use double/debiased machine learning (DML).\n- However, I think that this is a bit inefficient.\n- Can we use the method of Howard et al. 2019? That is, by using past observations, we can conduct DML for dependent samples.\n- This is also used by van der Laan 2008 and Kato, Yasui, and McAlinn 2020.\n- The Construction and Analysis of Adaptive Group Sequential Designs\n- The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive Experiments and a Paradox Concerning Logging Policy\n- I don't really understand how this paper can be applied.\n- I understand that the authors' study are motivated by the study of causal inference, but I could not understand the contribution to the field of bandit and design of experiments until the authors connect it a bit more with the literature on bandit and design of experiments.",
    "review_points_list": [
      "This study insists that 'Asymptotically Best Casual Effect Identification,' but there is not proof the asymptotic optimality.",
      "The 'Asymptotically Best' may refer to asymptotically efficient estimator for the causal effect.",
      "However it does not mean the asymptotic optimality of the best arm identification (BAI) algorithm.",
      "For more details, see Gariver and Kaufmann 2016.",
      "In the main algorithm, the authors do not use half of samples to use double/debiased machine learning (DML).",
      "However, I think that this is a bit inefficient.",
      "Can we use the method of Howard et al. 2019? That is, by using past observations, we can conduct DML for dependent samples.",
      "This is also used by van der Laan 2008 and Kato, Yasui, and McAlinn 2020.",
      "The Construction and Analysis of Adaptive Group Sequential Designs",
      "The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive Experiments and a Paradox Concerning Logging Policy",
      "I don't really understand how this paper can be applied.",
      "I understand that the authors' study are motivated by the study of causal inference, but I could not understand the contribution to the field of bandit and design of experiments until the authors connect it a bit more with the literature on bandit and design of experiments."
    ]
  },
  {
    "paper_id": "2506.02386v1",
    "submission_id": "zqo2sqixxbE",
    "submission_title": "Asymptotically Best Causal Effect Identification with Multi-Armed Bandits",
    "review_id": "QNKG3ZChB6Z",
    "input": {
      "title": "Asymptotically Best Causal Effect Identification with Multi-Armed Bandits",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- This paper is the first work that frames the problem of formula identification as a best-arm-identification problem.\n- The authors develop novel finite-sample confidence sequences with nuisance functions for a sample-splitting estimator of the asymptotic variance.\n- Having an uncentered influence function allows for confidence sequences that are insensitive to errors of the nuisance function estimates.\n- The paper is well-writed and structured while there are some minor issues.\n- This work covers two area, i.e. causal inference and bandits, which makes the exhibition of background knowledge important.\n- It is reasonable to move some background knowledge to the appendix, but it should be mentioned in the main text.\n- Little background knowledge on bandits theory is provided.\n- It would be great if the authors can give an overview on algorithms for best-arm-identification, especially LUCB and SE.\n- The authors connect the problem of formula identification with bandits, which is interesting and not the popular casual bandits/RL.\n- This work seems to be lack of technical and algorithmic contribution.\n- The only technical contribution is Theorem 1, which seems to be quite straightforward.\n- The algorithms are also just LUCB and SE with few changes.\n- I am curious about why the problem is formed as best-arm-identification instead of regret minimization.\n- With the confidence sequence, the UCB-type algorithms for regret minimization can be adapted just as LUCB and SE.\n- Is there any difficulty in doing so?",
    "review_points_list": [
      "This paper is the first work that frames the problem of formula identification as a best-arm-identification problem.",
      "The authors develop novel finite-sample confidence sequences with nuisance functions for a sample-splitting estimator of the asymptotic variance.",
      "Having an uncentered influence function allows for confidence sequences that are insensitive to errors of the nuisance function estimates.",
      "The paper is well-writed and structured while there are some minor issues.",
      "This work covers two area, i.e. causal inference and bandits, which makes the exhibition of background knowledge important.",
      "It is reasonable to move some background knowledge to the appendix, but it should be mentioned in the main text.",
      "Little background knowledge on bandits theory is provided.",
      "It would be great if the authors can give an overview on algorithms for best-arm-identification, especially LUCB and SE.",
      "The authors connect the problem of formula identification with bandits, which is interesting and not the popular casual bandits/RL.",
      "This work seems to be lack of technical and algorithmic contribution.",
      "The only technical contribution is Theorem 1, which seems to be quite straightforward.",
      "The algorithms are also just LUCB and SE with few changes.",
      "I am curious about why the problem is formed as best-arm-identification instead of regret minimization.",
      "With the confidence sequence, the UCB-type algorithms for regret minimization can be adapted just as LUCB and SE.",
      "Is there any difficulty in doing so?"
    ]
  },
  {
    "paper_id": "2106.06892v2",
    "submission_id": "z9Xs6T0y9Eg",
    "submission_title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
    "review_id": "gdjo2BBiCUl",
    "input": {
      "title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper proposes a novel approach to solving a classical problem in computer vision, namely, image denoising.\n- The authors design a deep neural network architecture that leverages a combination of convolutional and recurrent layers to learn the noise patterns in the input images.\n- The network is trained using a large-scale dataset of noisy images, and the authors demonstrate the effectiveness of their approach through a range of experiments.\n- The authors also provide a thorough analysis of the strengths and limitations of their approach, highlighting its advantages in terms of computational efficiency and noise robustness.",
    "review_points_list": [
      "The paper proposes a novel approach to solving a classical problem in computer vision, namely, image denoising.",
      "The authors design a deep neural network architecture that leverages a combination of convolutional and recurrent layers to learn the noise patterns in the input images.",
      "The network is trained using a large-scale dataset of noisy images, and the authors demonstrate the effectiveness of their approach through a range of experiments.",
      "The authors also provide a thorough analysis of the strengths and limitations of their approach, highlighting its advantages in terms of computational efficiency and noise robustness."
    ]
  },
  {
    "paper_id": "2106.06892v2",
    "submission_id": "z9Xs6T0y9Eg",
    "submission_title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
    "review_id": "CJim9spdoFu",
    "input": {
      "title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Previous work proposed the natural approach of solving a fractional + in-expectation LP relaxation, and then sequentially probing the edges (in random order) with probabilities obtained from the LP solution.\n- The goal of the analysis is to bound the minimum probability that any edge is probed, compared to its probability in the LP solution (which is an upper bound on the integral but offline optimum).\n- The authors of the new submission notice that this solution unfairly discriminates low-probability edges: out of k edges w.p. 1/k, we only have probability 1-1/e of picking one of them.\n- In contrast a single edge with probability 1 (which appears identical to the LP) will be picked w.p. 1.\n- This creates a bottleneck in the analysis when low and high probability edges are competing for matching the same vertex.\n- To overcome this issue, the authors propose to weigh down high-probability edges, yielding improved constant factors in the competitive analysis.\n- I think that this work will be quite interesting, but to a small crowd of specialized experts.",
    "review_points_list": [
      "Previous work proposed the natural approach of solving a fractional + in-expectation LP relaxation, and then sequentially probing the edges (in random order) with probabilities obtained from the LP solution.",
      "The goal of the analysis is to bound the minimum probability that any edge is probed, compared to its probability in the LP solution (which is an upper bound on the integral but offline optimum).",
      "The authors of the new submission notice that this solution unfairly discriminates low-probability edges: out of k edges w.p. 1/k, we only have probability 1-1/e of picking one of them.",
      "In contrast a single edge with probability 1 (which appears identical to the LP) will be picked w.p. 1.",
      "This creates a bottleneck in the analysis when low and high probability edges are competing for matching the same vertex.",
      "To overcome this issue, the authors propose to weigh down high-probability edges, yielding improved constant factors in the competitive analysis.",
      "I think that this work will be quite interesting, but to a small crowd of specialized experts."
    ]
  },
  {
    "paper_id": "2106.06892v2",
    "submission_id": "z9Xs6T0y9Eg",
    "submission_title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
    "review_id": "tdfKLSetoD6",
    "input": {
      "title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The main result is quite interesting. It improves on a sequence of earlier work in the area and, quite notably, the 0.382 algorithm is obtained by probing edges in random order!\n- Several variants and improved factors are discussed. A nice one (obtained as a corollary of the main result) is a best-known-to-date guarantee (of 0.432) for what the authors call prophet secretary under edge arrivals.\n- A DM has to select edges online to form a matching. The goal is to maximize the weight of the matching, and the benchmark is the expected weight of a max weight matching.\n- There are also many other results. These are a nice complement but somewhat less interesting to me.\n- One issue that is very interesting (and probably would be nice to have some more discussion in the paper) refers to the differences in the order of arrival of the edges.\n- For the 0.382 algorithm, the authors use random order, but it turns out that nothing better is known even if the algorithm could pick the probing order.\n- On the contrary, for the 0.432 algorithm, again the authors propose to use random order. But this does improve upon the best know algorithm with adversarial order.\n- Is there some sort of general statement connecting these three possible orderings (chosen by ALG, random, adversarial)?\n- What happens in the first problem if the order is adversarial? What happens in the second if the order can be chosen by ALG?\n- Overall I think this is a strong paper. It is, however, purely theoretical.\n- The authors discuss some applications in AI but very en passant and unconvincingly.",
    "review_points_list": [
      "The main result is quite interesting. It improves on a sequence of earlier work in the area and, quite notably, the 0.382 algorithm is obtained by probing edges in random order!",
      "Several variants and improved factors are discussed. A nice one (obtained as a corollary of the main result) is a best-known-to-date guarantee (of 0.432) for what the authors call prophet secretary under edge arrivals.",
      "A DM has to select edges online to form a matching. The goal is to maximize the weight of the matching, and the benchmark is the expected weight of a max weight matching.",
      "There are also many other results. These are a nice complement but somewhat less interesting to me.",
      "One issue that is very interesting (and probably would be nice to have some more discussion in the paper) refers to the differences in the order of arrival of the edges.",
      "For the 0.382 algorithm, the authors use random order, but it turns out that nothing better is known even if the algorithm could pick the probing order.",
      "On the contrary, for the 0.432 algorithm, again the authors propose to use random order. But this does improve upon the best know algorithm with adversarial order.",
      "Is there some sort of general statement connecting these three possible orderings (chosen by ALG, random, adversarial)?",
      "What happens in the first problem if the order is adversarial? What happens in the second if the order can be chosen by ALG?",
      "Overall I think this is a strong paper. It is, however, purely theoretical.",
      "The authors discuss some applications in AI but very en passant and unconvincingly."
    ]
  },
  {
    "paper_id": "2106.06892v2",
    "submission_id": "z9Xs6T0y9Eg",
    "submission_title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
    "review_id": "GMcr76anMWX",
    "input": {
      "title": "Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper provides original and interesting results.\n- The improved approximation guarantee for general graphs heavily builds upon a prior work and adds the idea of 'decreasing the probability of very likely edges'. This is a quite well-known trick but still leads to the improved guarantees in this setting.\n- I am not certain why a family of attenuation functions is better than having one that works.\n- The result for bipartite graphs with unit patience on one side is obtained by a pretty natural negative correlation rounding.\n- All their results also apply to the random order online setting except for the last mentioned bipartite case. This leads to interesting contention resolution schemes for general graphs and results for the prophet secretary problem in that setting.\n- I find the result for rank-1 matroids less interesting. The Lee and Singla result apply to all matroids with the same guarantee and I am not convinced that the presented schemes are much better.\n- Overall, I find it quite a nice collection of results.\n- A concern is why NeurIPS would be the right venue. The authors should do a better job explaining why this is related to learning (now they simply cite a couple of papers but they don't spend much effort in explaining the relationship).\n- Also a more detailed comment is if there is any known integrality gaps of the studied LP?",
    "review_points_list": [
      "The paper provides original and interesting results.",
      "The improved approximation guarantee for general graphs heavily builds upon a prior work and adds the idea of 'decreasing the probability of very likely edges'. This is a quite well-known trick but still leads to the improved guarantees in this setting.",
      "I am not certain why a family of attenuation functions is better than having one that works.",
      "The result for bipartite graphs with unit patience on one side is obtained by a pretty natural negative correlation rounding.",
      "All their results also apply to the random order online setting except for the last mentioned bipartite case. This leads to interesting contention resolution schemes for general graphs and results for the prophet secretary problem in that setting.",
      "I find the result for rank-1 matroids less interesting. The Lee and Singla result apply to all matroids with the same guarantee and I am not convinced that the presented schemes are much better.",
      "Overall, I find it quite a nice collection of results.",
      "A concern is why NeurIPS would be the right venue. The authors should do a better job explaining why this is related to learning (now they simply cite a couple of papers but they don't spend much effort in explaining the relationship).",
      "Also a more detailed comment is if there is any known integrality gaps of the studied LP?"
    ]
  },
  {
    "paper_id": "2209.03943v1",
    "submission_id": "w6U6g5Bvug",
    "submission_title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
    "review_id": "Ehs21GqaqFx",
    "input": {
      "title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The problem studied in this paper is an important one, given the extensive amount of explainability work in the field right now.\n- The paper is also well-written and uses sound statistical analysis methods.\n- I am concerned about whether the narrow scope of the experiments makes this paper a poor fit to this conference.\n- Their results are understandable that different explanation models will have different effects, particularly if information overload might slow an expert on certain tasks.\n- However, it seems reasonable that other explanation approaches might work much better, improving productivity.\n- E.g., what of an image-based explanation (with an easily read color map), which could be read at a glance, for an expert?\n- Or what of a natural language-based explanation (more tutorial-like) for a novice?\n- Or a model that adapts its explanations based on human performance?\n- It seems that this question is better studied in tandem with HCI.\n- Another question I have is how much does the quality of the agent's policy matter?\n- E.g., what if the cobot is dumb, or at least less optimal?\n- What effect do the different types of explanations have then?\n- Line 78: 'a ad hoc' -> 'an ad hoc'\n- Line 164: 'find-tune'\n- In Section 4.1, it would be good to have a citation for SAGAT.\n- In Section 5.2, why the use of deception?\n- Line 387: 'findings is limited'",
    "review_points_list": [
      "The problem studied in this paper is an important one, given the extensive amount of explainability work in the field right now.",
      "The paper is also well-written and uses sound statistical analysis methods.",
      "I am concerned about whether the narrow scope of the experiments makes this paper a poor fit to this conference.",
      "Their results are understandable that different explanation models will have different effects, particularly if information overload might slow an expert on certain tasks.",
      "However, it seems reasonable that other explanation approaches might work much better, improving productivity.",
      "E.g., what of an image-based explanation (with an easily read color map), which could be read at a glance, for an expert?",
      "Or what of a natural language-based explanation (more tutorial-like) for a novice?",
      "Or a model that adapts its explanations based on human performance?",
      "It seems that this question is better studied in tandem with HCI.",
      "Another question I have is how much does the quality of the agent's policy matter?",
      "E.g., what if the cobot is dumb, or at least less optimal?",
      "What effect do the different types of explanations have then?",
      "Line 78: 'a ad hoc' -> 'an ad hoc'",
      "Line 164: 'find-tune'",
      "In Section 4.1, it would be good to have a citation for SAGAT.",
      "In Section 5.2, why the use of deception?",
      "Line 387: 'findings is limited'"
    ]
  },
  {
    "paper_id": "2209.03943v1",
    "submission_id": "w6U6g5Bvug",
    "submission_title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
    "review_id": "xC6nN42EVhu",
    "input": {
      "title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The questions posed by the paper are important and relevant to the XAI field.\n- The paper and its novelty however is not very well position with respected work.\n- The question of whether explanations help Human-AI teams has been recently explored in several works which are not discussed in this submission (see list below for examples).\n- A lot of this related work also suggests that benefits from AI and XAI are higher when the human performance without AI is lower than AI alone.\n- A lot of other work has also shown that it is difficult to improve human-ai team performance for experts.\n- Thus, results in the paper need to be contextualized and clarified for novelty.\n- Perhaps some aspect of novelty to consider is the RL application in this case.\n- However, there is still some work in this area as well.\n- On human predictions with explanations and predictions of machine learning models: A case study on deception detection\n- Does the whole exceed its parts? The effect of AI explanations on complementary team performance.\n- Human Evaluation of Spoken vs. Visual Explanations for Open-Domain QA.\n- The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems.\n- Effect of AI Explanations on Human Perceptions of Patient-Facing AI-Powered Healthcare Systems.\n- Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making.\n- Mental Models of Mere Mortals with Explanations of Reinforcement Learning",
    "review_points_list": [
      "The questions posed by the paper are important and relevant to the XAI field.",
      "The paper and its novelty however is not very well position with respected work.",
      "The question of whether explanations help Human-AI teams has been recently explored in several works which are not discussed in this submission (see list below for examples).",
      "A lot of this related work also suggests that benefits from AI and XAI are higher when the human performance without AI is lower than AI alone.",
      "A lot of other work has also shown that it is difficult to improve human-ai team performance for experts.",
      "Thus, results in the paper need to be contextualized and clarified for novelty.",
      "Perhaps some aspect of novelty to consider is the RL application in this case.",
      "However, there is still some work in this area as well.",
      "On human predictions with explanations and predictions of machine learning models: A case study on deception detection",
      "Does the whole exceed its parts? The effect of AI explanations on complementary team performance.",
      "Human Evaluation of Spoken vs. Visual Explanations for Open-Domain QA.",
      "The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems.",
      "Effect of AI Explanations on Human Perceptions of Patient-Facing AI-Powered Healthcare Systems.",
      "Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making.",
      "Mental Models of Mere Mortals with Explanations of Reinforcement Learning"
    ]
  },
  {
    "paper_id": "2209.03943v1",
    "submission_id": "w6U6g5Bvug",
    "submission_title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
    "review_id": "0PgLP_k8QM7",
    "input": {
      "title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is well-written and clear.\n- The experiments are well-designed and the metrics capture the outcomes of interest.\n- The experiment task itself, collaboratively building a house in Minecraft, is an interesting and non-trivial task.\n- I also appreciate that the human-robot collaboration is unstructured, in that the human is free to choose the extent to which they collaborate with the robot; this makes the studies more ecologically valid.\n- The results of the study on time completion are surprising and useful: experts perform the task most efficiently with no robot explanations at all, and novices perform best with simple robot explanations (i.e., text) rather than more detailed explanations (i.e., decision trees).\n- This surprising result is of potential interest to the human-robot interaction community.\n- The main limitation of the work is that the policy is hand-crafted and very simple: a high-level and low-level policy, each of which is a two-level binary decision tree with a total of four leaf nodes.\n- It is unclear whether the results of this study could be generalized for more complex policies and explanations, that cannot be summarized so concisely.\n- I am also not sure whether this work is a good fit for NeurIPS, given that there is no algorithmic contribution.\n- I think this would be a strong submission for HRI, since the main contribution is the user studies.\n- The main novelty of this work is its thorough investigation of the utility of explainable AI for human-machine collaboration, in the form of two user studies on a non-trivial collaboration task in Minecraft.\n- There is no algorithmic or theoretical novelty.\n- The experiments are well-designed.\n- The analysis of the experiments is thorough and statistically sound.\n- This paper is well-written, and overall the motivation and user study design are explained clearly.\n- I just have one clarification question about the first user study\u2014were the status and decision-tree explanations for both the high and low level, or just for the low level?\n- I found it interesting that users had the most positive feelings toward the robot with decision-tree explanations, based on their answers to questions about trustworthiness, intelligence, positive teammate traits, etc.\n- But they actually performed worse with it, compared to robots with simpler explanations (that they didn\u2019t feel as positively about).\n- This is a valuable insight for the human-robot interaction community, although it is unclear whether this would generalize to more complex policies and tasks.\n- In terms of future work, something I am quite curious about is whether there is an explanation technique that would obtain similar positive user perception as the decision-tree explanations, without hurting performance on the task.\n- In Figures 3b and 3c, it would be good to state what the performance residuals are with respect to.\n- I assume it\u2019s with respect to the no-explanation condition.",
    "review_points_list": [
      "The paper is well-written and clear.",
      "The experiments are well-designed and the metrics capture the outcomes of interest.",
      "The experiment task itself, collaboratively building a house in Minecraft, is an interesting and non-trivial task.",
      "I also appreciate that the human-robot collaboration is unstructured, in that the human is free to choose the extent to which they collaborate with the robot; this makes the studies more ecologically valid.",
      "The results of the study on time completion are surprising and useful: experts perform the task most efficiently with no robot explanations at all, and novices perform best with simple robot explanations (i.e., text) rather than more detailed explanations (i.e., decision trees).",
      "This surprising result is of potential interest to the human-robot interaction community.",
      "The main limitation of the work is that the policy is hand-crafted and very simple: a high-level and low-level policy, each of which is a two-level binary decision tree with a total of four leaf nodes.",
      "It is unclear whether the results of this study could be generalized for more complex policies and explanations, that cannot be summarized so concisely.",
      "I am also not sure whether this work is a good fit for NeurIPS, given that there is no algorithmic contribution.",
      "I think this would be a strong submission for HRI, since the main contribution is the user studies.",
      "The main novelty of this work is its thorough investigation of the utility of explainable AI for human-machine collaboration, in the form of two user studies on a non-trivial collaboration task in Minecraft.",
      "There is no algorithmic or theoretical novelty.",
      "The experiments are well-designed.",
      "The analysis of the experiments is thorough and statistically sound.",
      "This paper is well-written, and overall the motivation and user study design are explained clearly.",
      "I just have one clarification question about the first user study\u2014were the status and decision-tree explanations for both the high and low level, or just for the low level?",
      "I found it interesting that users had the most positive feelings toward the robot with decision-tree explanations, based on their answers to questions about trustworthiness, intelligence, positive teammate traits, etc.",
      "But they actually performed worse with it, compared to robots with simpler explanations (that they didn\u2019t feel as positively about).",
      "This is a valuable insight for the human-robot interaction community, although it is unclear whether this would generalize to more complex policies and tasks.",
      "In terms of future work, something I am quite curious about is whether there is an explanation technique that would obtain similar positive user perception as the decision-tree explanations, without hurting performance on the task.",
      "In Figures 3b and 3c, it would be good to state what the performance residuals are with respect to.",
      "I assume it\u2019s with respect to the no-explanation condition."
    ]
  },
  {
    "paper_id": "2209.03943v1",
    "submission_id": "w6U6g5Bvug",
    "submission_title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
    "review_id": "uP0rK8zjCC7",
    "input": {
      "title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- The paper is a pleasure to read.\n- Measuring the effects of explanations in human-machine collaborative settings is extremely important for all interactive AI/ML applications.\n- The idea of doing so using a modified Minecraft platform is quite clever.\n- I could not find any major issues with the experimental design.\n- The evaluation is quite thorough.\n- To the best of my knowledge, the related work has good coverage with one exception that I could find:\n- the potentially harmful effects of machine explanations have been thoroughly explored in Ai et al., 'Beneficial and harmful explanatory machine learning'. MLJ 2021.\n- https://link.springer.com/article/10.1007/s10994-020-05941-0\n- Their results seem to anticipate the findings about the negative effects of explanations on novice players (although not the ones on expert players, as far as I could see).\n- It would be fair to mention and briefly discuss the link to this very relevant prior work.\n- This is a solid and welcome contribution and I do not have any major complaints about the paper, just one remark/question.\n- The explanations used in the experiments are quite small, essentially conjunctions of two literals.\n- Did you attempt to assess the impact of increasing the size/complexity of the explanations?\n- This question is quite central in the current XAI context, in which researchers do not shy away from presenting users with very detailed explanations.",
    "review_points_list": [
      "The paper is a pleasure to read.",
      "Measuring the effects of explanations in human-machine collaborative settings is extremely important for all interactive AI/ML applications.",
      "The idea of doing so using a modified Minecraft platform is quite clever.",
      "I could not find any major issues with the experimental design.",
      "The evaluation is quite thorough.",
      "To the best of my knowledge, the related work has good coverage with one exception that I could find:",
      "the potentially harmful effects of machine explanations have been thoroughly explored in Ai et al., 'Beneficial and harmful explanatory machine learning'. MLJ 2021.",
      "https://link.springer.com/article/10.1007/s10994-020-05941-0",
      "Their results seem to anticipate the findings about the negative effects of explanations on novice players (although not the ones on expert players, as far as I could see).",
      "It would be fair to mention and briefly discuss the link to this very relevant prior work.",
      "This is a solid and welcome contribution and I do not have any major complaints about the paper, just one remark/question.",
      "The explanations used in the experiments are quite small, essentially conjunctions of two literals.",
      "Did you attempt to assess the impact of increasing the size/complexity of the explanations?",
      "This question is quite central in the current XAI context, in which researchers do not shy away from presenting users with very detailed explanations."
    ]
  },
  {
    "paper_id": "2209.03943v1",
    "submission_id": "w6U6g5Bvug",
    "submission_title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
    "review_id": "UDhsY99-wgb",
    "input": {
      "title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
      "paper_content_placeholder": "TODO: Load paper content from extracted_files"
    },
    "output": "- Despite the claim in the paper, there is a bit of work looking at XAI in human-robot teaming; although it is not directly called this.\n- Some articles I know of: ...\n- The `status explanation' appears to be the same as the idea of communicating intent.\n- This has been studied in prior work in human-agent teaming; e.g., there are the articles that I know of, presumably there are others:\n- ...\n- The studies using Malmo and the tasks are excellent.\n- The task itself is reasonably rich and do run an interactive collaborative task is difficult to setup.\n- This is the type of evaluation I'd love to see more of, and would love to do more of.\n- ...\n- The study is well designed and separating studies between situation awareness and effects of SA effectively is a sound decision.\n- ...\n- I *think* the analysis of the expert vs novice is unsound.\n- ...\n- Some comments/questions:\n- ...\n- Did you validate the 20 hours Minecraft minimum?\n- ...\n- The two polic-information levels in the 2nd study are interesting.\n- ...\n- It would be good to get some idea of what the different surveys... and why they are important/useful.\n- ...\n- It would be great if the results from Q2 could be graphed if possible.\n- ...\n- How was trust measured?\n- ...\n- page 5: 'find-tuned' --> 'fine-tuned'\n- ...\n- page 6 and beyond: the use of 'p < 0.05'...\n- ...\n- page 9: 'HMT' is used for the first time and is not introduced.",
    "review_points_list": [
      "Despite the claim in the paper, there is a bit of work looking at XAI in human-robot teaming; although it is not directly called this.",
      "Some articles I know of: ...",
      "The `status explanation' appears to be the same as the idea of communicating intent.",
      "This has been studied in prior work in human-agent teaming; e.g., there are the articles that I know of, presumably there are others:",
      "...",
      "The studies using Malmo and the tasks are excellent.",
      "The task itself is reasonably rich and do run an interactive collaborative task is difficult to setup.",
      "This is the type of evaluation I'd love to see more of, and would love to do more of.",
      "...",
      "The study is well designed and separating studies between situation awareness and effects of SA effectively is a sound decision.",
      "...",
      "I *think* the analysis of the expert vs novice is unsound.",
      "...",
      "Some comments/questions:",
      "...",
      "Did you validate the 20 hours Minecraft minimum?",
      "...",
      "The two polic-information levels in the 2nd study are interesting.",
      "...",
      "It would be good to get some idea of what the different surveys... and why they are important/useful.",
      "...",
      "It would be great if the results from Q2 could be graphed if possible.",
      "...",
      "How was trust measured?",
      "...",
      "page 5: 'find-tuned' --> 'fine-tuned'",
      "...",
      "page 6 and beyond: the use of 'p < 0.05'...",
      "...",
      "page 9: 'HMT' is used for the first time and is not introduced."
    ]
  }
]